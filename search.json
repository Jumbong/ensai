[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "",
    "text": "somment mondial IA"
  },
  {
    "objectID": "posts/welcome/index.html#déroulement-du-sommet",
    "href": "posts/welcome/index.html#déroulement-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Déroulement du sommet",
    "text": "Déroulement du sommet\nCe sommet, qui s’est tenu sur deux jours (1er et 2 novembre), a réuni des sommités en IA, des entrepreneurs, des chercheurs et des représentants politiques de haut niveau. Parmi eux se trouvaient Antonio Guterres, Secrétaire général de l’ONU; Kamala Harris, vice-présidente des États-Unis; Giorgia Meloni, Première ministre italienne et seule dirigeante du G7 présente; et Elon Musk, CEO de X (ex Twitter)."
  },
  {
    "objectID": "posts/welcome/index.html#résultats-du-sommet",
    "href": "posts/welcome/index.html#résultats-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Résultats du sommet",
    "text": "Résultats du sommet\n\nUn accord sur la sécurité des systèmes d’IA a été signé par les grandes puissances mondiales telles que la Chine, les États-Unis et l’Union européenne, soulignant une responsabilité partagée dans la régulation de l’IA.\nUn premier rapport sur l’IA a été confié à Yoshua Bengio, scientifique canadien récompensé par le prix Turing, chargé d’évaluer les risques et avantages de l’IA.\nAntonio Guterres a plaidé pour une approche universelle : il a exhorté à une gestion de l’IA collective et éthique, alignée avec les principes fondamentaux et les droits de l’homme énoncés dans la charte des Nations Unies.\nElon Musk, lors d’un échange avec Rishi Sunak, a mis en lumière les risques de l’IA, même s’il envisage une “ère d’abondance”, alertant sur le fait que les robots humanoïdes pourraient un jour dépasser les capacités humaines.\n\nL’IA peut certes faciliter le travail et accélérer la résolution des problèmes, mais elle soulève également des inquiétudes. Si un robot peut penser et agir à la place d’une équipe de quatre personnes, que deviendront ces travailleurs ? C’est l’une des questions cruciales que soulève l’essor de l’IA."
  },
  {
    "objectID": "posts/welcome/index.html#prochaines-étapes",
    "href": "posts/welcome/index.html#prochaines-étapes",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Prochaines étapes",
    "text": "Prochaines étapes\nLe prochain sommet se déroulera à Paris, se concentrant sur des problèmes immédiats tels que la désinformation et l’impact sur l’emploi, avec une date encore à déterminer."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/p_value.html",
    "href": "posts/TechniquesMethodesScoring/p_value.html",
    "title": "The P-value under the Bootstrap Method with python",
    "section": "",
    "text": "Suppose you perform a study, and you have precise data, specifically ( n ) observations ( x_1 ) to ( x_n ) from ( n ) samples ( X_1 ) to ( X_n ), but you don’t know the distribution from which the data are drawn. Here, I assume you understand the difference between an observation and a sample. An observation \\(x_1\\) may be seen as the realization of a function \\(X_1\\).\nNow, suppose that you want to test if the mean of the data is equal to 0. For example, suppose the sample represents the CAC-40 current price, and you compute the empirical mean in the observation $ _{i=1}^{n} and you find 0 can you conclude that the mean of the distribution of data is 0 ? It’s difficult to conclude anything solely from the fact that the mean of the sample is zero. This result might be due to chance. To gain more confidence, the p-value is a statistical term that helps us determine the likelihood that the observed result is due to chance. Before defining the p-value, let’s clarify some mathematical terms we need :\n\nFirst, you need the observations \\((x_1,...x_n)\\) from the sample \\((X_1,...X_n)\\). Let’s assume that we don’t know the distribution of the sample. The sample could be the price of a stock, for example.\nNext, you need the hypothesis and the complementary hypothesis. The null hypothesis H0 is constructed alongside the alternative hypothesis H1. The null hypothesis might state that the mean is zero, while the alternative hypothesis might state that the mean is different from zero : \\(H0 : \\mu = 0\\) and \\(H1 : \\mu \\neq 0\\).\nThen, you need the test statistic T estimate under the null hypothesis. It might be the empirical mean, denoted as \\(T = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\).\nThen you need to compute the value of the test statistic in the data \\(t_{obs} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\) in the data you observed. Finally, you need to make a decision to accept or reject the null hypothesis. You reject the null hypothesis if the test statistic T is greater than the observed value \\(t_{obs}\\)(T &gt; \\(t_{obs}\\)).\n\nBecause the test statistic T is a random variable, you don’t directly observe it. The tool that allows us to make a decision here is to compute the probability that the test statistic is larger than the observed value. This probability, which we will call alpha (()), helps us decide whether to reject the null hypothesis.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Generate x values\nx = np.linspace(-4, 4, 1000)\n# Generate y values for the normal distribution\ny = norm.pdf(x, 0, 1)\n\n# Set up the plot\nplt.figure(figsize=(10, 5))\n\n# Plot the normal distribution curve\nplt.plot(x, y, color='blue')\n\n# Shade the rejection region\nx_fill = np.linspace(norm.ppf(0.95), 4, 100)\ny_fill = norm.pdf(x_fill, 0, 1)\nplt.fill_between(x_fill, 0, y_fill, color='red', alpha=0.5)\n\n# Add a vertical line for the critical value\nplt.axvline(norm.ppf(0.95), color='brown', linestyle='-', linewidth=2)\n\n# Add text for the regions\nplt.text(-2, 0.1, r\"$\\mathrm{No\\  reject}\\ H_0$\", fontsize=15, color='blue', rotation=0, ha='center')\nplt.text(3, 0.1, r\"$\\mathrm{Reject}\\ H_0$\", fontsize=15, color='brown', rotation=0, ha='center')\n\n# Add text for the critical value\nplt.text(norm.ppf(0.95), 0.02, r'$t_\\alpha$', fontsize=15, color='black', ha='center')\n\n# Remove y-axis\nplt.gca().axes.get_yaxis().set_visible(False)\n# Remove x-axis\nplt.gca().axes.get_xaxis().set_visible(False)\n# Add horizontal line\nplt.axhline(0, color='black',linewidth=0.5)\n\n# Set x-axis limits\nplt.xlim(-4, 4)\n\n# Set labels\nplt.xlabel('')\nplt.ylabel('')\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nNow, we can define the p-value as the smallest alpha threshold at which we will reject the test(T&gt;\\(t_{obs}\\)). In other words, the p-value is the probability of observing an extreme value as extreme as the observed value.\nSo for a given alpha threshold, if the p-value is less than the alpha threshold, we reject the null hypothesis. If the p-value is greater than the alpha threshold, we don’t reject the null hypothesis.\nHere, we have two scenarios to compute the p-value. First, you might know the distribution of the test statistic, or you might not know the distribution of the test statistic. It is easier to compute the p-value when you know the distribution of the test statistic. Here we will assume that you don’t know the distribution of the test statistic.\n\n\nThe bootstrap method is a resampling method that allows us to estimate the distribution of the test statistic. The idea is to resample the data B times and compute the test statistic for each resample. The distribution of the test statistic is then the distribution of the B test statistics (T1, T2, …, TB).\n The p-value is then the proportion of the test statistics that are greater than the observed value. In other words, you compare the observed value to the distribution of the test statistic. If the test statistic is greater than the observed value, you reject the null hypothesis(you note true) and if the test statistic is less than the observed value, you don’t reject the null hypothesis(you note false).\n\n\n\np-value\n\n\nThe p-value is then the proportion of the true values :\n\\[ p-value = \\frac{1}{B} \\sum_{i=1}^{B} \\mathbb{1}(T_i &gt; t_{obs}) \\]\nwhere ( (T_i &gt; t_{obs}) ) is the indicator function that returns 1 if the test statistic is greater than the observed value and 0 otherwise.\n\n\n\n\n\n\nAlgorithm to compute the p-value under the bootstrap method\n\n\n\n\nVariables :\n\n( x_1, …, x_n ) : the observations\n( t_{obs} ) : the observed value of the test statistic\nB : the number of bootstrap samples\n\nBegin :\n\nFor b in 1 to B : - Sample with replacement n observations from ( x_1, …, x_n ) to get ( x_1^b, …, x_n^b ) - Compute the test statistic \\(T_b\\) on the bootstrap sample ( x_1^b, …, x_n^b ) - Compute the p-value : ( p-value = {i=1}^{B} (T_i &gt; t{obs}) )"
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/p_value.html#the-p-value-under-a-unknown-distribution-of-the-test-statistic-using-the-bootstrap-method.",
    "href": "posts/TechniquesMethodesScoring/p_value.html#the-p-value-under-a-unknown-distribution-of-the-test-statistic-using-the-bootstrap-method.",
    "title": "The P-value under the Bootstrap Method with python",
    "section": "",
    "text": "The bootstrap method is a resampling method that allows us to estimate the distribution of the test statistic. The idea is to resample the data B times and compute the test statistic for each resample. The distribution of the test statistic is then the distribution of the B test statistics (T1, T2, …, TB).\n The p-value is then the proportion of the test statistics that are greater than the observed value. In other words, you compare the observed value to the distribution of the test statistic. If the test statistic is greater than the observed value, you reject the null hypothesis(you note true) and if the test statistic is less than the observed value, you don’t reject the null hypothesis(you note false).\n\n\n\np-value\n\n\nThe p-value is then the proportion of the true values :\n\\[ p-value = \\frac{1}{B} \\sum_{i=1}^{B} \\mathbb{1}(T_i &gt; t_{obs}) \\]\nwhere ( (T_i &gt; t_{obs}) ) is the indicator function that returns 1 if the test statistic is greater than the observed value and 0 otherwise.\n\n\n\n\n\n\nAlgorithm to compute the p-value under the bootstrap method\n\n\n\n\nVariables :\n\n( x_1, …, x_n ) : the observations\n( t_{obs} ) : the observed value of the test statistic\nB : the number of bootstrap samples\n\nBegin :\n\nFor b in 1 to B : - Sample with replacement n observations from ( x_1, …, x_n ) to get ( x_1^b, …, x_n^b ) - Compute the test statistic \\(T_b\\) on the bootstrap sample ( x_1^b, …, x_n^b ) - Compute the p-value : ( p-value = {i=1}^{B} (T_i &gt; t{obs}) )"
  },
  {
    "objectID": "posts/TBATS/index.html",
    "href": "posts/TBATS/index.html",
    "title": "TBATS : N’a pas de contraintes de saisonnalité",
    "section": "",
    "text": "Dans notre quête pour décrypter les tendances de consommation énergétique en France, tout au long de notre projet de série temporelle, nous avons été confrontés à un défi de taille : prédire efficacement la consommation future d’énergie dans un contexte de double saisonnalité. En explorant au-delà des méthodes classiques telles que les modèles SARIMA et ARIMA, nous avons rencontré des obstacles liés à l’estimation des variations saisonnières hebdomadaires et annuelles. Ces complexités nous ont conduits vers une solution innovante : le modèle TBATS.\nLe modèle TBATS, une avancée significative dans l’analyse des séries temporelles, embrasse la multifacette de la saisonnalité grâce à une élégante synthèse de fonctions trigonométriques. Contrairement aux modèles ARIMA et SARIMA qui peinaient à capturer les subtilités de nos données, TBATS a brillé par sa capacité à intégrer des périodicités multiples.\nDans cet article, nous plongeons au cœur de la méthodologie mathématique du modèle TBATS avant de le mettre en application sur nos données de consommation énergétique. Rejoignez-nous pour une exploration de la modélisation prédictive avec Python et la librairie tbats, et découvrez comment nous avons illuminé le chemin vers des prévisions énergétiques plus précises.\n\n1 Présentation du modèle TBATS\nLe modèle TBATS ou encore (Trigonometric, Box-Cox transform, ARMA errors, Trend and Seasonal components)(De Livera, Hyndman, and Snyder 2011) a pour paramètres TBATS(\\(\\omega\\), {p,q}, \\(\\phi\\), \\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\)) où :\n\n\\(\\omega\\) correspond à la transformation de Box-Cox.\n{p,q} correspond aux paramètres de l’ARMA.\n\\(\\phi\\) correspond à la tendance.\n\\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\) correspond aux paramètres de saisonnalité.\n\\(k_1,...,k_n\\) correspond aux nombres de Fourier de séries pairs.\n\nLe model s’écrit de la manière suivante :\n\\[\ny_t(\\omega) = l_{t-1} + \\phi b_{t-1} + \\sum_{i=1}^{T} s_{t-1}(i) + \\alpha d_t\n\\] \\[\nb_t = b_{t-1} + \\beta d_t\n\\] \\[\ns_t(i) = \\sum_{j=1}^{k_i} s_{j,t}(i)\n\\] \\[\ns_{j,t}(i) = s_{j,t-1}(i) \\cos \\lambda_j(i) + s^{*}_{j,t-1}(i) \\sin \\lambda_j(i) + \\gamma^{(i)}_1 d_t\n\\] \\[\ns^{*}_{j,t}(i) = -s_{j,t-1}(i) \\sin \\lambda_j(i) + s^{*}_{j,t-1}(i) \\cos \\lambda_j(i) + \\gamma^{(i)}_2 d_t\n\\] \\[\n\\lambda_j(i) = \\frac{2\\pi j}{m}\n\\]\noù :\n\n\\(i = 1, \\ldots, T\\)\n\\(d_t\\) est un processus ARMA ( p, q ),\n\\(\\alpha\\), \\(\\beta\\), \\(\\gamma_1\\) et \\(\\gamma_2\\) sont des paramètres de lissage,\n\\(l_0\\) est le niveau initial,\n\\(b_0\\) est la valeur de la pente.\n\nLes erreurs de prévisions seront modélisées par les indicateurs de qualité suivants :\n\nMean Squared Error (MSE) : \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\\)\nRoot Mean Squared Error (RMSE) : \\(RMSE = \\sqrt{MSE}\\)\nMean Absolute Error (MAE) : \\(MAE = \\frac{1}{n} \\sum_{i=1}^{n} |e_i|\\)\nMean Absolute Percentage Error (MAPE) : \\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{e_i}{y_i} \\right| \\cdot 100\\%\\)\nMean Error (ME) : \\(ME = \\frac{1}{n} \\sum_{i=1}^{n} e_i\\)\nMean Percentage Error (MPE) : $MPE = _{i=1}^{n} % $\nMean Absolute Scaled Error (MASE): \\(MASE = \\frac{n}{n-m} \\frac{\\sum_{i=1}^{n} |e_i|}{\\sum_{i=m+1}^{n} |y_i - y_{i-m}|}\\)\nAutocorrelation function of errors at lag 1 (ACF1) : \\(ACF1 = \\frac{\\sum_{i=1}^{n-1} (e_i - ME) \\cdot (e_{i+1} - ME)}{\\sum_{i=1}^{n} (e_i - ME)^2}\\)\n\noù : \\(e_t\\) is the error \\(e_t = y_t -{ y_t}^*\\)\n\\(y_t\\) est la valeur actuelle, \\(y_t^*\\) est la valeur prédicte , m est la période de saisonalité.\nDans l’analyse des prédictions de modèles statistiques, la compréhension des erreurs est cruciale. En comparant le Mean Error (ME) et le Mean Absolute Error (MAE), nous pouvons déterminer si les valeurs prédites sont systématiquement plus élevées ou plus faibles que les valeurs réelles, indiquant un biais directionnel. De même, la comparaison entre le Mean Percentage Error (MPE) et le Mean Absolute Percentage Error (MAPE) révèle l’ampleur de ce biais en termes de pourcentage.\nMais ce n’est pas tout. L’analyse du Mean Squared Error (MSE) peut révéler la présence de valeurs aberrantes ou d’erreurs exceptionnellement élevées dans les prédictions. Ces erreurs extrêmes se manifestent souvent par un écart significatif entre le MAE et le Root Mean Squared Error (RMSE), où le RMSE, en donnant plus de poids aux grandes erreurs, met en lumière les défauts plus subtils de notre modèle prédictif.\n\n\n2 Présentation des données\nDans notre étude appronfondie sur la demande énergétique en france, nous avons plongé au coeur des données de consommation, mésurées en mégawattts, révelatrices des tendances de consommation du pays. Afin d’affiner notre analyse nous avons présenter les données sous forme de séries temporelles journalières. Cette approche nous permettra d’aborder plus aisement l’analyse.\n\n\n3 Le dendogramme de la série temporelle\nNous avons mis en évidence des motifs saisonniers clés à travers la représentation d’un dendogramme. Cet outil permettra de vérifier que 365 et 7 sont des saisonnalités.\n\nfrom scipy import signal\n\nfrequencies, power_spectral_density = signal.periodogram(df_con_daily['consommation'].values)\n\n\n# Tracé du périodogramme\nplt.figure(figsize=(5, 3))\nplt.plot(frequencies, power_spectral_density)\nplt.title('Périodogramme')\nplt.xlabel('Fréquence')\nplt.ylabel('Densité spectrale de puissance')\n\n# Ajout de la ligne verticale à 1/12\nplt.axvline(x=1/365, color='red', linestyle='--')\nplt.axvline(x=1/7, color='yellow', linestyle='--')\n\nplt.show()\n\n\n\n\ndendrogramme\n\n\nNous observons que les cylces de 365 et 7 se détachent nettement, mettant en évidence une saisonnalité annuelle est hebdommadaire.\nEn plus de l’analyse dendrogramme, une autre méthode efficace pour mettre en lumière les saisonnalités dans nos données énergétiques consiste à décomposer la série temporelle. Cette approche permet d’isoler et d’examiner les composantes saisonnières annuelles et hebdomadaires de manière distincte. L’analyse des fonctions d’autocorrélation et d’autocorrélation partielle offre également des insights précieux. Cependant, pour rester concentrés sur les aspects les plus pertinents de notre étude, nous choisirons de ne pas approfondir cette méthode dans ce cadre. Les paramètres du model TBATS(False,{0,0},0.85,{&lt;7,365&gt;}) correspondent au mieux aux données.\nDans notre démarche analytique, une étape cruciale a été la segmentation de nos données en ensembles d’entraînement et de test. Cette division stratégique est essentielle pour affiner et évaluer la précision de nos modèles prédictifs. La variable date_cutoff joue un rôle clé dans ce processus, définissant le point de séparation entre les périodes d’entraînement et de test.\n\nimport pandas as pd\nfrom sktime.datatypes import check_raise\nfrom datetime import datetime\n\ny = df_con_daily['consommation']\ny.index = pd.to_datetime(y.index)\n\n\n\ndate_cutoff = pd.Timestamp('2019-12-31')\n\n# Ensuite, effectuez la comparaison\ny_train = df_con_daily[df_con_daily.index &lt; date_cutoff]['consommation']\ny_test = df_con_daily[df_con_daily.index &gt;= date_cutoff]['consommation']\ny_train.index = pd.to_datetime(y_train.index)\n\nEnsuite nous avons instancier le modèle TBATS avec les différentes saisonnalités 7 et 365. n_jobs facilite le temps de computation. Pour plus de détails voir [TBATS](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.tbats.TBATS.html). Assurez vous que vos index ou la colonne date soit de type pandas.core.indexes.datetimes.DatetimeIndex. Pour cela vous pouvez vous servir de chatgpt.\n\nforecaster = TBATS(sp = [7, 365], n_jobs = 1)\nmodel = forecaster.fit(y_train)\n\n# Prediction\nfh = len(y_test)\ny_pred = model.forecast(fh)\n\nfig, ax = plt.subplots(figsize = (15,5))  \ny.plot(title = 'TBATS dayly energie consumption', xlabel = '', ax = ax)\ny_pred.plot(ax = ax)\nax.legend(['Actual Values', 'Forecast'])\nplt.show()\n\nUne fonction pour les différentes métriques pour évaluer la qualité du modèle est donnée par :\n\n\n\nprevision\n\n\n\ndef print_metrics(y_true, y_pred, model_name):\n    mae_ = mean_absolute_error(y_true, y_pred)\n    rmse_ = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape_ = mean_absolute_percentage_error(y_true, y_pred)\n    smape_ = mean_absolute_percentage_error(y_true, y_pred, symmetric = True)\n    \n    dict_ = {'MAE': mae_, 'RMSE': rmse_,\n             'MAPE': mape_, 'SMAPE': smape_ }\n    \n    df = pd.DataFrame(dict_, index = [model_name])\n    return(df.round(decimals = 2)) \n\nPour avoir les résultats des performances du modèle, il faut exécuter cette fonction.\n\nprint_metrics(y_test, y_pred, 'TBATS Forecaster')\n\n\n\n4 Résultats de Prévision avec TBATS\nNous avons essayer de mettre les résultats dans un tableau :\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate \n\ntable = [[\"TBATS Forecaster\",\"290.94\",\"412.76\",\"0.07\",\"0.06\"]]\n\nMarkdown((tabulate(\n    table,\n    headers = [\"\",\"MAE\",\"RMSE\",\"MAPE\",\"SMAPE\"]\n)\n))\n\n\nIndicateurs de performance\n\n\n\nMAE\nRMSE\nMAPE\nSMAPE\n\n\n\n\nTBATS Forecaster\n290.94\n412.76\n0.07\n0.06\n\n\n\n\n\n\n\n5 Conclusion\n\nLe Mean Absolute Error (MAE) : Il montre qu’en moyenne, les prévisions s’écartent de 290.94 unités des valeurs réelles, nous donnant une idée de l’erreur moyenne absolue.\nLe Root Mean Squared Error (RMSE) : Sa valeur de 493.17, n’étant pas très élévée comparé au MAE, nous pouvons conclure que les erreurs de prévision sont relativement faibles.\nLe Mean Absolute Percentage Error (MAPE) et le Symmetric Mean Absolute Percentage Error (SMAPE) : Ils indiquent une erreur moyenne de prédiction de 0.07%, ce qui est considéré comme relativement précis dans notre contexte.\n\nAinsi le modèle TBATS est un modèle qui permet de faire des prévisions acceptables sur les données possèdant des saisonnalités multiples.\n\n5.0.1 References\n\n\nBROŻYNA, Jacek, Grzegorz Mentel, Beata Szetela, and Wadim Strielkowski. 2018. “MULTI-SEASONALITY IN THE TBATS MODEL USING DEMAND FOR ELECTRIC ENERGY AS a CASE STUDY.” Economic Computation & Economic Cybernetics Studies & Research 52 (1). https://www.researchgate.net/profile/Grzegorz-Mentel/publication/323868510_Multi-Seasonality_in_the_TBATS_Model_Using_Demand_for_Electric_Energy_as_a_Case_Study/links/5ab0afafaca2721710fe20b8/Multi-Seasonality-in-the-TBATS-Model-Using-Demand-for-Electric-Energy-as-a-Case-Study.pdf.\n\n\nDe Livera, Alysha M., Rob J. Hyndman, and Ralph D. Snyder. 2011. “Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing.” Journal of the American Statistical Association 106 (496): 1513–27. https://doi.org/10.1198/jasa.2011.tm09771."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html",
    "href": "posts/publicationMeduim/recuperationapi.html",
    "title": "Data retrieval from WEB API with Python",
    "section": "",
    "text": "In the current digital era, access and exploitation of data have become essential for many organizations. Web APIs( Application Programming Interface) offer a standardized interface to access data from the web. Thanks to its robusts libraries, Python greatly simplifies the process of retrieving data from the web. In this article, we will explore how to retrieve data from a web API using Python."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#introduction",
    "href": "posts/publicationMeduim/recuperationapi.html#introduction",
    "title": "Data retrieval from WEB API with Python",
    "section": "",
    "text": "In the current digital era, access and exploitation of data have become essential for many organizations. Web APIs( Application Programming Interface) offer a standardized interface to access data from the web. Thanks to its robusts libraries, Python greatly simplifies the process of retrieving data from the web. In this article, we will explore how to retrieve data from a web API using Python."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#what-is-a-web-api",
    "href": "posts/publicationMeduim/recuperationapi.html#what-is-a-web-api",
    "title": "Data retrieval from WEB API with Python",
    "section": "What is a Web API?",
    "text": "What is a Web API?\nA web API is a method for requesting and sending data between a client and a server. The client can be a web browser, a mobile application, or any other device that can access the internet. The server is a computer that hosts the data and processes the requests. There are many types of web APIs for accessing different types of data : - Spotify API for music data - Twitter API for social media data - Google Maps API for location data - CDS API for climate data\nTo access data from a web API, using Python, we need to follow these steps: 1. Find the API documentation 2. Install the necessary libraries 3. Make a HTTP request to the API to retrieve the data 4. Transform the data into a python object easily manipulated.\nThe most API web provide data in JSON format. JSON format is easily readable and manipulated in Python because it is similar to a Python dictionary. Let’s see how the HTTP request works in general and how to retrieve data from a web API using Python."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#http-request-in-python",
    "href": "posts/publicationMeduim/recuperationapi.html#http-request-in-python",
    "title": "Data retrieval from WEB API with Python",
    "section": "HTTP Request in Python",
    "text": "HTTP Request in Python\nHTTP Request format\nFirst, let’s study the format of an HTTP request, such as the dozens you make every day through your web browser. When you enter the following, URL in your browser’s address bar :\n\nhttps://jumbong.github.io/ensai/posts/ClimateScenario/substainablefinance.html/\n\nYour browser will send a request to the server concerned(these request will not only content the target URL, but also other information that will not dwell on here). In the previous URL we can distinguish 3 sub-parts :\n\nhttps://: Indicates the protocol to use to make the request(in this case https). In this chapter, we will use only be interested in the HTTP and HTTPS protocols(the secure version of the HTTP protocol).\njumbong.github.io: The domain name of the server to which the request is addressed.\n/ensai/posts/ClimateScenario/substainablefinance.html/: The path to the resource on the server.\n\nSimilarly, when calling a web API, we will specify the protocole to use, the machine to contact, the path to the desired resource and a number of parameters that will describe our request. Here is an example of a request to a web API(the Google Maps directions API in this case):\n\nhttps://maps.googleapis.com/maps/api/directions/json?origin=Toronto&destination=Montreal\n\nYou can copy/paste this URL into your browser’s address bar and observe what you get in return. Note that the result of this request is in JSON format. In fact, if you study the URL more closely, you’ll see that we’ve asked to get the result in this format. Additionally, we’ve specified in the URL that we want to get the route information from Toronto (origin parameter) to Montreal (destination parameter).\nYou should also notice that, in response to this request, the Google Maps API actually returns an error message. Indeed, to be authorized to use this API, you need to have an API key and provide this key in the form of an additional parameter (named key in the Google Maps APIs for example). So the previous request would become:\n\nhttps://maps.googleapis.com/maps/api/directions/json?origin=Toronto&destination=Montreal&key=VOTRE_CLE\n\nin which you will have to replace YOUR_KEY with a key that you have previously generated and that will allow you to use the web service in an authenticated manner. To create an API key, you need to go to the developer interface of the API concerned (here for the Google Maps Directions API for example)."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#retrieving-data-from-a-web-api-using-the-python-requests-module",
    "href": "posts/publicationMeduim/recuperationapi.html#retrieving-data-from-a-web-api-using-the-python-requests-module",
    "title": "Data retrieval from WEB API with Python",
    "section": "Retrieving data from a web API using the python requests module",
    "text": "Retrieving data from a web API using the python requests module\n\n\n\n\n\n\nHTTP requests in (very) brief\n\n\n\nIn the HTTP protocol, there are several types of requests to perform the exchange between the client and the server. In particular, GET requests are widely used when the client requests a resource from the server. This is a request to download a document. It is possible to transmit parameters to filter the response; in this case, the parameters will be transferred “in clear” (in the URL used for the request).\nPOST requests, like GET, allow downloading a document from the server to the client, but with more sophistication: the parameters are hidden and it is possible to request to update data on the server as part of the request.\nThere are other HTTP requests that we will not detail here.\n\n\nThe previous section provided a refresher on the format of HTTP requests, and you were asked to perform HTTP requests using your browser. If you now want to automatically retrieve the result of an HTTP request to manipulate it in Python, the most convenient way is to perform the HTTP request from within Python. To do this, we use the requests module. This module includes a get function that allows you to perform GET-type HTTP requests (I’ll let you guess the name of the function that allows you to perform HTTP POST requests :) :\n\nimport requests\n\nurl = \"http://my-json-server.typicode.com/rtavenar/fake_api/tasks\"\n\nreponse = requests.get(url)\nprint(reponse)\n\n&lt;Response [200]&gt;\n\n\nThe get function returns a Response object. This object contains the response to the request, including the status code (200 if the request was successful).\n\n\n\n\n\n\nStatus codes\n\n\n\n\n20x : the request is successful\n\nExample: 200 OK\n\n40x : error due to client\n\nExample : 404 page not found\n\n50x : error due to server\n\nExample : 504 Gateway Timeout\n\n\n\n\nYou can obtain the result of our request in two forms: the raw text of the result, which is stored in response.text, and the formatted version (in the form of a dictionary or list) of this result, which you can obtain via response.json().\n\ncontenu_txt = reponse.text\nprint(type(contenu_txt))\n\n&lt;class 'str'&gt;\n\n\n\ncontenu = reponse.json()\nprint(type(contenu))\n\n&lt;class 'list'&gt;\n\n\n\nprint(contenu)\n\n[{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}, {'userId': 1, 'id': 2, 'title': 'quis ut nam facilis et officia qui', 'completed': False}, {'userId': 1, 'id': 3, 'title': 'fugiat veniam minus', 'completed': False}, {'userId': 1, 'id': 4, 'title': 'et porro tempora', 'completed': True}, {'userId': 1, 'id': 8, 'title': 'quo adipisci enim quam ut ab', 'completed': True}, {'userId': 3, 'id': 44, 'title': 'cum debitis quis accusamus doloremque ipsa natus sapiente omnis', 'completed': True}, {'userId': 3, 'id': 45, 'title': 'velit soluta adipisci molestias reiciendis harum', 'completed': False}, {'userId': 3, 'id': 46, 'title': 'vel voluptatem repellat nihil placeat corporis', 'completed': False}]\n\n\nFurthermore, if you want to pass parameters to the HTTP request (what was after the ? symbol in the URLs above), you can do so when calling requests.get :\n\nimport requests\n\nurl = \"http://my-json-server.typicode.com/rtavenar/fake_api/tasks\"\n\nreponse = requests.get(url, params=\"userId=3\")\ncontenu = reponse.json()\nprint(contenu)\n\n[{'userId': 3, 'id': 44, 'title': 'cum debitis quis accusamus doloremque ipsa natus sapiente omnis', 'completed': True}, {'userId': 3, 'id': 45, 'title': 'velit soluta adipisci molestias reiciendis harum', 'completed': False}, {'userId': 3, 'id': 46, 'title': 'vel voluptatem repellat nihil placeat corporis', 'completed': False}]\n\n\nThe code above corresponds to what you would get in your browser by entering the URL http://my-json-server.typicode.com/rtavenar/fake_api/tasks?userId=3.\nIn practice, in many cases, Python modules exist to allow the use of public APIs without having to manage HTTP requests directly. This is the case, for example, with the tweepy module (for the Twitter API) or the graphh module (which allows access to the GraphHopper API, which is a free alternative to Google Maps)."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#conclusion",
    "href": "posts/publicationMeduim/recuperationapi.html#conclusion",
    "title": "Data retrieval from WEB API with Python",
    "section": "Conclusion",
    "text": "Conclusion\nThank you for reading. If you have any suggestions don’t hesitate, I will take it in consideration."
  },
  {
    "objectID": "posts/publicationMeduim/recuperationapi.html#references",
    "href": "posts/publicationMeduim/recuperationapi.html#references",
    "title": "Data retrieval from WEB API with Python",
    "section": "References",
    "text": "References\n\nPython requests module documentation"
  },
  {
    "objectID": "posts/NotionCle/index.html",
    "href": "posts/NotionCle/index.html",
    "title": "Notions clés pour comprendre le réchauffement climatique",
    "section": "",
    "text": "Le réchauffement climatique est une notion d’actualité. Il est important de comprendre les mécanismes qui le sous-tendent, les mécanismes qui servent de base à sa compréhension. Connait-on vraiment l’atmosphère ? Nous avons souvent attendu parler de rayonnement, d’albedo, de gaz à effet de serre et de bilan radioactif. Savons-nous vraiment ce que cela signifie ?\nLa première partie de ce document consistera à présenter brièvement l’atmosphère et ces différentes couches. Ensuite, nous aborderons les notions de rayonnement, d’albedo, de gaz à effet de serre et de bilan radioactif.",
    "crumbs": [
      "About",
      "climat",
      "Notions clés pour comprendre le changement climatique"
    ]
  },
  {
    "objectID": "posts/NotionCle/index.html#les-couches-de-latmosphère-terrestre.",
    "href": "posts/NotionCle/index.html#les-couches-de-latmosphère-terrestre.",
    "title": "Notions clés pour comprendre le réchauffement climatique",
    "section": "Les couches de l’atmosphère terrestre.",
    "text": "Les couches de l’atmosphère terrestre.\nL’atmosphère est constitué de plusieurs couches, dont les plus importantes pour comprendre le réchauffement climatique sont la troposphère et la stratosphère.\n\nLa troposhère est la couche inférieure de l’atmosphère, la couche la plus proche du sol. Elle contient les nuages et la majorité de la vapeur d’eau de l’atmosphère. Elle constitue les 5/6 de la masse totale de l’atmosphère. C’est dans cette couche que se déroule la plupart des phénomènes météorologiques. La température diminue avec l’altitude dans cette couche.\nLa stratosphère compose le niveau supérieur, jusqu’à 50 km d’altitude et héberge la couche d’ozone. La température augmente avec l’altitude dans cette couche, absorbant une grande partie du rayonnement solaire.\n\nJe n’insisterai pas sur les autres couches qui sont la mésosphère, la thermosphère et l’exosphère.",
    "crumbs": [
      "About",
      "climat",
      "Notions clés pour comprendre le changement climatique"
    ]
  },
  {
    "objectID": "posts/MarketRiskGen/index.html",
    "href": "posts/MarketRiskGen/index.html",
    "title": "Market risk generalities",
    "section": "",
    "text": "A market risk is a risk of loss an investment value due to a variation of the market factors. The market factors are the parameters that influence the price of a financial instrument. The most common market factors are:\n\nInterest rates\nEquity prices\nForeign exchange rates\nCommodity prices\n\nThe interest rate risk is the risk that the value of a financial instrument(ex:bond) will decline due to an increase of the interest rates.\nC’est donc une offre dont la valeur va baisser si les taux augmentent. C’est le cas des obligations.\nExemple : A zero coupon bond with a face value of 1000€ and a maturity of 10 years. The interest rate is 5%. The value of the bond is p = \\frac{1000}{(1+0.05)^{10}} = 613.\nIf the interest rate increases to 6%, the value of the bond is p = \\frac{1000}{(1+0.06)^{10}} = 558\nUne autre chose qu’il faut remarquer est que si les taux d’intérêts augmentent, la valeur des obligations diminue. Qu’elle est l’interprétation économique de cela ?\n\n\nSi les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue.\n\n\n\nUne banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "href": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "title": "Market risk generalities",
    "section": "",
    "text": "Si les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#curve-risk",
    "href": "posts/MarketRiskGen/index.html#curve-risk",
    "title": "Market risk generalities",
    "section": "",
    "text": "Une banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "href": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "title": "Market risk generalities",
    "section": "Sensitivity-based methods",
    "text": "Sensitivity-based methods\n\nDuration\nLa duration exprime comment un porte-feuille est sensible aux variations des taux d’intérêts.\nExemple: Un porte-feuille avec une duration de 5 ans signifie que si les taux d’intérêts augmentent de 1%, la valeur du porte-feuille va diminuer de 5%.\nDonc une duration élevée signifie un risque élevé.\nMaintenant si on a deux obligations(bonds): Le premier de nominal 1000, maturité 10 ans, de prix 900 et de coupon 5%. Le deuxième de nominal 1000, maturité 10 ans, de prix 1100 et de coupon 7%.\nSur quels bond va t-on investir ?\nPour répondre à cette question, nous avons besoin d’un outil: the yield to maturity.\nThe yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre. Mathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\noù C_i = nominal* coupon i!=N C_N = nominal + coupon*nominal\nIl faut deux hypothèses pour calculer le yield to maturity: - Le coupon est réinvesti au taux d’intérêt du marché. - Le titre est détenu jusqu’à la maturité. On a défini le yield to maturity, on peut définir la duration.\nLa duration est la sensibilité de la valeur d’un titre par rapport au yield to maturity. Mathématiquement,\nD = -\\frac{1}{P}\\frac{dP}{dy}.\nConséquence de la duration:\n\nSi le taux d’intérêt augmente diminue, nous allons investir dans un porte-feuille avec une duration élevée.\nSoit une obligation dont le prix est de 90 qui paye des coupons annnuels de 5% de nominal 1000 et de maturité 5 ans.\n\nDeterminons son yield to maturity.\nOn sait que le yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre.\nMathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\nPour déterminer la duration, on a besoin de calculer la dérivée de P par rapport à y.\non sait que la dérivée peut être calculée comme une limite de la formule suivante:\n\\frac{dP}{dy} = \\lim_{\\Delta y \\to 0} \\frac{P(y+\\Delta y) - P(y- \\Delta y)}{2 \\Delta y}\nAinsi si on a déterminé le yield to maturity, on peut déterminer la duration. Il faut pour cela, calculer le prix de l’obligation pour deux valeurs de y: y+\\Delta et y-\\Delta y. Prenons \\Delta = 0.01 Pour y1 = y+\\Delta P(y+\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y+\\Delta)^i}\npour y2 = y-\\Delta P(y-\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y-\\Delta)^i}\nFinalement, D = -\\frac{1}{P}\\frac{P(y+\\Delta) - P(y- \\Delta y)}{2 \\Delta y}\nIl y’a une hypothèse forte derrière la duration: Une courbe de déplacement parallèle. C’est à dire que si les taux d’intérêts augmentent de 1%, la courbe des taux d’intérêts va se déplacer parallèlement vers le haut."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#the-greeks",
    "href": "posts/MarketRiskGen/index.html#the-greeks",
    "title": "Market risk generalities",
    "section": "The Greeks",
    "text": "The Greeks\nLa valeur d’un porte-feuille dépend de plusieurs facteurs, qui peuvent interagir entre eux. Pour mesurer l’impact de ces facteurs sur la valeur du porte-feuille, on utilise les lettres grecques : delta, gamma, vega, theta, rho.\n\nDelta\nLe delta exprime le changement entre un changement de la valeur du porte-feuille et un changement de la valeur du sous-jacent. Exemple : Soit C_0 le prix d’une option de maturité T et de strike k. Soit S_t le prix du sous-jacent à la date t.\npayoff de l’option : max(S_T - k, 0), le prix de l’option est le payoff actualisé : C_0 = \\frac{max(S_T - k, 0)}{(1+r)^T}\nLes facteurs de risques de cette option sont : le prix du sous-jacent, le taux d’intérêt et la volatilité.\nLe delta de cette option est : \\frac{\\partial C_0}{\\partial S_t}\nComment se couvrir de la variation du prix du sous-jacent ?\nIl faut construire un porte-feuille qui a un delta égal à 0. C’est à dire que si le prix du sous-jacent augmente, la valeur du porte-feuille ne change pas.\non peut par exemple considéré un porte-feuille de prix P^{'} = C_0 + \\alpha S_t. La nouvelle option est de sensibilité nulle par rapport au prix du sous-jacent si \\alpha = -\\frac{\\partial C_0}{\\partial S_t}. Ceci revient à dire que pour se couvrir du risque de la variation de prix du sous-jacent, il faut vendre à decouvert c’est-à-dire short selling \\frac{\\partial C_0}{\\partial S_t} unités du sous-jacent. Le delta d’une action est 1.\nUn portefeuille avec un delta de 0 est option risque neutre. Il est construit pour (hedging purposes) se couvrir du risque de la variation du prix du sous-jacent.\n\n\nGamma\nLe gamma est la dérivée seconde du prix de l’option par rapport au prix du sous-jacent. Il mesure la sensibilité du delta par rapport au prix du sous-jacent. Son implication dans le cas d’un portefeuille delta-neutre est qu’il a besoin d’être réajusté régulièrement si son gamma est élevé. Le gamma est donc de mesurer le degré d’exposition au risque qu’une position couverte(hedged position) dévellopera si la couverture(hedge) n’est pas réajustée.\n\n\nVega\nLe vega mesure le changement du prix de l’option par rapport à la volatilité du sous-jacent. Il mesure la sensibilité du prix de l’option par rapport à la volatilité du sous-jacent.\nUn portefeuille avec un vega de 0 est vega-neutre. Il est construit pour se couvrir du risque de la variation de la volatilité du sous-jacent. C’est-à-dire qu’il est insensitive à la variation de la volatilité du sous-jacent.\n\n\nTheta and Rho\nLe Theta et le rho déterminent respectivement le taux de changement de la valeur d’un portefeuille par rapport au temps to maturity et par rapport au taux d’intérêt.\nEn practique, les sensibilités d’un portefeuille sont calculées intensivement, intra-day(à l’intérieur de la journée) et quotidiennement pour chaque traded product. Ils sont utilisées par les traders afin de gérer leur risque, their position(hedging) et par les managers pour expliquer leur perte et leur profit(P&L). Cependant, elles souffrent de plusieurs limitations: elles ne peuvent pas être comparées entre plusieurs activités pour conclure qu’une activité est plus risquée qu’une autre."
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html",
    "href": "posts/machineLearning3A/TP1_solution.html",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "",
    "text": "I will begin this post by reminder what OLS is and then I will introduce PCA-OLS, Lasso and Ridge regression.\nThe OLS estimator is any coefficient vector \\(\\hat{\\theta_n}\\) = \\((\\hat{\\theta_1}\\), \\(\\hat{\\theta_2}\\), …, \\(\\hat{\\theta_p})^T\\) \\(\\in\\) \\(R^p\\) such that :\n\\[\\hat{\\theta_n}=argmin_{\\theta \\in R^p} \\sum_{i=1}^{n} (y_i - x_i^T\\theta)^2\\]\nWhere \\(x_i\\) is the vector of features of the i-th observation and \\(y_i\\) is the target variable of the i-th observation.\nIn matrix notation, we have :\nZ = \\(\\begin{bmatrix} x_1^T \\\\ x_2^T \\\\ ... \\\\ x_n^T \\end{bmatrix}\\), Y = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n \\end{bmatrix}\\), \\(1_n\\) = \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ ... \\\\ 1 \\end{bmatrix}\\)\n\\(\\hat{\\theta_n}=argmin_{\\theta \\in R^p} ||Y - Z\\theta||_2^2\\)\nand the OLS estimator is defined by the normal equation :\n\\[Z^TZ\\hat{\\theta_n} = Z^TY\\]\nNote that solution of this equation is not unique without further assumptions on the data.\nFor instance, if \\(u \\in Ker(Z)\\), then \\(\\hat{\\theta_n} + u\\) is also a solution of the normal equation.\nThe solution will be unique if only \\(Ker(Z)\\) = Ker(\\(Z^TZ\\)) = \\({0}\\) which is equivalent to \\(Z^TZ\\) being invertible and this is the case if \\(Z\\) has full column rank. The solution is then given by :\n\\[\\hat{\\theta_n} = (Z^TZ)^{-1}Z^TY\\]\nIf \\(Ker(Z) \\neq {0}\\), then the OLS estimator is not unique. In this case, the set of solutions writes \\(\\hat{\\theta_n} + Ker(Z)\\) where \\(\\hat{\\theta_n}\\) is the particular solution given by the normal equation.\nThere are four cases where you should not use OLS estimator : When we have ill-conditionning, multicollinearity, more variables that samples, and when the model overfit.\nThere are many ways to overcome the issues mentioned above. The PCA before OLS is one of the most common ways to deal with multicollinearity. The other way is to use Lasso and Ridge regression.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#the-pca-ols",
    "href": "posts/machineLearning3A/TP1_solution.html#the-pca-ols",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "1 The PCA-OLS",
    "text": "1 The PCA-OLS\nThe PCA-OLS is a method that consists in applying PCA to the features to reduce the dimension of the data and then apply OLS to the reduced data. This method is useful when the number of features is large compared to the number of samples and when the features are correlated.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#the-ridge-regression",
    "href": "posts/machineLearning3A/TP1_solution.html#the-ridge-regression",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "2 The Ridge Regression",
    "text": "2 The Ridge Regression\nThe ridge regression is defined as follows : \\[\\hat{\\theta_n}^{ridge} = argmin_{\\theta \\in R^p} \\sum_{i=1}^{n} (y_i - x_i^T\\theta)^2 + \\lambda ||\\theta||_2^2\\]\nWhere \\(\\lambda\\) is the regularization parameter.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#the-lasso-regression",
    "href": "posts/machineLearning3A/TP1_solution.html#the-lasso-regression",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "3 The Lasso Regression",
    "text": "3 The Lasso Regression\nThe Lasso regression is defined as follows : \\[\\hat{\\theta_n}^{lasso} = argmin_{\\theta \\in R^p} \\sum_{i=1}^{n} (y_i - x_i^T\\theta)^2 + \\lambda ||\\theta||_1\\]\nWhere \\(\\lambda\\) is the regularization parameter.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#implementation-in-python",
    "href": "posts/machineLearning3A/TP1_solution.html#implementation-in-python",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "4 Implementation in Python",
    "text": "4 Implementation in Python\nWe work with the data set diabetes accessible in python. The initial data consists of n = 442 patients and p = 10  covariates. The output variable Y is a score reflecting the disease progressing.  For fun, a bad robot has contaminated the data set by adding 200 inappropriate exploratory variables. Since  simply noising the data was not sufficient for the robot, he also arbitrarily permuted the variables. To complete  the picture, the robot has erased any trace of his villainous act and thus we do not know which variables are  relevant.  The new data set contains n = 442 patients and p = 210 covariates denoted by X. Are you capable to resolve the  enigma created by the playful machine and retrieve the relevant variables ?\nBefore starting, we will split the data into a training set and a test set and we will standardize the data.\nLet’s start by importing the necessary libraries and loading the data.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#importing-the-necessary-libraries",
    "href": "posts/machineLearning3A/TP1_solution.html#importing-the-necessary-libraries",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "5 Importing the necessary libraries",
    "text": "5 Importing the necessary libraries\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.stats import norm\n\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nimport seaborn as sns\n\nnp.random.seed(2)",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#importation-of-data-which-the-last-column-is-the-output",
    "href": "posts/machineLearning3A/TP1_solution.html#importation-of-data-which-the-last-column-is-the-output",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "6 Importation of data which the last column is the output",
    "text": "6 Importation of data which the last column is the output\nProvide the number of the explanatory variables and the number of the observations\n\ndf = pd.read_csv(\"data/data_dm3.csv\", header=None)\nA = np.array(df)\nprint(A.shape)\n\nX = A[:, :-1]          # on récupère les covariables\nY = A[:, -1]           # puis l'output\n\nn, d = X.shape\nprint(\"X dimensions are: %s x %s \" % (n, d))\n\n(442, 211)\nX dimensions are: 442 x 210 \n\n\nIt is important to check if the data is well centered and normalized.\n\nprint(\"To ensure that means and variances are properly computed for\",\n      \"the variables,\\nwe check the dimensions. It is %s with axis=0, %s with axis=1.\\n\"\n      % (len(X.mean(axis=0)) == d, len(X.mean(axis=1)) == d))\n\nprint(\"Are explanatory variables: \\n centered? %s\\n normalized? %s\\n\"\n      % (np.allclose(X.mean(axis=0), 0), np.allclose(X.std(axis=0), 1)))\n\nprint(\"Is the output variable: \\n centered? %s\\n normalized? %s\"\n      % (np.allclose(Y.mean(axis=0), 0), np.allclose(Y.std(axis=0), 1)))\n\nTo ensure that means and variances are properly computed for the variables,\nwe check the dimensions. It is True with axis=0, False with axis=1.\n\nAre explanatory variables: \n centered? True\n normalized? True\n\nIs the output variable: \n centered? False\n normalized? False\n\n\nEach variable of X is centered and normalized, but not Y.\nLet’s draw the scatterplots of covariates and the output variable.\n\n#sns.set(style=\"ticks\")\ndf_sub = pd.DataFrame(X[:, np.random.choice(range(210), 4)])\ndf_sub['y'] = Y\n\nsns.pairplot(df_sub)\nplt.show()\n\n\n\n\n\n\n\n\nThe selected covariates seem to be poorly correlated with each other. The are also poorly correlated with the explanatory variable. Before starting, will split the data into a training set and a test set and we will standardize the data.\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2,\n                                                    random_state=2)\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit the scaler on the training data only\nscaler.fit(X_train)\n\n# Apply the transformation to both the training set and the test set.\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nn_train, _ = X_train.shape\nprint(X_train.shape, X_test.shape)\n\n(353, 210) (89, 210)",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#pca-ols",
    "href": "posts/machineLearning3A/TP1_solution.html#pca-ols",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "7 PCA-OLS",
    "text": "7 PCA-OLS\nFirst, we provide the matrix covariance for the X_train_scaled and compute the svd of the covariance matrix (here, we could as well use the classical spectral decomposition, and not the SVD):\n\ndf = pd.DataFrame(X_train_scaled)\nCOV = df.cov() \nf = plt.figure(figsize=(8, 4))\nplt.matshow(COV, fignum=f.number)\ncb = plt.colorbar()\n#cb.ax.tick_params(label=FALSE,labelsize=0)\nplt.title('Covariance Matrix', fontsize=16)\n\nText(0.5, 1.0, 'Covariance Matrix')\n\n\n\n\n\n\n\n\n\nLet us compute the svd of the covariance matrix (here, we could as well use the classical spectral decomposition, and not the SVD):\n\nU, s, V = np.linalg.svd(COV, full_matrices = True)\n\nNow, we will plot the eigenvalues to see those are significantly different of 0 and We record the index of the last non-zero eigenvalue.\n\nfig = plt.figure()\nplt.plot(s)\nplt.axvline(52, c='r')\n#plt.axhline(0,c='r')\nplt.title(\"eigenvalues values of the matrix correlation\")\nplt.show()\n\n\n\n\n\n\n\n\nWe notice that only the first 60 values are (significantly) different from 0. This means that the design matrix is rank-deficient (60 &lt; 210). Thus attempting a ‘’PCA before OLS’’ strategy is legitimate. We record the index of the last non-zero eigenvalue.\n\nprint((s[0:52]&gt;0.05))\nprint(s[52]) #indice n°9 pour la dixième valeur!\nelbow = 52  \n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n0.9448172106956519\n\n\nIf we first, compute regression without PCA, we have :\n\n7.1 OLS-Regression\nWe will plot the values of the calculated regression coefficients (without the intercept). On another graph, do the same with the classical least squares method.\n\nregr0 = LinearRegression()\nregr0.fit(X_train_scaled, y_train)\n\nfig = plt.figure()\nplt.plot(regr0.coef_,'o')\nplt.title(\"OLS  coefficients\")\nplt.show()\n\n\n\n\n\n\n\n\nWe observe that the coefficients are extremely large. This is due to the fact that the design matrix is rank-deficient. The OLS estimator is not stable and small changes in the data can lead to large changes in the OLS estimator. If we decide now to reduce through PCA, before compute regression, we have :\n\n\n7.2 PCA before OLS\n\nX_train_reduce = np.dot(X_train_scaled, U[:, :elbow])\nX_test_reduce = np.dot(X_test_scaled, U[:, :elbow])\n\nregr1 = LinearRegression()\nregr1.fit(X_train_reduce, y_train)\n\nfig = plt.figure()\nplt.plot(regr1.coef_, 'o')\nplt.title(f\"OLS  coefficients de PCA_before_OLS on the {elbow} first components\")\nplt.show()\n\n\n\n\n\n\n\n\nThe coefficients are now more stable. The PCA method brings some improvements compared to OLS.\n\n\n7.3 Comparison of the two methods\nFor both methods: Plot the residuals of the prediction on the test sample. Plot their density (we can use a histogram). Calculate the coefficient of determination on the test sample. Calculate the prediction risk on the test sample.\n\nR20 = regr0.score(X_test_scaled, y_test) \nR21 = regr1.score(X_test_reduce, y_test)\n\ndef MSE(y_pred, y_true):\n    return np.mean((y_pred - y_true) ** 2)\n\n\n# et le R2 ajusté ?\npred_error0 = MSE(regr0.predict(X_test), y_test)\npred_error1 = MSE(regr1.predict(X_test_reduce), y_test)\n\nprint(\"The R2 of OLS:            %.3f\" % R20)\nprint(\"The R2 of PCA before OLS: %.3f\\n\" % R21)\nprint(\"The error prediction of OLS computed on the test sample:            %.2f\" % pred_error0)\nprint(\"The error prediction of PCA before OLS computed on the test sample: %.2f\" % pred_error1)\n\nThe R2 of OLS:            -0.044\nThe R2 of PCA before OLS: 0.044\n\nThe error prediction of OLS computed on the test sample:            100712213635840763604324319232.00\nThe error prediction of PCA before OLS computed on the test sample: 5281.50\n\n\nThere are evidence that the OLS is a poor model. With a R2 = -0.044, negatif, the model is poorly fitted to the data. Even if the PCA-OLS is not good, it do better compare to the OLS.\n\neps0 = regr0.predict(X_test_scaled) - y_test\neps1 = regr1.predict(X_test_reduce) - y_test\nx=len(eps0)\nprint(x)\nplt.figure()\nplt.plot(range(x),eps0,'o',label=\"ols\")\nplt.plot(range(x),eps1,'o', label=\"pca_before_ols\")\nplt.legend(loc=1)\nplt.title(\"visualization of residuals\")\nplt.show()\n\n89\n\n\n\n\n\n\n\n\n\n\neps = pd.DataFrame(np.c_[eps0, eps1], columns=['OLS', 'PCA_before_OLS'])\neps.hist(bins=20) #histogram\nplt.show()\n\n\n\n\n\n\n\n\nIn general, the PCA method brings some improvements compared to OLS. But it depends on the chosen (random) test sample. It can happen (but it is rare here) that PCA loses information and leads to a lower prediction.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#lasso",
    "href": "posts/machineLearning3A/TP1_solution.html#lasso",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "8 LASSO",
    "text": "8 LASSO\nUsing the lassoCV function of sklearn, choose the regularization parameter of LASSO. Give the prediction risk calculated on the test. Compare it to the others.\n\nlasso = LassoCV()\nlasso.fit(X_train_scaled, y_train)\n\n\n\n\n# The estimator chose automatically its lambda:\nprint(lasso.alpha_)\n\n5.686623247060891\n\n\n\npred_error_lasso = MSE(lasso.predict(X_test_scaled), y_test)\n\n\nprint(\"Remind us the previous estimated risks.\\n\")\n\nfor method, error in zip([\"ols           \", \"pca_before_ols\", \"lasso         \"],\n                         [pred_error0, pred_error1, pred_error_lasso]):\n    print(method + \" : %.2f\" % error)\n\nRemind us the previous estimated risks.\n\nols            : 100712213635840763604324319232.00\npca_before_ols : 5281.50\nlasso          : 3386.13",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#lasso-feature-selection-and-ols",
    "href": "posts/machineLearning3A/TP1_solution.html#lasso-feature-selection-and-ols",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "9 LASSO feature selection and OLS",
    "text": "9 LASSO feature selection and OLS\n\nprint(\"Selected variables with LASSO %s \" % np.sum(lasso.coef_ &gt; 0 ))\n\n\nX_train_sel_lasso = X_train_scaled[:, lasso.coef_ &gt; 0 ]\nX_test_sel_lasso = X_test_scaled[:, lasso.coef_ &gt; 0]\n\nregr2Lasso = LinearRegression()\nregr2Lasso.fit(X_train_sel_lasso, y_train)\n\npred_error_LSLASSO = MSE(regr2Lasso.predict(X_test_sel_lasso), y_test)\n\nprint(\"MSE :\\n\"),\n\nfor method, error in zip([\"ols           \", \"pca_before_ols\", \"lasso         \", \"ls_lasso      \"],\n                         [pred_error0, pred_error1, pred_error_lasso, pred_error_LSLASSO]):\n    print(method + \" : %.2f\" % error)\n\nSelected variables with LASSO 8 \nMSE :\n\nols            : 100712213635840763604324319232.00\npca_before_ols : 5281.50\nlasso          : 3386.13\nls_lasso       : 3921.41",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/TP1_solution.html#ridge",
    "href": "posts/machineLearning3A/TP1_solution.html#ridge",
    "title": "Ordinary Least squares variants : Ridge, PCA-OLS AND LASSO",
    "section": "10 RIDGE",
    "text": "10 RIDGE\n\nridge = RidgeCV(alphas =  [.1,.2, .5, 1, 2, 5, 10, 20 , 50,100, 200, 500], cv = 10 )\nridge.fit(X_train_scaled, y_train)\nridge.alpha_ \n\n100.0\n\n\n\npred_error_ridge = MSE(ridge.predict(X_test), y_test)\n\n\n10.1 Comparison of the three methods\n\nprint(\"Remind us the previous estimated risks.\\n\")\n\nfor method, error in zip([\"ols           \", \"pca_before_ols\", \"lasso         \", \"lslasso       \", \"ridge         \"],\n                         [pred_error0, pred_error1, pred_error_lasso,pred_error_LSLASSO, pred_error_ridge]):\n    print(method + \" : %.2f\" % error)\n\nRemind us the previous estimated risks.\n\nols            : 100712213635840763604324319232.00\npca_before_ols : 5281.50\nlasso          : 3386.13\nlslasso        : 3921.41\nridge          : 3967.63\n\n\nIn this document, we try to overcome all the problems that can be encountered in the OLS method. We have seen that the PCA-OLS method, the LASSO and the RIDGE can be used to improve the prediction.\nThank you for reading. I hope that it will be useful.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP1: PCA-OLS, Ridge, Lasso"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/index.html",
    "href": "posts/machineLearning3A/index.html",
    "title": "Examen de machine learning",
    "section": "",
    "text": "Il est possible que ce soit Da Veiga qui assure vos cours, mais je vais vous offrir un aperçu de ce à quoi pourrait ressembler votre examen. C’est important, car nous avons tendance à sous-estimer ce type d’activité, surtout lorsqu’il autorise l’utilisation de générateurs de texte tels que ChatGPT. Cette année, peu d’étudiants ont achevé le projet, la charge de données étant longue et fastidieuse. Je vous conseille de vous y prendre en avance. Préparez des fonctions exécutant certaines tâches spécifiques, que je vous expliquerai progressivement.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP: Examen"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/index.html#box-plot",
    "href": "posts/machineLearning3A/index.html#box-plot",
    "title": "Examen de machine learning",
    "section": "Box plot",
    "text": "Box plot\nLe graphique ci-dessous montre la distribution de chaque variable en fonction de la variable cible.\n\n# box plot\n# Transformed Cover_Type to categorical\nY_sampled = Y_sampled.astype('category')\n\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterprétation: Nous allons nous concentrer sur la variable Elevation.\nOn peut voir que la variable Elevation est très discriminante.\nLes types de couverture 1, 2, et 7 montrent des médianes relativement élevées pour l’élévation, avec 7 ayant la médiane la plus élevée, suivie par 1 et 2.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP: Examen"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/index.html#elastic-net-regression",
    "href": "posts/machineLearning3A/index.html#elastic-net-regression",
    "title": "Examen de machine learning",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nC’est une méthode de machine learning qui combine la régression Ridge et Lasso. Elle est utilisée pour résoudre le problème de surraprentissage, la multicollinéarité et la sélection de variables.\nPassons à sa modélisation :\nNous avons utilisé ici un modèle de régression logistique avec une pénalité elasticnet. Nous avons utilisé une validation croisée pour trouver le meilleur paramètre de régularisation. Nous avons utilisé une pénalité elasticnet avec un ratio de 0.5. Nous avons utilisé un solver saga qui est adapté aux problèmes multiclasse. Nous avons utilisé une tolérance de 0.01. Nous avons utilisé un random state de 12345.\n\nclf_l1l2_LR = LogisticRegressionCV(penalty='elasticnet', l1_ratios=[0.5], \n                                   cv=5, multi_class=\"multinomial\", \n                                 solver=\"saga\",tol=0.01, random_state=12345)\n\nmodel = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\naccuracy_LR = accuracy_score(Y_test, prediction)\n\nprint(\"Accuracy of Logistic Regression :\",\"%.3f\" % accuracy_LR)\n\nAccuracy of Logistic Regression : 0.714\n\n\nJ’ai un accuracy de 0.714. Ce n’est pas mal. Nous pouvons voir la matrice de confusion.\n\n# Confusion matrix\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(Y_test, prediction)\n\n# Display the confusion matrix using Seaborn's heatmap\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\n\n\n\n\n\n\n\n\nLes valeurs sur la diagonale principale (de haut à gauche à bas à droite) représentent le nombre de prédictions correctes pour chaque classe. Par exemple, il y a 29724 prédictions correctes pour la classe 0, 44616 pour la classe 1, et ainsi de suite.\nLes valeurs hors de la diagonale indiquent les erreurs de classification. Par exemple, 11947 instances de la classe 0 ont été incorrectement prédites comme appartenant à la classe 1.\nLa classe 0 a le plus grand nombre de faux positifs, c’est-à-dire que de nombreuses instances d’autres classes ont été incorrectement prédites comme appartenant à la classe 0.\nLes classes avec le moins de prédictions incorrectes (et donc les plus sombres dans la visualisation) sont la classe 3 et la classe 6, avec respectivement 187 et 1993 prédictions correctes. Les cases avec un fond plus clair, en dehors de la diagonale, indiquent des erreurs moins fréquentes entre les classes spécifiques.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP: Examen"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/index.html#random-forest",
    "href": "posts/machineLearning3A/index.html#random-forest",
    "title": "Examen de machine learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nOOB error (Out-of-bag error)\nOOB est une méthode de validation croisée pour les forêts aléatoires. Chaque arbre dans la forêt est construit à partir d’un échantillon bootstrap du jeu de données d’entraînement. Certaines observations sont laissées de côté et non utilisées dans la construction d’un arbre donné. Ces observations “hors sac” peuvent être utilisées pour évaluer les performances de cet arbre. Du coup on peut utiliser cette méthode pour évaluer la performance de la forêt aléatoire et ajuster les hyperparamètres.\nLe code ci-dessous montre comment calculer l’erreur OOB pour un modèle de forêt aléatoire. Il permet en particulier de sélectionner la profondeur de l’arbre dans la forêt aléatoire. Le modèle de forêt aléatoire est entraîné avec une profondeur d’arbre de 10, 20 et 30. L’erreur OOB est calculée pour chaque modèle. Le modèle avec la plus petite erreur OOB est sélectionné.\n\n## Training Random Forest\n\n\n\n\ndepths = [10, 20, 30]\noob_errors = []\nmodels = []\nbest_oob_error = float('inf')\nbest_model = None\ni = 0\nfor depth in depths:\n    print(i)\n    model = RandomForestClassifier(max_depth=depth, oob_score=True, random_state=42,\n                                   n_estimators=100,  \n                                   warm_start=True  # This allows us to add more estimators later if needed\n                                  )\n    model.fit(X_train, Y_train)\n    oob_error = 1 - model.oob_score_\n    oob_errors.append(oob_error)\n    models.append(model)\n    if oob_error &lt; best_oob_error:\n        best_oob_error = oob_error\n        best_model = model\n    i = i+1\n    print(\"Done\")\n\n# Print OOB errors for each model\nfor depth, error in zip(depths, oob_errors):\n    print(f\"Depth: {depth}, OOB Error: {error}\")\n\n0\nDone\n1\nDone\n2\nDone\nDepth: 10, OOB Error: 0.21680518234371537\nDepth: 20, OOB Error: 0.05706860237215716\nDepth: 30, OOB Error: 0.038628770096964526\n\n\n\n\nAccuracy et matrice de confusion\n\n# Compute the accuracy of the best random forest model\npredictions = best_model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy of the Best Random Forest: {:.3f}\".format(accuracy))\n\n# Display the confusion matrix\nconf_matrix = confusion_matrix(Y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\nAccuracy of the Best Random Forest: 0.962\n\n\n\n\n\n\n\n\n\nNous avons un accuracy de 0.962. C’est très bien si on compare avec le modèle de régression logistique qui a un accuracy de 0.714. Nous pouvons voir la matrice de confusion.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "TP: Examen"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html",
    "href": "posts/ClimatMeteo/index.html",
    "title": "Différence entre climat et météo",
    "section": "",
    "text": "Il fera plus chaud demain, le temps était très pluvieurs hier, il fait froid aujourd’hui, les températures seront supérieures de trois degrés par rapport à la normale, etc. Ces phrases sont des exemples qui montrent que la météo est présente dans notre quotidien. Dès qu’on allume la télévision, la radio, nous voulons savoir comment nous habiller, si nous devons prendre un parapluie, etc.\nRéchauffement climatique, fonte des calottes polaires, hausse du niveau des mers, records de températures, diminution des nos gaz à effet de serre… Le climat aussi est un sujet d’actualité, il mobilise les scientifiques, les politiques, les citoyens.\nAu fond sait-on vraiment ce que c’est le climat et qu’elle est la différence entre le climat et la météo? Reprenons ces notions à la base.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#introduction",
    "href": "posts/ClimatMeteo/index.html#introduction",
    "title": "Différence entre climat et météo",
    "section": "",
    "text": "Il fera plus chaud demain, le temps était très pluvieurs hier, il fait froid aujourd’hui, les températures seront supérieures de trois degrés par rapport à la normale, etc. Ces phrases sont des exemples qui montrent que la météo est présente dans notre quotidien. Dès qu’on allume la télévision, la radio, nous voulons savoir comment nous habiller, si nous devons prendre un parapluie, etc.\nRéchauffement climatique, fonte des calottes polaires, hausse du niveau des mers, records de températures, diminution des nos gaz à effet de serre… Le climat aussi est un sujet d’actualité, il mobilise les scientifiques, les politiques, les citoyens.\nAu fond sait-on vraiment ce que c’est le climat et qu’elle est la différence entre le climat et la météo? Reprenons ces notions à la base.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#définition-de-la-météo",
    "href": "posts/ClimatMeteo/index.html#définition-de-la-météo",
    "title": "Différence entre climat et météo",
    "section": "Définition de la météo",
    "text": "Définition de la météo\nLa météo c’est le temps présent qui est caractérisé par l’ensemble des paramètres de l’atmosphère tels que la température, la pression atmosphérique, l’humidité, la vitesse et la direction du vent, la quantité de précipitations, etc. Ces paramètres peuvent être évalués à la perception; on ouvre la fenêtre, on observe le temps qu’il fait, on ressent la température, on voit s’il pleut, etc. On peut aussi les mesurer à l’aide de nombreux instruments et ainsi obtenir une description scientifique du temps qu’il fait. Par exemple, ce matin, il fait 15°C, le ciel est couvert, il pleut, le vent souffle à 20 km/h, etc. Maintenant, passons à la définition du climat.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#définition-du-climat",
    "href": "posts/ClimatMeteo/index.html#définition-du-climat",
    "title": "Différence entre climat et météo",
    "section": "Définition du climat",
    "text": "Définition du climat\nLorsqu’on parle de climat, il est toujours question de l’atmosphère et de ces interactions avec le sol mais la façon et les échelles de temps pour l’étudier sont complétement différentes. Le climat est la succession de temps qu’il fait en un lieu donné sur une très longue période de temps. Le climat est décrit selon des éléments statistiques. Par exemple, à Bruz, il pleut en moyenne 1000 mm par an, la température moyenne est de 12°C, etc.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#récapitulatif",
    "href": "posts/ClimatMeteo/index.html#récapitulatif",
    "title": "Différence entre climat et météo",
    "section": "Récapitulatif",
    "text": "Récapitulatif\nLa météo est une notion instantanée, son évolution est perceptible directement par l’humain. Le climat est une notion statistique avec une évolution incomplètement perceptible par l’humain. C’est donc grâce à un enregistement méthodique et régulier de la météo dans un même lieu et sur une longue période que l’on parvient à décrire la météo.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#illustration-par-un-exemple",
    "href": "posts/ClimatMeteo/index.html#illustration-par-un-exemple",
    "title": "Différence entre climat et météo",
    "section": "Illustration par un exemple",
    "text": "Illustration par un exemple\nConsidérons les données ci-dessous qui représentent la précipitation maximale tombée en 24 heures pour chaque mois de Janvier 1950 à Décembre 2020. Ces données sont issues de la station météorologique de Rennes.\nChaque point correspond à une donnée météorologique.\n\nimport matplotlib.pyplot as plt\n# Data Visualization Libraries:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\n\n\n\ndata = pd.read_csv('data.csv',sep=';')\ncolumns = ['AAAAMM','RRAB']\ndata = data[columns]\n\n\ndata['AAAAMM'] = pd.to_datetime(data['AAAAMM'],format='%Y%m')\n# faire un tri par date\ndata = data.sort_values(by='AAAAMM')\n# Tracer le graphe en fonction de la date\nfig,ax = plt.subplots(figsize=(10,5))\nsns.lineplot(data=data, x='AAAAMM', y='RRAB')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Précipitation maximale en 24h (mm)\")\nplt.title(\"Précipitation maximale en 24h pour chaque mois de Janvier 1950 à Juin 2021\")\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessous représente le diagramme en bâton de la précipitation maximale pour chaque mois depuis 1950 à 2022. On peut déduire que la précipitation maximale moyenne en Janvier est de 14 mm. Nous avons donc une donnée climatique.\n\nfig,ax = plt.subplots(figsize=(10,5))\n\ndata = data.assign(month=lambda x: x['AAAAMM'].dt.month)\n\n# Calculer la moyenne pour chaque mois\nmonthly_means = data.groupby('month')['RRAB'].mean()\n\n\nsns.barplot(\n    data= data,\n    \n    x='month',\n    y='RRAB',\n)\nplt.xlabel(\"Month\")\nplt.xticks(ticks = range(12), labels = [\"Jan\", \"Feb\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"]);",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#conclusion",
    "href": "posts/ClimatMeteo/index.html#conclusion",
    "title": "Différence entre climat et météo",
    "section": "Conclusion",
    "text": "Conclusion\nLa météo et le climat sont deux notions différentes. Pour terminer, il faut retenir que un orage violent, de fortes précipitations, demain il fera 25°C, le 1er juillet 2022 est 1.5°C plus chaud que la normale, etc. sont des notions météorologiques. En revanche, la temperature moyenne en Janvier est de 5°C, il pleut en moyenne 1000 mm par an, etc. sont des notions climatiques.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/substainablefinance.html",
    "href": "posts/ClimateScenario/substainablefinance.html",
    "title": "Understanding Sustainable Finance and its Regulations",
    "section": "",
    "text": "Sustainable finance is a team that can be difficult to define. So what exactly should we understand when we hear the term sustainable finance? To grasp The concept we need to make the difference between traditional finance and sustainable finance.\n\n\nTraditional finance targets economic performance by taking the account purely financial criteria. The main goal here is to maximize the profitability, the share price or growth prospects.\nSustainable finance, on the other hand, takes into account extra-financial criteria. These criteria are environmental, social and governance (ESG) criteria. The goal is to reconcile economic performance with positive social and environmental impacts to meet the goals of sustainable development.\n\n\n\nThe ESG criteria are the three pillars of sustainable finance. Let’s start with the environmental criteria. Their role is to measure the direct or indirect impact of the activity of an economic entity on the environment. For example, it might be to analyze a company’s CO2 emissions, its energy consumptions or the recycling of the waste it produces.\nThen there are the social criteria. They aim to measure the performance of an economic entity on social issues. For example, it might be to analyze the diversity of the workforce, the health and safety of employees, parity or the number of employees with disabilities.\nFinally, the governance criteria assess the way in which the economic entity is managed, administered and monitored. This involves for instance, analyzing the composition of the board of directors, the remuneration of the management, the transparency of the information provided or the fight against corruption."
  },
  {
    "objectID": "posts/ClimateScenario/substainablefinance.html#definition-of-sustainable-finance-and-esg-criteria.",
    "href": "posts/ClimateScenario/substainablefinance.html#definition-of-sustainable-finance-and-esg-criteria.",
    "title": "Understanding Sustainable Finance and its Regulations",
    "section": "",
    "text": "Sustainable finance is a team that can be difficult to define. So what exactly should we understand when we hear the term sustainable finance? To grasp The concept we need to make the difference between traditional finance and sustainable finance.\n\n\nTraditional finance targets economic performance by taking the account purely financial criteria. The main goal here is to maximize the profitability, the share price or growth prospects.\nSustainable finance, on the other hand, takes into account extra-financial criteria. These criteria are environmental, social and governance (ESG) criteria. The goal is to reconcile economic performance with positive social and environmental impacts to meet the goals of sustainable development.\n\n\n\nThe ESG criteria are the three pillars of sustainable finance. Let’s start with the environmental criteria. Their role is to measure the direct or indirect impact of the activity of an economic entity on the environment. For example, it might be to analyze a company’s CO2 emissions, its energy consumptions or the recycling of the waste it produces.\nThen there are the social criteria. They aim to measure the performance of an economic entity on social issues. For example, it might be to analyze the diversity of the workforce, the health and safety of employees, parity or the number of employees with disabilities.\nFinally, the governance criteria assess the way in which the economic entity is managed, administered and monitored. This involves for instance, analyzing the composition of the board of directors, the remuneration of the management, the transparency of the information provided or the fight against corruption."
  },
  {
    "objectID": "posts/ClimateScenario/substainablefinance.html#the-3-tools-of-sustainable-finance.",
    "href": "posts/ClimateScenario/substainablefinance.html#the-3-tools-of-sustainable-finance.",
    "title": "Understanding Sustainable Finance and its Regulations",
    "section": "2 The 3 tools of sustainable finance.",
    "text": "2 The 3 tools of sustainable finance.\nIn practice, sustainable finance use 3 tools to implement the ESG criteriaI : “Socially responsible investment” (SRI), “Green finance” and “Solidarity-based finance”\nSocially responsible investment (SRI) is an investment strategy based on the integration of ESG criteria into the selection of financial assets. Investing in the renewable energy sector or excluding companies that do not respect human rights are examples of SRI.\nGreen finance covers actions and financial operations that promote the energy and ecological transition and the fight against climate change. This includes green bonds, green loans or green funds in particular.\nSolidarity-based finance is a form of finance that aims to finance projects designed to fight against social exclusion or promote social integration. This includes microcredit, social entrepreneurship or ethical banking.\nThank you for reading and see you soon!I"
  },
  {
    "objectID": "posts/ClimateScenario/globalWarning.html",
    "href": "posts/ClimateScenario/globalWarning.html",
    "title": "UNDERSTANDING THE ECOLOGICAL CRISIS",
    "section": "",
    "text": "The world is currently experiencing a major ecological crisis, a main aspect of which is global warming. Before argue about ecological crisis, First, you need to know what is global warming.\nGlobal warming is linked to the greenhouse effect. In the atmosphere, Greenhouse gases trap the infrared radiation emitted by the Earth once the sun has warmed its surface. This phenomenon is natural and necessary for life on Earth as it keep the planet warm enough to sustain life. However, since the industrial revolution, human activity have increased the concentration of greenhouse gases in the atmosphere, leading to an increase in the greenhouse effect and therefore global warming. This is known as anthropogenic global warming. Amounts the greenhouse gasses we can find carbon dioxide, methane, nitrous oxide, and fluorinated gases."
  },
  {
    "objectID": "posts/ClimateScenario/globalWarning.html#what-is-global-warming",
    "href": "posts/ClimateScenario/globalWarning.html#what-is-global-warming",
    "title": "UNDERSTANDING THE ECOLOGICAL CRISIS",
    "section": "",
    "text": "The world is currently experiencing a major ecological crisis, a main aspect of which is global warming. Before argue about ecological crisis, First, you need to know what is global warming.\nGlobal warming is linked to the greenhouse effect. In the atmosphere, Greenhouse gases trap the infrared radiation emitted by the Earth once the sun has warmed its surface. This phenomenon is natural and necessary for life on Earth as it keep the planet warm enough to sustain life. However, since the industrial revolution, human activity have increased the concentration of greenhouse gases in the atmosphere, leading to an increase in the greenhouse effect and therefore global warming. This is known as anthropogenic global warming. Amounts the greenhouse gasses we can find carbon dioxide, methane, nitrous oxide, and fluorinated gases."
  },
  {
    "objectID": "posts/ClimateScenario/globalWarning.html#the-impacts-of-human-activities-on-global-warming",
    "href": "posts/ClimateScenario/globalWarning.html#the-impacts-of-human-activities-on-global-warming",
    "title": "UNDERSTANDING THE ECOLOGICAL CRISIS",
    "section": "2 The impacts of human activities on global warming",
    "text": "2 The impacts of human activities on global warming\nAlmost all human activities emit greenhouse gases. The main sources of these gases are the burning of fossil fuels (coal, oil, and natural gas) for energy production, transportation, and heating, as well as deforestation and intensive agriculture.\nFurthermore, our activity often reduce the capacity of ecosystems to absorb these gases. Forests, soils and oceans are the main natural carbon sinks(puits de carbone). Their degradation through deforestation, urbanization, agriculture or pollution reduce their capacity to absorb CO2."
  },
  {
    "objectID": "posts/ClimateScenario/globalWarning.html#the-consequences-of-global-warming",
    "href": "posts/ClimateScenario/globalWarning.html#the-consequences-of-global-warming",
    "title": "UNDERSTANDING THE ECOLOGICAL CRISIS",
    "section": "3 The consequences of global warming",
    "text": "3 The consequences of global warming\nIf the train continues, the consequence will be dramatic. In 2019 the Eearth’s temperature was already one degree warmer than in the pre-industrial era. This may seem negligible, but it is enoromous : The last ice age, the average temperature on Earth was only five degrees lower than today. A few degrees can radically transform the climate.\nA rise of one degree can profoundly alter the balance of ecosystems and increase the frequency of extreme weather events like droughts, floods, cyclones.\nClimate change can also affect agricultural productivity and industrial sectors’ supply of primary materials, As well as decreasing the economy as whole. Climate change affects the overalll health and safety of populations : Natural disasters, migrations, conflicts over access to resources,etc.\nThe consequences of global warming are already visible. To face them, we must starting today, aim for carbon neutrality : not emitting more greenhouse gases than we can absorb. This is the only way to limit the rise in temperature to 1.5 degrees, as recommended by the IPCC."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Menu",
    "section": "",
    "text": "Courbe de taux et valorisation d’obligations\n\n\n\n\n\nLors de la valorisation d’une obligation, il est essentiel de considérer des éléments clés tels que le prix sale (dirty price) et le prix net (clean price), lesquels dépendent de facteurs variés incluant le taux d’intérêt, la maturité et le taux de coupon. Dans cette analyse, nous avons généré la courbe des taux spot en utilisant des obligations zéro coupon et évalué le prix d’une obligation sous l’hypothèse d’une courbe de taux déterministe. Par la suite, nous avons exploré un modèle simplifié de Hull et White, examiné les spécificités d’une obligation munie d’une clause de rappel et discuté des stratégies de couverture contre le risque de taux d’intérêt. \n\n\n\n\n\nJunior Jumbong\n\n\n\n\n\n\n\n\n\n\n\n\nModélisation de la value at risk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUNDERSTANDING THE ECOLOGICAL CRISIS\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nsimple tool in R to analyze data : Graph representationExecution\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nData Schiller : CAPE Ratio\n\n\n\n\n\n\nMarket\n\n\nStock\n\n\n\n\n\n\n\n\n\nNov 16, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinary Least squares variants : Ridge, PCA-OLS AND LASSO\n\n\n\n\n\n\nMachine Learning\n\n\nOLS-Regression\n\n\nPCA\n\n\nLasso\n\n\nRidge\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nThe P-value under the Bootstrap Method with python\n\n\n\n\n\n\np-value\n\n\nbootstrap\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Risk Measures\n\n\n\n\n\n\nClimate Risk\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nData retrieval from WEB API with Python\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nNotions clés pour comprendre le réchauffement climatique\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nDifférence entre climat et météo\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMaitrise des données financières avec Python\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning QCM\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Sustainable Finance and its Regulations\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nTBATS : N’a pas de contraintes de saisonnalité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nPSI and CRAMER’S V\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nLeadership : Pouvoir, autorité et légitimité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMarket risk generalities\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation of a real and continuous random variable\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nIA : Eléments clés du premier sommet mondial\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ClimateScenario/index.html",
    "href": "posts/ClimateScenario/index.html",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "",
    "text": "This document is primarily intended for second year students at ENSAI in general, and for those who are particularly interested in biostatistics. When we deal with data, an easy way to understand our work is to create a graph. This gives us an intuition about what we are doing. The advice to follow is to choose simplicity when using graphs and to favor those that everyone knows how to interpret. So when you create a graph, think about your audience and not your personal preferences.\nThere are also many concerns that we use every day to take decision and we don’t know what they mean. For example the p-value. In this document, we will go through those concepts and explain them in a simple way.",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#introduction",
    "href": "posts/ClimateScenario/index.html#introduction",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "",
    "text": "This document is primarily intended for second year students at ENSAI in general, and for those who are particularly interested in biostatistics. When we deal with data, an easy way to understand our work is to create a graph. This gives us an intuition about what we are doing. The advice to follow is to choose simplicity when using graphs and to favor those that everyone knows how to interpret. So when you create a graph, think about your audience and not your personal preferences.\nThere are also many concerns that we use every day to take decision and we don’t know what they mean. For example the p-value. In this document, we will go through those concepts and explain them in a simple way.",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#presentation-of-data.",
    "href": "posts/ClimateScenario/index.html#presentation-of-data.",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "2 Presentation of data.",
    "text": "2 Presentation of data.\nThe data used in this document come from ParisSaclay Introduction à la statistique avec R - Session 21 if the mooc is available when you will read this document. The dataset has 799 observations and 26 variables. One observation represents a prisoner, and variables correspond to the mental health status of the prisoner.\n\n## import library for readcsv\nlibrary(readr)\n\n## import data\nsetwd(\"C:/Users/johns/OneDrive/Bureau/code/blog/ensai/posts/ClimateScenario\")\n\ngetwd()\n\n[1] \"C:/Users/johns/OneDrive/Bureau/code/blog/ensai/posts/ClimateScenario\"\n\nsmp2 &lt;- read_delim(\"data/smp2.csv\", delim = \";\",  \n    escape_double = FALSE, trim_ws = TRUE,show_col_types = FALSE)\n\nstr(smp2)\n\nspc_tbl_ [799 × 26] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ age         : num [1:799] 31 49 50 47 23 34 24 52 42 45 ...\n $ prof        : chr [1:799] \"autre\" NA \"prof.intermediaire\" \"ouvrier\" ...\n $ duree       : num [1:799] 4 NA 5 NA 4 NA NA 5 4 NA ...\n $ discip      : num [1:799] 0 0 0 0 1 0 0 0 1 0 ...\n $ n.enfant    : num [1:799] 2 7 2 0 1 3 5 2 1 2 ...\n $ n.fratrie   : num [1:799] 4 3 2 6 6 2 3 9 12 5 ...\n $ ecole       : num [1:799] 1 2 2 1 1 2 1 2 1 2 ...\n $ separation  : num [1:799] 0 1 0 1 1 0 1 0 1 0 ...\n $ juge.enfant : num [1:799] 0 0 0 0 NA 0 1 0 1 0 ...\n $ place       : num [1:799] 0 0 0 1 1 0 1 0 0 0 ...\n $ abus        : num [1:799] 0 0 0 0 0 0 0 0 1 1 ...\n $ grav.cons   : num [1:799] 1 2 2 1 2 1 5 1 5 5 ...\n $ dep.cons    : num [1:799] 0 0 0 0 1 0 1 0 1 0 ...\n $ ago.cons    : num [1:799] 1 0 0 0 0 0 0 0 0 0 ...\n $ ptsd.cons   : num [1:799] 0 0 0 0 0 0 0 0 0 0 ...\n $ alc.cons    : num [1:799] 0 0 0 0 0 0 0 0 1 1 ...\n $ subst.cons  : num [1:799] 0 0 0 0 0 0 1 0 1 0 ...\n $ scz.cons    : num [1:799] 0 0 0 0 0 0 0 0 0 0 ...\n $ char        : num [1:799] 1 1 1 1 1 1 1 1 4 1 ...\n $ rs          : num [1:799] 2 2 2 2 2 1 3 2 3 2 ...\n $ ed          : num [1:799] 1 2 3 2 2 2 3 2 3 2 ...\n $ dr          : num [1:799] 1 1 2 2 2 1 2 2 1 2 ...\n $ suicide.s   : num [1:799] 0 0 0 1 0 0 3 0 4 0 ...\n $ suicide.hr  : num [1:799] 0 0 0 0 0 0 1 0 1 0 ...\n $ suicide.past: num [1:799] 0 0 0 0 1 0 1 0 1 0 ...\n $ dur.interv  : num [1:799] NA 70 NA 105 NA NA 105 84 78 60 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   age = col_double(),\n  ..   prof = col_character(),\n  ..   duree = col_double(),\n  ..   discip = col_double(),\n  ..   n.enfant = col_double(),\n  ..   n.fratrie = col_double(),\n  ..   ecole = col_double(),\n  ..   separation = col_double(),\n  ..   juge.enfant = col_double(),\n  ..   place = col_double(),\n  ..   abus = col_double(),\n  ..   grav.cons = col_double(),\n  ..   dep.cons = col_double(),\n  ..   ago.cons = col_double(),\n  ..   ptsd.cons = col_double(),\n  ..   alc.cons = col_double(),\n  ..   subst.cons = col_double(),\n  ..   scz.cons = col_double(),\n  ..   char = col_double(),\n  ..   rs = col_double(),\n  ..   ed = col_double(),\n  ..   dr = col_double(),\n  ..   suicide.s = col_double(),\n  ..   suicide.hr = col_double(),\n  ..   suicide.past = col_double(),\n  ..   dur.interv = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nFor our analysis, we will use four variables :\n\nage : numerical variable (the age of the prisoner in years)\nn.enfant : numerical variable (the number of children)\ndep.cons : Existence of a depressive disorder (0 = no, 1 = yes)\nprof : prisoner’s profession (1 = unemployed, 2 = farmer, 3 = independent, 4 = executive, 5 = employee, 6 = worker, 7 = retired, 8 = student).\n\nThe first thing we want to do is to analyze the distribution of each variable.",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#distribution-of-categorical-variables-profession.",
    "href": "posts/ClimateScenario/index.html#distribution-of-categorical-variables-profession.",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "3 Distribution of categorical variables: profession.",
    "text": "3 Distribution of categorical variables: profession.\n\n3.1 Barplot\nThe graphic tool allow us to represent a distribution of categorical variables is the barplot. Before we built the distribution of variable, we have an amazing tool in our hands to summarize the data. It is the table function. This function allows us to count the number of occurrences of each category in a variable.\n\n## count the number of occurrences of each category in a variable\ntable(smp2$prof)\n\n\n       agriculteur            artisan              autre              cadre \n                 6                 90                 31                 24 \n           employe            ouvrier prof.intermediaire        sans emploi \n               135                227                 58                222 \n\n\nThe result of the table function is a vector. We can use this vector to create a barplot.\n\n## create a barplot\nbarplot(table(smp2$prof), main = \"Distribution of profession\", xlab = \"Profession\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n3.2 Pie chart\nAnother way to represent the distribution of the category variable is the pie chart. The pie chart is a circle divided into sectors. Each sector represents a category of the variable. The size of the sector is proportional to the frequency of the category. Statistics don’t like the pie chart because it is difficult to compare the size of the sectors. However, it is a good way to represent the distribution of a variable when the number of categories is small.\n\n## create a pie chart and put the legend on the right and the percentage\npie(table(smp2$dep.cons), main = \"Distribution of depressive disorder\", col = c(\"red\", \"blue\"), labels = c(\"No\", \"Yes\"), cex = 0.8)",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#distribution-of-numerical-variables-age",
    "href": "posts/ClimateScenario/index.html#distribution-of-numerical-variables-age",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "4 Distribution of numerical variables: age",
    "text": "4 Distribution of numerical variables: age\n\n4.1 Histogram\nThe histogram is a graphical representation of the distribution of a numerical variable. Unlike barplot, We don’t need to use the table function to create an histogram.\n\n## create an histogram\nhist(smp2$age, main = \"Distribution of age\", xlab = \"Age\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nWe can also plot in the same graphic the density of the variable. The density is the probability of the variable.\n\n## create an histogram with density\nhist(smp2$age, main = \"Distribution of age\", xlab = \"Age\", ylab = \"Density\", freq = FALSE)\nlines(density(smp2$age,na.rm =T), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n4.2 Boxplot\nThe boxplot is a graphical representation of the distribution of a numerical variable. The boxplot is composed of a box and two whiskers. The box represents the interquartile range (IQR). The whiskers represent the minimum and maximum values of the variable. The boxplot is a good way to detect outliers in the data.\n\n## create a boxplot\nboxplot(smp2$age, main = \"Distribution of age\", xlab = \"Age\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n4.3 Association between categorical and numerical variables\nThe boxplot is a good way to represent the distribution of a numerical variable according to a categorical variable.\n\n## create a boxplot\nboxplot(smp2$age ~ smp2$dep.cons, main = \"Distribution of age according to depressive disorder\", xlab = \"Depressive disorder\", ylab = \"Age\")",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#association-between-two-numerical-variables",
    "href": "posts/ClimateScenario/index.html#association-between-two-numerical-variables",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "5 Association between two numerical variables",
    "text": "5 Association between two numerical variables\nWhen you deal with two numerical variables.The best way to represent the association between them is the scatter plot.\n\n## create a scatter plot\nplot(smp2$age, smp2$n.enfant, main = \"Association between age and number of children\", xlab = \"Age\", ylab = \"Number of children\")\n\n\n\n\n\n\n\n\nAs you see in this graphic, it seem like all the points are not represented. This is because the number of point is too large. R have a good function to deal with the problems : jitter. This function add a little noise to the data to avoid the overplotting.\n\n## create a scatter plot\nplot(jitter(smp2$age), jitter(smp2$n.enfant), main = \"Association between age and number of children\", xlab = \"Age\", ylab = \"Number of children\")",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#association-between-two-categorical-variables",
    "href": "posts/ClimateScenario/index.html#association-between-two-categorical-variables",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "6 Association between two categorical variables",
    "text": "6 Association between two categorical variables\nThe best way to represent the association between two categorical variables is to use a contingency table. A contingency table displays the frequency or count of observations that fall into each combination of the categories of the two variables. We can use the table of contingency to create a barplot.\n\n## create a contingency table\ntable(smp2$dep.cons, smp2$prof)\n\n   \n    agriculteur artisan autre cadre employe ouvrier prof.intermediaire\n  0           4      62    22    16      79     131                 31\n  1           2      28     9     8      56      96                 27\n   \n    sans emploi\n  0         133\n  1          89\n\n## create a barplot\nbarplot(table(smp2$dep.cons, smp2$prof), main = \"Association between depressive disorder and profession\", xlab = \"Depressive disorder\", ylab = \"Frequency\")",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#the-link-between-variable.",
    "href": "posts/ClimateScenario/index.html#the-link-between-variable.",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "7 The link between variable.",
    "text": "7 The link between variable.\n\n7.1 Correlation between two numerical variables\nThere are many ways to study the correlation between two numerical variables. The most common ways are the Pearson correlation coefficient and the Spearman correlation. The Pearson correlation coefficient is used when the two variables are normally distributed. The Spearman correlation is used when the two variables are not normally distributed. We can not use those tools if Variables have a missing values.\n\n## Pearson correlation\ncor(smp2$age, smp2$n.enfant, use = \"complete.obs\")\n\n[1] 0.4326039\n\n## Spearman correlation\ncor(smp2$age, smp2$n.enfant, method = \"spearman\", use = \"complete.obs\")\n\n[1] 0.4243662",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#temporal-evolution-of-a-numerical-variable",
    "href": "posts/ClimateScenario/index.html#temporal-evolution-of-a-numerical-variable",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "8 Temporal evolution of a numerical variable",
    "text": "8 Temporal evolution of a numerical variable\nI will finish by showing how to represent the evolution of the mean of a numerical variable over time. This is very useful In biostatistic When we want to show the evolution of a disease over time. I will use the package gplots to create the graph. You can use ggplot2 to create the same graph. We will use a temporal data outils_hdrs.csv. The data have 3 variables : - NUMERO : the id of the patient - VISIT : the visit of the patient - HDRS : the score of the patient\n\nhdrs &lt;- read_csv2(\"data/outils_hdrs.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 1053 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\ndbl (3): NUMERO, VISIT, HDRS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(hdrs)\n\nspc_tbl_ [1,053 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ NUMERO: num [1:1053] 96 96 96 96 96 96 96 96 157 157 ...\n $ VISIT : num [1:1053] 0 4 7 14 21 28 42 56 0 4 ...\n $ HDRS  : num [1:1053] 34 26 12 7 5 1 1 1 27 19 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   NUMERO = col_double(),\n  ..   VISIT = col_double(),\n  ..   HDRS = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n# install packages\n\n#install.packages(\"gplots\")  \n\nlibrary(gplots)  \n\n### Representation of the evolution of the mean of the HDRS score over time\nplotmeans(hdrs$HDRS~hdrs$VISIT,gap=0,barcol=\"black\",main=\"Evolution moyenne du HDRS dans le temps\",xlab=\"Visite\",ylab=\"HDRS moyen\")\n\n\n\n\n\n\n\n\nTo compute the same graph with ggplot2, you need to compute the mean, the first quartile and the third quartile of the HDRS score for each visit.\n\nlibrary(dplyr)\nhdrs_mean &lt;- hdrs %&gt;% \n  group_by(VISIT) %&gt;%\n  summarise(mean = mean(HDRS,na.rm =T),\n            lower = quantile(HDRS,0.25,na.rm = T),\n            upper = quantile(HDRS,0.75,na.rm = T))\nhead(hdrs_mean)\n\n# A tibble: 6 × 4\n  VISIT  mean lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  27.9  25      30\n2     4  25.2  23      28\n3     7  20.6  16      25\n4    14  16.7  12      22\n5    21  14.3   8      20\n6    28  11.5   6.5    16\n\n\nNow we can create the graph with ggplot2.\n\nlibrary(ggplot2)\nggplot(hdrs_mean, aes(x = VISIT, y = mean)) +\n  geom_point(size = 3) +\n  geom_line(size = 0) +\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=1)+\n  scale_x_continuous(expand=c(0,0), breaks=c(0,4,7,14,21,28,42,56)) +\n  geom_vline(xintercept = 21, linetype=\"dashed\",\n               color = \"black\", size=1) +\n    theme_classic() +\n    theme(axis.line.x = element_line(color=\"black\", size = 0.5),\n          axis.line.y = element_line(color=\"black\", size = 0.5)) +\n    theme(axis.text=element_text(size=12, face=\"bold\"),\n          legend.title=element_blank(), legend.key = element_blank()) +\n    theme(axis.text=element_text(size=12), axis.title=element_text(size=12)) +\n    xlab(\"VISIT\")+\n    ylab(\"The mean score of the patient\") +\n    theme(legend.position = c(1,1)) +\n    theme(plot.margin = margin(1, 1, 1, 0.5, \"cm\"))\n\n\n\n\n\n\n\n\nLet’s explain each part of this graphic :\n\nThe ggplot(hdrs_mean, aes(x = VISIT, y = mean)) + function creates the graph. The VISIT variable is on the x-axis and the mean variable is on the y-axis.\nThe geom_point(size = 3) + function creates the points of the graph.\nThe geom_line(size = 0) + function creates the line of the graph.\nThe geom_errorbar(aes(ymin=lower, ymax=upper), width=1) + function creates the error bars of the graph.\nThe scale_x_continuous(expand=c(0,0), breaks=c(0,4,7,14,21,28,42,56)) + function sets the x-axis.\nThe geom_vline(xintercept = 21, linetype=“dashed”, color = “black”, size=1) + function creates a vertical line at the 21st visit.\nThe theme_classic() + function sets the theme of the graph.\nThe theme(axis.line.x = element_line(color=“black”, size = 0.5), axis.line.y = element_line(color=“black”, size = 0.5)) + function sets the color and size of the axis lines.\nThe theme(axis.text=element_text(size=12, face=“bold”), legend.title=element_blank(), legend.key = element_blank()) + function sets the size and style of the axis text.\nThe theme(axis.text=element_text(size=12), axis.title=element_text(size=12)) + function sets the size of the axis text.\nThe xlab(“VISIT”)+ ylab(“The mean score of the patient”) + function sets the labels of the x-axis and y-axis.\nThe theme(legend.position = c(1,1)) + function sets the position of the legend.\nThe theme(plot.margin = margin(1, 1, 1, 0.5, “cm”)) + function sets the margins of the graph.",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimateScenario/index.html#conclusion",
    "href": "posts/ClimateScenario/index.html#conclusion",
    "title": "simple tool in R to analyze data : Graph representation, p-value, test, odds ratio, etc.",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nI will finish here And I hope that you will enjoy it. I will continue to write about the p-value, the test, the odds ratio, etc. in the next document.",
    "crumbs": [
      "About",
      "climat",
      "simple tool in R to analyze data : Graph representationExecution"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/climateRiskMeasure.html",
    "href": "posts/ClimatMeteo/climateRiskMeasure.html",
    "title": "Climate Risk Measures",
    "section": "",
    "text": "This paper is for those who believe in the critical importance of sustainability and a future with low carbon emissions. That’s why I published it on Medium. I hope that it will be helpful to those who are interested in the integration of climate risk into traditional business models. When I was working on my final year project, I was looking for a way to integrate climate risk into asset allocation models. As I reviewed the literature, I read many articles.I think it’s important to understand climate risk metrics before making a complicated decision model.\nWhen working with a portfolio of a n asset, The concern of investor is to determine the weight \\(w=(w_1,...w_n)\\) of each asset in the portfolio. The investor wants to know how much invest in each asset. Answering this question is the same as determining what weight assign to each asset.\nThe traditional portfolio optimization responds to this question. It requires input data such as the vector of expected returns \\((\\mu_1,...,\\mu_n)\\) of asset, the vector \\(\\sigma\\) of asset volatilities and the correlation matrix \\(\\rho\\) of asset returns. We can then compute the first and second moment of the stochastics portfolio return. In particular, the portfolio risk corresponds to the portfolio volatility \\(\\sigma(x)\\). To be more precise, using the parameters \\(\\sigma\\) and \\(\\rho\\), we can compute the portfolio covariance matrix \\(\\Sigma\\), which is defined as follows: \\(\\Sigma_{ij} = \\sigma_i \\sigma_j \\rho_{ij}\\). The first moment is equal to the expected return of the portfolio \\(\\mu(x) = \\sum_{i=1}^{n} w_i \\mu_i\\) While the second is equal to the variance of the portfolio returns \\(\\sigma(x) = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{n} w_i w_j \\Sigma_{ij}}\\).\nWhen introducing climate-related risk, we have to define another measure \\(C(w_1,...w_n)\\) that assesses the risk of climate change. Let consider a climate metrics \\(C_i\\) of associated with asset \\(i\\). The nature of the climate risk is then different from the volatility risk measure since the latter satisfy the sod additivity property : \\[\\sigma(w) &lt;= \\sum_{i=1}^{n} w_i \\sigma_i\\]\nEven if \\(C(w)\\) is a convex risk measure, it is an abuse of language, because there is no way to diversify the climate risk: \\[C(w) \\not &lt; \\sum_{i=1}^{n} w_i C_i\\]\nTherefore, \\(C(w)\\) play more the rule of an expected loss than a risk measure. In this paper, we will present some climate risk measures. First, we will present carbon footprint and carbon intensity."
  },
  {
    "objectID": "posts/ClimatMeteo/climateRiskMeasure.html#introduction",
    "href": "posts/ClimatMeteo/climateRiskMeasure.html#introduction",
    "title": "Climate Risk Measures",
    "section": "",
    "text": "This paper is for those who believe in the critical importance of sustainability and a future with low carbon emissions. That’s why I published it on Medium. I hope that it will be helpful to those who are interested in the integration of climate risk into traditional business models. When I was working on my final year project, I was looking for a way to integrate climate risk into asset allocation models. As I reviewed the literature, I read many articles.I think it’s important to understand climate risk metrics before making a complicated decision model.\nWhen working with a portfolio of a n asset, The concern of investor is to determine the weight \\(w=(w_1,...w_n)\\) of each asset in the portfolio. The investor wants to know how much invest in each asset. Answering this question is the same as determining what weight assign to each asset.\nThe traditional portfolio optimization responds to this question. It requires input data such as the vector of expected returns \\((\\mu_1,...,\\mu_n)\\) of asset, the vector \\(\\sigma\\) of asset volatilities and the correlation matrix \\(\\rho\\) of asset returns. We can then compute the first and second moment of the stochastics portfolio return. In particular, the portfolio risk corresponds to the portfolio volatility \\(\\sigma(x)\\). To be more precise, using the parameters \\(\\sigma\\) and \\(\\rho\\), we can compute the portfolio covariance matrix \\(\\Sigma\\), which is defined as follows: \\(\\Sigma_{ij} = \\sigma_i \\sigma_j \\rho_{ij}\\). The first moment is equal to the expected return of the portfolio \\(\\mu(x) = \\sum_{i=1}^{n} w_i \\mu_i\\) While the second is equal to the variance of the portfolio returns \\(\\sigma(x) = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{n} w_i w_j \\Sigma_{ij}}\\).\nWhen introducing climate-related risk, we have to define another measure \\(C(w_1,...w_n)\\) that assesses the risk of climate change. Let consider a climate metrics \\(C_i\\) of associated with asset \\(i\\). The nature of the climate risk is then different from the volatility risk measure since the latter satisfy the sod additivity property : \\[\\sigma(w) &lt;= \\sum_{i=1}^{n} w_i \\sigma_i\\]\nEven if \\(C(w)\\) is a convex risk measure, it is an abuse of language, because there is no way to diversify the climate risk: \\[C(w) \\not &lt; \\sum_{i=1}^{n} w_i C_i\\]\nTherefore, \\(C(w)\\) play more the rule of an expected loss than a risk measure. In this paper, we will present some climate risk measures. First, we will present carbon footprint and carbon intensity."
  },
  {
    "objectID": "posts/ClimatMeteo/climateRiskMeasure.html#carbon-footprint",
    "href": "posts/ClimatMeteo/climateRiskMeasure.html#carbon-footprint",
    "title": "Climate Risk Measures",
    "section": "2 Carbon Footprint",
    "text": "2 Carbon Footprint\nCarbon footprint is an indicator of the amount of greenhouse gas(GHG) emissions caused by human activities. Greenhouse gases absorb and emit radiation energy emitted by the Earth once the sun has warmed its surface, causing the greenhouse effect. The most common greenhouse gases are carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). It’s important to remind that the greenhouse effect Was crucial for the development of human life on Earth. Without it, the average temperature of the Earth would be -18°C. With the greenhouse effect, the current temperature of Earth’s surface is about +15°C. However, the increase in greenhouse gas emissions due to human activities has led to an increase in the greenhouse effect, which has caused global warming.\nCarbon footprint is expressed in CO2 equivalent (CO\\(_2\\)e), which is a term for describing different GHGs in a common unit. In another word, a quantity of GHG is express as C0\\(_2\\)e By multiplying the GHG amount by its global warming potential(GWP).The GWP of a gas is the amount of a gas that would cause the same amount of warming as a CO2 over a specific time period. For example, the IPCC’s \\(4^{th}\\) assessment report has used the following rules : 1 kg of methane corresponds to 25 kg of CO_\\(2\\) and 1 kg of nitrous oxide corresponds to 298 kg of CO\\(_2\\).\nThis indicator allows to properly compare companies in the same sector or have the same characteristics. If the size of companies are not the same, it is better to use normalize indicator like carbon intensity.\nTo provide a common measure That can be used by all companies and to limit bias. The Greenhouse gas protocol has defined three scopes of carbon emissions.\n### The three scopes of carbon emissions\n\nScope 1 refers to direct emissions from sources that are owned or controlled by the company. For example, we can mention emissions from combustion in owned or controlled boilers, furnaces, vehicles, and air conditioning systems.\nScope 2 denotes indirect emissions from the generation of purchased electricity, steam, heating, and cooling consumed by the company. For example, we can mention emissions from the generation of electricity purchased and used by the company.\nScope 3 includes all other indirect emissions that occur in a company’s value chain. For example, we can mention emissions from the extraction and production of purchased materials and fuels, transport-related activities in vehicles not owned or controlled by the reporting entity, electricity-related activities not covered in Scope 2, outsourced activities, waste disposal, etc.\n\nIn what follows, we distinguish these three carbon emission measures by introducing the notations CE1, CE2 and CE3. Moreover, they are generally expressed in tons of carbon dioxide equivalent or tCO\\(_2\\)e. Now we have a tool to construt our carbon metrics\n\n2.1 Carbon metrics\nThe carbon metrics that is generally used to compare entities or companies is the carbon Intensity. The carbon intensity of company i with respect to scope j is defined as the ratio of the carbon emissions of scope j to the activity of the company. The carbon intensity of company i with respect to scope j is defined as follows:\n\\[CI_{ij} = \\frac{CE_{ij}}{Y_i}\\]\nwhere \\(CE_{ij}\\) is the carbon emissions of company i with respect to scope j and \\(Y_i\\) is the activity of company i. The carbon intensity of company i is then defined as the sum of the carbon intensity of each scope:\nwhere Y is an indicator measuring the activity of the company. For example, it can be the revenue, the number of employees, the number of products sold, etc. To apply this, we will you the data of the company. The data below comes from Trucost reporting year 2019.\n\nimport pandas as pd\ncarbon_footprint = pd.read_csv('carbon_footprint.csv', index_col=0)\ncarbon_footprint.head()\n\n\n\n\n\n\n\n\nCompany\nEmission Scope 1\nEmission Scope 2\nEmission Scope 3\nRevenue\n\n\n\n\n0\nAlphabet\n74462.0\n5116949\n7166240\n161857\n\n\n1\nAmazon\n5760000.0\n5500000\n20054722\n280522\n\n\n2\nApple\n50463.0\n862127\n27618943\n260174\n\n\n3\nBP\n49199999.0\n5200000\n103840194\n276850\n\n\n4\nDanone\n722122.0\n944877\n28969780\n28308\n\n\n\n\n\n\n\nTo compute the carbon intensity, we can first compute the carbon intensity for each scope and then sum them up.\n\ncarbon_footprint['CE1'] = carbon_footprint['Emission Scope 1'] / carbon_footprint['Revenue']\ncarbon_footprint['CE2'] = carbon_footprint['Emission Scope 2'] / carbon_footprint['Revenue']\ncarbon_footprint['CE3'] = carbon_footprint['Emission Scope 3'] / carbon_footprint['Revenue']\ncarbon_footprint['CI'] = carbon_footprint['CE1'] + carbon_footprint['CE2'] + carbon_footprint['CE3']\ncarbon_footprint.head()\n\n\n\n\n\n\n\n\nCompany\nEmission Scope 1\nEmission Scope 2\nEmission Scope 3\nRevenue\nCE1\nCE2\nCE3\nCI\n\n\n\n\n0\nAlphabet\n74462.0\n5116949\n7166240\n161857\n0.460048\n31.614011\n44.275132\n76.349191\n\n\n1\nAmazon\n5760000.0\n5500000\n20054722\n280522\n20.533149\n19.606305\n71.490728\n111.630182\n\n\n2\nApple\n50463.0\n862127\n27618943\n260174\n0.193959\n3.313655\n106.155661\n109.663275\n\n\n3\nBP\n49199999.0\n5200000\n103840194\n276850\n177.713560\n18.782734\n375.077457\n571.573751\n\n\n4\nDanone\n722122.0\n944877\n28969780\n28308\n25.509467\n33.378444\n1023.377844\n1082.265755\n\n\n\n\n\n\n\nOne advantage of carbon intensity is that it reduces the skewness of the data.\n\n## Skewness of the data\ncarbon_footprint[['Emission Scope 1', 'Emission Scope 2', 'Emission Scope 3', 'CI']].skew()\n\nEmission Scope 1    2.165456\nEmission Scope 2    0.054302\nEmission Scope 3    1.243665\nCI                  1.559668\ndtype: float64\n\n\n\n## Skewness of the data\ncarbon_footprint[['CE1', 'CE2', 'CE3', 'CI']].skew()\n\nCE1    2.087192\nCE2    0.813709\nCE3    1.997709\nCI     1.559668\ndtype: float64\n\n\nLet’s now visualize the spearman correlation between the several carbon metrics.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nsns.heatmap(carbon_footprint[['Emission Scope 1', 'Emission Scope 2', 'Emission Scope 3']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nsns.heatmap(carbon_footprint[['CE1', 'CE2', 'CE3']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\nWe notice that carbon emissions are highly correlated that carbon intensity because of the economic size effect (\\(33\\%\\) in average vs \\(23\\%\\) in average). Now let’s visualize the rank correlation between the carbon metrics.\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(carbon_footprint[['Emission Scope 1', 'Emission Scope 2', 'Emission Scope 3']].corr(method='spearman'), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(carbon_footprint[['CE1', 'CE2', 'CE3']].corr(method='spearman'), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\nWe notice that rank correlation between carbon emissions is higher than the rank correlation between carbon intensity, and rank correlation are very lower for all carbon intensity.\nLet’s finish this section with the additivity property of carbon intensity.\n\n\n2.2 Additivity property of carbon intensity\nFor a company i, the carbon intensity is additive. For example, the carbon intensity for a company i with respect to scope 1,2 and 3 is defined as follows: \\[\n\\begin{align}\n    CL_{i,1+2+3} &= \\frac{CE_{i,1} + CE_{i,2} + CE_{i,3}}{Y_i} \\\\\n                 &= CI_{i,1} + CI_{i,2} + CI_{i,3}\n\\end{align}\n\\]\nHowever, this property is not satisfied when we consider the carbon intensity of a portfolio of companies. Let us consider a portfolio of weights \\(w=(w_1,...,w_n)\\) investing in n assets(stock, bond, etc). Its carbon emissions are defined as follows:\n\\[\nCE_{j} = \\sum_{i=1}^{n} \\frac{W_i}{MV_i} CE_{ij} = \\sum_{i=1}^{n} \\bar{w_i} CE_{ij}\n\\]\nwhere \\(MV_i\\) is the market value the company i and j is the scope of carbon emissions, and \\(W_i\\) is the dollar amount invested in company i. \\(\\bar{w_i}\\) is the ownership ratio:\n\\[\n\\bar{w_i} = \\frac{W_i}{MV_i}\n\\]\nIf you invest \\((x_1,...,x_n)\\) where \\(x_i\\) is the dollar amount invested in company i, \\(x=\\sum_{i=1}^{n} x_i\\) is the total amount invested in the portfolio. We deduce that :\n\\[\n\\bar{w_i} = w_i \\frac{x}{MV_i}\n\\]\nand:\n\\[\n\\begin{align}\nCE_{j} &= \\sum_{i=1}^{n} w_i \\frac{x}{MV_i} CE_{ij} \\\\\n&= x \\sum_{i=1}^{n} \\frac{w_i}{MV_i} CE_{ij} \\\\\n&= x \\sum_{i=1}^{n} \\frac{w_i Y_i}{MV_i} \\frac{CE_{ij}}{Y_i} \\\\\n\\end{align}\n\\]\nThis equation equation gives us an interpretation of the carbon emission as weighted average of the carbon intensity where the normalization is the market value of the company. In other words :\n\\[\nCE_{j} = x \\sum_{i=1}^{n} w_i CI_{ij}^{MV}\n\\]\nwhere \\(CI_{ij}^{MV} =\\frac{CE_{ij}}{MV_i}\\) is the carbon intensity of company i with respect to scope j normalized by the market value of the company.\n\n\n2.3 Be aware of mixing large capitalization and small capitalization companies.\nIf we assume that the equation below is true:\n\\[\nCE_{j} = \\sum_{i=1}^{n} \\frac{W_i}{MV_i} CE_{ij} = \\sum_{i=1}^{n} \\bar{w_i} CE_{ij}\n\\]\nThen we can show that (refer to the page 8 and in appendix ix A.4.1 on page 45 on this book for more details):\n\\[\nCI_{j}(w) = \\sum_{i=1}^{n} x_i CI_{ij}\n\\]\nwhere the weights \\(x_i\\) are defined as follows:\n\\(x_i = \\frac{w_i\\frac{Y_i}{MV_i}}{\\sum_{i=1}^{n} w_i\\frac{Y_i}{MV_i}} = \\frac{w_i SR_i}{\\sum_{i=1}^{n} w_i SR_i}\\)\nwhere \\(SR_i = \\frac{Y_i}{MV_i}\\) is the revenue-to-market value (or sales-to-price) ratio.\nNow consider two issuers. The first issuer has a carbon emissions, revenue and market respectively equal to \\(CE_{1,j} = 5 \\times 10^6\\), \\(Y_1 = 2 \\times 10^5\\) and \\(MV_1 = 10^7\\). The second issuer has a carbon emissions, revenue and market respectively equal to \\(CE_{2,j} = 5 \\times 10^7\\), \\(Y_2 = 4 \\times 10^6\\) and \\(MV_2 = 10^7\\). We suppose we invest x=$10 mm to the issuers.\nLet’s write utility classes to compute the carbon intensity of each issuer, the carbon emissions of the portfolio and the carbon intensity of the portfolio.\n\nfrom collections.abc import Iterable\nclass CarbonIntensity:\n    @staticmethod\n    def carbon_intensity_issuer(\n        __CE: Iterable[float],\n        __Y: Iterable[float],\n\n    ):\n\n        return __CE / __Y\n    @staticmethod\n    def revenue_to_market(\n        Y: Iterable[float],\n        MV: Iterable[float]\n    ):\n\n        return Y / MV\n    @staticmethod\n    def carbon_emission_portfolio(\n        x: Iterable[float],\n        CE: Iterable[float],\n        w: Iterable[float],\n        MV: Iterable[float]\n    ):\n    \n        return sum([x * w * CE / MV for x, w, CE, MV in zip(x, w, CE, MV)])\n    @staticmethod\n    def carbon_intensity_portfolio(\n        w: Iterable[float],\n        Y: Iterable[float],\n        MV: Iterable[float],\n        CI: Iterable[float]\n    ):\n        total_revenue_tomarket = sum([(w * Y) / (MV) for w, Y, MV in zip(w, Y, MV)])\n        return sum([(w*Y*CI)/(total_revenue_tomarket* MV) for w, Y, CI, MV in zip(w, Y, CI, MV)])\n\nIt follows that the carbon intensity of the first issuer is equal to 25 and the carbon intensity of the second issuer is equal to 12.5. It is given by the following code:\n\nCE1, CE2 = 5e6, 5e7\nY1, Y2 = 2e5, 4e6\n\nCarbonIntensity.carbon_intensity_issuer(CE1, Y1), CarbonIntensity.carbon_intensity_issuer(CE2, Y2)\n\n(25.0, 12.5)\n\n\nThe revenue-to-market ratio of the first issuer is equal to 0.02 and the revenue-to-market ratio of the second issuer is equal to 0.4. It is given by the following code:\n\nMV1, MV2 = 1e7, 1e7\nCarbonIntensity.revenue_to_market(Y1, MV1), CarbonIntensity.revenue_to_market(Y2, MV2)\n\n(0.02, 0.4)\n\n\nNow, let’s compute the carbon emissions of the portfolio and the carbon intensity of the portfolio. If the weights of the first issuer is \\(w_1\\) and the weights of the second issuer is \\(w_2\\), The revenue that will be use to compute the carbon intensity of the portfolio is equal: \\(Y = w_1 \\times Y_1 + w_2 \\times Y_2\\) The weights \\(x_1\\) and \\(x_2\\) are given by the following code:\n\nweights = {\n    'w1': [\"0%\", \"10%\", \"20%\", \"30%\", \"50%\", \"70%\", \"80%\", \"90%\", \"100%\"],\n    'w2': [\"100%\", \"90%\", \"80%\", \"70%\", \"50%\", \"30%\", \"20%\", \"10%\", \"0%\"]}\nweights['w1'] = [float(w.replace('%', '')) / 100 for w in weights['w1']]\nweights['w2'] = [float(w.replace('%', '')) / 100 for w in weights['w2']]\n\nFirst we Compute the revenue to compute the carbon intensity of the portfolio :\n\ncarbon_intensity_portfolio_approach = pd.DataFrame()\ncarbon_intensity_portfolio_approach['w1'] = weights['w1']\ncarbon_intensity_portfolio_approach['w2'] = weights['w2']\ncarbon_intensity_portfolio_approach['Y *10^6'] = carbon_intensity_portfolio_approach['w1'] * Y1/1e6 + carbon_intensity_portfolio_approach['w2'] * Y2/1e6\ncarbon_intensity_portfolio_approach\n\n\n\n\n\n\n\n\nw1\nw2\nY *10^6\n\n\n\n\n0\n0.0\n1.0\n4.00\n\n\n1\n0.1\n0.9\n3.62\n\n\n2\n0.2\n0.8\n3.24\n\n\n3\n0.3\n0.7\n2.86\n\n\n4\n0.5\n0.5\n2.10\n\n\n5\n0.7\n0.3\n1.34\n\n\n6\n0.8\n0.2\n0.96\n\n\n7\n0.9\n0.1\n0.58\n\n\n8\n1.0\n0.0\n0.20\n\n\n\n\n\n\n\nThen we compute the carbon emissions of the portfolio and the carbon intensity of the portfolio:\n\ncarbon_intensity_portfolio_approach['CE*10^6'] = CarbonIntensity.carbon_emission_portfolio(\n    x=[10e6, 10e6],\n    CE=[CE1, CE2],\n    w=[carbon_intensity_portfolio_approach['w1'], carbon_intensity_portfolio_approach['w2']],\n    MV=[MV1, MV2]\n)/1e6\ncarbon_intensity_portfolio_approach\n\n\n\n\n\n\n\n\nw1\nw2\nY *10^6\nCE*10^6\n\n\n\n\n0\n0.0\n1.0\n4.00\n50.0\n\n\n1\n0.1\n0.9\n3.62\n45.5\n\n\n2\n0.2\n0.8\n3.24\n41.0\n\n\n3\n0.3\n0.7\n2.86\n36.5\n\n\n4\n0.5\n0.5\n2.10\n27.5\n\n\n5\n0.7\n0.3\n1.34\n18.5\n\n\n6\n0.8\n0.2\n0.96\n14.0\n\n\n7\n0.9\n0.1\n0.58\n9.5\n\n\n8\n1.0\n0.0\n0.20\n5.0\n\n\n\n\n\n\n\nThen we compute the carbon intensity of the portfolio. To do this, we need the carbon emissions of the portfolio and the revenue of the portfolio. The carbon intensity of the portfolio is given by the following code:\n\ncarbon_intensity_portfolio_approach['CI/Y'] = carbon_intensity_portfolio_approach['CE*10^6'] / carbon_intensity_portfolio_approach['Y *10^6']\ncarbon_intensity_portfolio_approach \n\n\n\n\n\n\n\n\nw1\nw2\nY *10^6\nCE*10^6\nCI/Y\n\n\n\n\n0\n0.0\n1.0\n4.00\n50.0\n12.500000\n\n\n1\n0.1\n0.9\n3.62\n45.5\n12.569061\n\n\n2\n0.2\n0.8\n3.24\n41.0\n12.654321\n\n\n3\n0.3\n0.7\n2.86\n36.5\n12.762238\n\n\n4\n0.5\n0.5\n2.10\n27.5\n13.095238\n\n\n5\n0.7\n0.3\n1.34\n18.5\n13.805970\n\n\n6\n0.8\n0.2\n0.96\n14.0\n14.583333\n\n\n7\n0.9\n0.1\n0.58\n9.5\n16.379310\n\n\n8\n1.0\n0.0\n0.20\n5.0\n25.000000\n\n\n\n\n\n\n\nwe finish this section by computing the carbon intensity of the portfolio using the weights defined by: \\(x_i = \\frac{w_i\\frac{Y_i}{MV_i}}{\\sum_{i=1}^{n} w_i\\frac{Y_i}{MV_i}} = \\frac{w_i SR_i}{\\sum_{i=1}^{n} w_i SR_i}\\)\ncarbon_intensity_portfolio_approach[‘CI’] = CarbonIntensity.carbon_intensity_portfolio( w=[carbon_intensity_portfolio_approach[‘w1’], carbon_intensity_portfolio_approach[‘w2’]], Y=[Y1, Y2], MV=[MV1, MV2], CI=[CarbonIntensity.carbon_intensity_issuer(CE1, Y1), CarbonIntensity.carbon_intensity_issuer(CE2, Y2)] ) carbon_intensity_portfolio_approach"
  },
  {
    "objectID": "posts/Leadership/index.html",
    "href": "posts/Leadership/index.html",
    "title": "Leadership : Pouvoir, autorité et légitimité",
    "section": "",
    "text": "Leadership : Pouvoir, autorité et légitimité\n\n\n\nLe Pouvoir se reçoit, l’autorité se construit.\nComme l’éminent penseur Christian Monjou, débutons avec une citation marquante. Niccolò Machiavel, une figure clé dans l’étude du leadership et de la stratégie politique, affirmait : ‘Les qualités nécessaires pour conserver le pouvoir ne sont pas les mêmes que celles pour l’acquérir.’ Cette citation soulève un point crucial dans le domaine de la gestion du leadership et de l’autorité en entreprise. Lorsqu’on crée une entreprise, devenir un patron ou un chef d’entreprise confère du pouvoir, mais pas nécessairement de l’autorité. L’autorité, dans le contexte de la leadership efficace, est une reconnaissance octroyée par autrui ; elle repose sur la légitimité perçue par les collaborateurs. Un leader en cours de construction de son autorité n’est pas automatiquement reconnu comme légitime. En effet, la légitimité en leadership et en gestion d’équipe se manifeste à travers le respect et la confiance que l’on inspire à son entourage. Souvent, ceux qui doivent affirmer leur propre légitimité en sont en réalité dépourvus. La construction de l’autorité est donc un élément essentiel à l’efficacité du leadership dans le monde des affaires et la gestion d’équipe.\n\n\nLa construction de l’autorité\nL’essence du leadership transformationnel réside dans la capacité à favoriser la croissance et l’épanouissement de la communauté à laquelle on appartient. Cela implique d’encourager et de soutenir le développement personnel et professionnel des membres de l’équipe. Le concept de développement personnel est crucial dans le cadre du leadership efficace. En effet, un leader qui cesse de s’investir dans son propre développement personnel risque de perdre la légitimité aux yeux de ses collaborateurs. Dans la construction de l’autorité, la notion de style personnel joue également un rôle clé. Un style de leadership rigide ou répétitif peut s’avérer contre-productif. Il est important de rester adaptable et innovant, quelle que soit la situation. Cela soulève une question fondamentale : le leadership est-il inné ou acquis ? La réponse réside souvent dans la capacité d’un individu à évoluer et à s’adapter continuellement, renforçant ainsi son autorité et son influence au sein de son organisation.\n\n\nL’origine du leadership\nLa question de savoir si le leadership est inné ou acquis est un débat central dans le domaine du développement du leadership. Bien qu’il puisse sembler que certaines personnes naissent leaders, cette idée comporte des risques. En effet, se reposer uniquement sur des compétences innées en leadership peut conduire à une stagnation, limitant l’innovation et l’adaptabilité. Comme le soulignait le théoricien du leadership, Berson, ‘Plaquer du mécanique sur du vivant, c’est risquer le ridicule.’ Un leader efficace ne se contente pas de compter sur des compétences innées, mais s’interroge continuellement sur l’adéquation de son style de leadership avec les besoins actuels de sa communauté ou de son organisation. Un exemple éloquent est celui du leadership féminin, où l’adaptabilité et l’empathie jouent souvent un rôle clé dans la réussite et l’impact du leadership.\n\n\nElisabeth I er\nLa figure que vous voyez est celle d’Elisabeth Ière elisabeth., reine d’Angleterre ayant régné durant 45 ans (de 1558 à 1603). Fille de Henri VIII et Anne Boleyn, elle fut la dernière souveraine de la dynastie des Tudors et la première femme à régner sur l’Angleterre et l’Irlande. Son ascension au trône, perçue initialement comme un coup du hasard, est un exemple remarquable de la manière dont un leader peut transformer une opportunité imprévue en un règne influent et significatif. Sa présence imposante, souvent décrite comme une ‘hypertrophie de signe’, reflétait une stratégie de communication visant à établir sa légitimité et son autorité. En effet, Elisabeth Ière comprenait l’importance de l’image et du symbolisme dans la construction du leadership. Elle incarnait la dualité du ‘signe et du sens’ – un aspect essentiel dans la représentation du leadership. Son règne illustre parfaitement la nécessité pour un leader de maintenir un équilibre entre son identité privée et sa persona publique. Finalement, l’efficacité de son leadership résidait dans sa capacité à maîtriser l’art de la communication – un aspect crucial pour tout leader aspirant à un impact durable.\n\n\nLa qualité de la parole\nDans le contexte actuel de l’éducation et du leadership, une transformation notable s’est opérée par rapport aux méthodes traditionnelles. Autrefois, l’entrée d’un enseignant dans une salle de classe était synonyme de respect absolu et d’une reconnaissance incontestée de son savoir. Cependant, dans le monde moderne, marqué par l’accès instantané à l’information via la technologie numérique, cette dynamique a évolué. Imaginez un professeur mentionnant un pourcentage spécifique, comme 2.23 %, devant des étudiants équipés de smartphones. Il est probable qu’ils vérifient immédiatement cette information, remettant en question l’exactitude des données présentées. Cette situation illustre un changement fondamental : la légitimité dans le domaine de l’éducation et du leadership ne repose plus exclusivement sur la transmission du savoir, mais de plus en plus sur la qualité de la communication et de l’engagement. L’ère numérique, en économisant du temps, nous incite non pas à intensifier notre présence numérique, mais plutôt à valoriser davantage le temps consacré à la communication directe et significative. Selon Marcel Mauss, ce principe d’échange symbolique, où la qualité de la communication exige en retour un engagement – ici sous forme de loyauté – est essentiel. Enfin, la vraie mesure d’un leader réside dans sa capacité à anticiper les idées potentiellement nuisibles à sa communauté et à persuader celle-ci d’adopter de nouvelles perspectives pour éviter les pièges futurs. Mener une communauté du connu vers l’inconnu est une des responsabilités les plus ardues d’un leader.\n\n\nUn monde de changement\nNous vivons dans une ère de transformation rapide, caractérisée par un marché mondial en constante évolution. À l’ère numérique et face aux défis de la transition écologique, les communautés et organisations qui ne reconnaissent pas ce changement de paradigme risquent l’obsolescence. Cette transformation s’illustre particulièrement avec l’avènement de l’intelligence artificielle, comme en témoigne le développement de technologies telles que ChatGPT. De plus, les appels urgents du GIEC et des communautés scientifiques, notamment des astrophysiciens, mettent en lumière l’imminence d’une crise écologique mondiale. Il est impératif pour les entreprises, et plus spécifiquement pour les banques, d’évaluer et de gérer ces risques. L’enjeu n’est pas simplement financier, mais il s’agit de la préservation de la vie sur Terre dans toutes ses formes. La responsabilité sociale des entreprises et la durabilité environnementale sont devenues des facteurs clés dans la stratégie d’entreprise moderne, dictant une nouvelle approche vers un avenir plus durable et conscient."
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html",
    "href": "posts/machineLearning3A/QCM.html",
    "title": "Examen de machine learning QCM",
    "section": "",
    "text": "qcm",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?",
    "text": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?\n\n\nMinimize training error\n\n\nMinimize testing error\n\n\nMinimize a combination of training and testing error\n\n\nMaximize model complexity\n\n\nRéponse: C) Minimize training error",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "href": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "title": "Examen de machine learning QCM",
    "section": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?",
    "text": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?\n\n\nFeature selection\n\n\nOverfitting\n\n\nUnderfitting\n\n\nData preprocessing errors\n\n\nRéponse: B) Overfitting",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "href": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "title": "Examen de machine learning QCM",
    "section": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?",
    "text": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?\n\n\nLinear regression\n\n\nRidge regression\n\n\nLasso regression\n\n\nSupport vector regression\n\n\nRéponse: C) Lasso regression",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "href": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "title": "Examen de machine learning QCM",
    "section": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?",
    "text": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?\n\n\nGradient Descent\n\n\nNewton’s Method\n\n\nGenetic Algorithms\n\n\nDecision Trees\n\n\nRéponse: A) Gradient Descent",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "href": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "title": "Examen de machine learning QCM",
    "section": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?",
    "text": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?\n\n\nMaximize the margin between data points from different classes.\n\n\nMinimize the margin between data points from the same class.\n\n\nMaximize the number of support vectors.\n\n\nMinimize the number of support vectors.\n\n\nRéponse: A) Maximize the margin between data points from different classes.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "href": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "title": "Examen de machine learning QCM",
    "section": "Q6: In Ridge regression, what does the regularization term penalize?",
    "text": "Q6: In Ridge regression, what does the regularization term penalize?\n\n\nThe magnitude of the coefficients\n\n\nThe number of features\n\n\nThe mean squared error\n\n\nThe bias of the model\n\n\nRéponse: A) The magnitude of the coefficients",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "href": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "title": "Examen de machine learning QCM",
    "section": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?",
    "text": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?\n\n\nPrincipal Component Analysis (PCA)\n\n\nK-Means Clustering\n\n\nSupport Vector Machine (SVM)\n\n\nK-Nearest Neighbors (KNN)\n\n\nRéponse: C) Support Vector Machine (SVM)",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?",
    "text": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?\n\n\nIncreasing model complexity reduces bias and increases variance\n\n\nIncreasing model complexity increases both bias and variance\n\n\nDecreasing model complexity reduces bias and increases variance\n\n\nDecreasing model complexity reduces both bias and variance\n\n\nRéponse: A) Increasing model complexity reduces bias and increases variance",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "href": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "title": "Examen de machine learning QCM",
    "section": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?",
    "text": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?\n\n\nIt reduces overfitting\n\n\nIt simplifies the optimization problem\n\n\nIt allows SVM to handle non-linear data\n\n\nIt speeds up the training process\n\n\nRéponse: C) It allows SVM to handle non-linear data",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q10: What is the primary purpose of cross-validation in machine learning?",
    "text": "Q10: What is the primary purpose of cross-validation in machine learning?\n\n\nTo train a model on multiple datasets\n\n\nTo select the best hyperparameters for a model\n\n\nTo overfit the model to the training data\n\n\nTo evaluate a model’s performance on the training data\n\n\nRéponse: B) To select the best hyperparameters for a model",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Machine learning 3A",
      "QCM surprise"
    ]
  },
  {
    "objectID": "posts/MarketFinance/schiller.html",
    "href": "posts/MarketFinance/schiller.html",
    "title": "Data Schiller : CAPE Ratio",
    "section": "",
    "text": "For our final year project at ENSAI, we embarked on an ambitious endeavor to evaluate the impact of climate change on stock portfolio allocation, utilizing data from the Network for Greening the Financial System (NGFS). The NGFS is a collective of central banks and financial supervisors dedicated to exchanging best practices and fostering the development of environmental and climate risk management within the financial sector. This group provides various models based on climate change scenarios, which project future variables essential for our analysis.\nWe decided to focus our research on four countries: the United States, the UK, China, and France. Our objective was to construct a model capable of predicting stock prices in each country under different climate change scenarios, leveraging macroeconomic variables. Due to the lack of direct access to stock prices for all these countries, we adopted a methodology proposed by Robert J. Shiller to find a proxy for stock prices. By predicting this proxy using macroeconomic variables, we aimed to gain insights into future stock prices, which would then inform our portfolio allocation strategy.\nAn integral part of our analysis involved the CAPE ratio, developed by economist Robert J. Shiller. The CAPE ratio, or Cyclically Adjusted Price to Earnings ratio, is a valuation measure for stocks. It is calculated by dividing the stock price by the average of the company’s earnings over the last ten years, adjusted for inflation. This approach smooths out short-term fluctuations in earnings and provides a more stable measure of a stock’s valuation, making it particularly useful for long-term investment decisions. By incorporating the CAPE ratio into our model, we sought to leverage its predictive power to enhance our understanding of how climate change scenarios might influence stock valuations and, consequently, portfolio allocation decisions.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "CAPE Ratio"
    ]
  },
  {
    "objectID": "posts/MarketFinance/schiller.html#introduction",
    "href": "posts/MarketFinance/schiller.html#introduction",
    "title": "Data Schiller : CAPE Ratio",
    "section": "",
    "text": "For our final year project at ENSAI, we embarked on an ambitious endeavor to evaluate the impact of climate change on stock portfolio allocation, utilizing data from the Network for Greening the Financial System (NGFS). The NGFS is a collective of central banks and financial supervisors dedicated to exchanging best practices and fostering the development of environmental and climate risk management within the financial sector. This group provides various models based on climate change scenarios, which project future variables essential for our analysis.\nWe decided to focus our research on four countries: the United States, the UK, China, and France. Our objective was to construct a model capable of predicting stock prices in each country under different climate change scenarios, leveraging macroeconomic variables. Due to the lack of direct access to stock prices for all these countries, we adopted a methodology proposed by Robert J. Shiller to find a proxy for stock prices. By predicting this proxy using macroeconomic variables, we aimed to gain insights into future stock prices, which would then inform our portfolio allocation strategy.\nAn integral part of our analysis involved the CAPE ratio, developed by economist Robert J. Shiller. The CAPE ratio, or Cyclically Adjusted Price to Earnings ratio, is a valuation measure for stocks. It is calculated by dividing the stock price by the average of the company’s earnings over the last ten years, adjusted for inflation. This approach smooths out short-term fluctuations in earnings and provides a more stable measure of a stock’s valuation, making it particularly useful for long-term investment decisions. By incorporating the CAPE ratio into our model, we sought to leverage its predictive power to enhance our understanding of how climate change scenarios might influence stock valuations and, consequently, portfolio allocation decisions.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "CAPE Ratio"
    ]
  },
  {
    "objectID": "posts/MarketFinance/schiller.html#the-schiller-cape-ratio",
    "href": "posts/MarketFinance/schiller.html#the-schiller-cape-ratio",
    "title": "Data Schiller : CAPE Ratio",
    "section": "2 The Schiller CAPE Ratio",
    "text": "2 The Schiller CAPE Ratio\nIn 1998, Robert Schiller and John Campell found that long-term equity market returns are not random walks and could be predicted through a measure they constructed: the Cyclically Adjusted Price–Earnings ratio (CAPE ratio). Schiller and Campbell calculated the CAPE ratio by dividing a long-term index of stock market prices and earnings (the index considered is the S&P 500 index) by the average earnings per share of S&P 500 companies over the last ten years, with earnings and stock prices measured in real terms. Then\n\\[\nCAPE_t = \\frac{P_t}{[(EARN_t+EARN_{t-1}+...+EARN_{t-10})/10]}\n\\]\nWith \\(P_t\\) the real stock market price (adjusted for inflation) and earnings level and \\(EARN_t\\) the average earnings per share of S&P 500 companies.\nUsing the average earnings over the last ten years instead of the earnings of the current year allows to smooth out the impact of business cycles on the earnings and stock prices.\nThe data provided below enables the computation of the CAPE ratio. The consumer price index (CPI) adjusts the stock prices and earnings for inflation, ensuring an accurate measure.\n\nimport pandas as pd\nimport numpy as np\n\n# Read Excel file\ndata_schiller = pd.read_excel(\"data/ie_data_Schiller_2023.xls\", sheet_name=\"Data\", header=None)\n\n# Drop the first 7 rows and certain columns\ndata_schiller = data_schiller.drop(data_schiller.index[0:7]).reset_index(drop=True)\ndata_schiller = data_schiller.drop(columns=[1, 14, 16])\n\n# Set column names\ndata_schiller.columns = ['Date', 'S&P Comp', 'Dividend', 'Earnings', 'Consumer Price CPI', \n                             'Date Fraction', 'Long Interest Rate', 'Real price', 'Real Dividend', \n                             'Real Total Return Price', 'Real Earnings','Real TR Scaled Earnings', \n                             'CAPE', 'TR CAPE', 'Excess CAPE Yield', 'Monthly Total Bond Returns', \n                             'Real Total Bond Returns', '10 Years Annualized Stock Real Return', \n                             '10 Years Annualized Bonds Real Return', 'Real 10 Years excess Annualized Returns'\n                          ]\n\n\n# Convert columns 2 to 20 to numeric and the Date column to string\ndata_schiller.iloc[:, 1:20] = data_schiller.iloc[:, 1:20].apply(pd.to_numeric, errors='coerce')\ndata_schiller['Date'] = data_schiller['Date'].astype(str)\n\n# Extract year and quarter from the Date, then form a new Date format\nyear = data_schiller['Date'].str.slice(0, 4)\nquarter = data_schiller['Date'].str.slice(5, 7)\ndate = year + \"-\" + quarter + \"-01\"\ndata_schiller['Date'] = pd.to_datetime(date, errors='coerce', format='%Y-%m-%d')\n\n# Drop rows where Date is NA\ndata_schiller = data_schiller.dropna(subset=['Date'])\n\ndata_schiller[['Date', 'S&P Comp', 'Dividend', 'Earnings', 'Consumer Price CPI', \n                             'Real price', 'Real Dividend', \n                             'Real Total Return Price', 'Real Earnings', \n                             'CAPE', '10 Years Annualized Stock Real Return', \n                             \n]].head()\n\n\n\n\n\n\n\n\nDate\nS&P Comp\nDividend\nEarnings\nConsumer Price CPI\nReal price\nReal Dividend\nReal Total Return Price\nReal Earnings\nCAPE\n10 Years Annualized Stock Real Return\n\n\n\n\n1\n1871-01-01\n4.44\n0.26\n0.4\n12.464061\n109.050018\n6.385812\n109.050018\n9.824326\nNaN\n0.130609\n\n\n2\n1871-02-01\n4.5\n0.26\n0.4\n12.844641\n107.248908\n6.196604\n107.765291\n9.533236\nNaN\n0.130858\n\n\n3\n1871-03-01\n4.61\n0.26\n0.4\n13.034972\n108.266269\n6.106124\n109.298845\n9.394036\nNaN\n0.130951\n\n\n4\n1871-04-01\n4.74\n0.26\n0.4\n12.559226\n115.536124\n6.337425\n117.171191\n9.749884\nNaN\n0.122056\n\n\n5\n1871-05-01\n4.86\n0.26\n0.4\n12.273812\n121.215781\n6.484795\n123.479273\n9.976607\nNaN\n0.122638\n\n\n\n\n\n\n\nThe table below shows the statistic descriptif of the CAPE. The mean value is 16.50\n\ndata_schiller['CAPE'].astype(float).describe()\n\ncount    1713.000000\nmean       17.401851\nstd         7.193108\nmin         4.784241\n25%        11.951097\n50%        16.501404\n75%        21.137767\nmax        44.197940\nName: CAPE, dtype: float64\n\n\n\ndata_schiller['logCape'] = np.log(data_schiller['CAPE'].astype(float))\n\n\n2.1 Interpretation of CAPE Ratio\nThe CAPE ratio is a ratio of two elements. Mathematically, it is elevated when the stock price is high relative to the average earnings over the last ten years, or conversely, when these average earnings are low compared to the stock price. Hence, it can be viewed as an indicator of the stock market’s or a specific company’s profitability.\nThis ratio was at a record 28 in January 1997, with the only other instance (at that time) of a comparably high ratio occurring in 1929. Shiller and Campbell asserted the ratio was predicting that the real value of the market would be 40% lower in ten years than it was at that time. That forecast proved to be remarkably prescient, as the market crash of 2008 contributed to the S&P 500 plunging 60% from October 2007 to March 2009.\nThe CAPE ratio for the S&P 500 climbed steadily in the second decade of this millennium as the economic recovery in the U.S. gathered momentum, and stock prices reached record levels. As of June 2018, the CAPE ratio stood at 33.78, compared with its long-term average of 17.40. The fact that the ratio had previously only exceeded 30 in 1929 and 2000 triggered a raging debate about whether the elevated value of the ratio portends a major market correction, see Figure 1\n\nimport matplotlib.pyplot as plt\n\nplt.plot(data_schiller['Date'], data_schiller['CAPE'])\n# The vertical line of the max value of the CAPE ratio\nplt.axvline(x=data_schiller['Date'][data_schiller['CAPE'].idxmax()], color='r', linestyle='--')\nplt.title('CAPE Ratio')\nplt.xlabel('Date')\nplt.ylabel('CAPE Ratio')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: CAPE Ratio",
    "crumbs": [
      "About",
      "Ensai 3A",
      "CAPE Ratio"
    ]
  },
  {
    "objectID": "posts/MarketFinance/schiller.html#schillers-database-and-methodology",
    "href": "posts/MarketFinance/schiller.html#schillers-database-and-methodology",
    "title": "Data Schiller : CAPE Ratio",
    "section": "3 Schiller’s database and methodology",
    "text": "3 Schiller’s database and methodology\nThe Schiller database at our disposal contains monthly U.S. data on 19 variables from 1871 to 2023. These include the \\(P_t\\) value of the S&P index, earnings, dividends, the CAPE ratio, the annualized 10-year real return for the equity market and the same for the bond market. The 10 year annualized real return refers to annual returns over the next 10 years either on the stock market or on the bond market. They are observed over 10 years. This variable therefore refers to future or prospective returns. All these variables will be particularly useful in our study.\nThe formula for the 10-year annualized real return can be represented as follows: \\[\nR_{\\text{annualized}} = \\left( \\frac{R_{\\text{t}}}{R_\\text{t+10 year}} \\right)^{\\frac{1}{10}} - 1\n\\]\nWith \\(R_t\\) the real total return price at the time t and \\(R_{t+10 year}\\) the real total return price at the time t+10 years.\nSchiller and Campbell adopt a methodology to relate the CAPE ratio to the level of forward equity market returns.\nShiller’s CAPE methodology regresses the forward 10-year annualized real stock return (\\(RET_t\\)) on the current value of the CAPE ratio, over 1881–2004. They found:\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\n# choose data from 1881 to 2004\nsample_train = (data_schiller['Date'] &gt;= '1881-01-01') & (data_schiller['Date'] &lt;= '2004-12-31')\ntrain_schiller = data_schiller[sample_train]\n\ntest_schiller = data_schiller[~sample_train]\ntrain_schiller['logCape'] = pd.to_numeric(train_schiller['logCape'], errors='coerce')\ntrain_schiller['10 Years Annualized Stock Real Return'] = pd.to_numeric(train_schiller['10 Years Annualized Stock Real Return'], errors='coerce')\n\n# Dropping rows with NaN values after the conversion (if any)\ntrain_schiller = train_schiller.dropna(subset=['logCape', '10 Years Annualized Stock Real Return'])\n\nmodel = smf.ols(formula='Q(\"10 Years Annualized Stock Real Return\") ~ logCape', data=train_schiller).fit()\n\n\n# Display the summary of the regression\nprint(model.summary())\n\n                                        OLS Regression Results                                        \n======================================================================================================\nDep. Variable:     Q(\"10 Years Annualized Stock Real Return\")   R-squared:                       0.352\nModel:                                                    OLS   Adj. R-squared:                  0.352\nMethod:                                         Least Squares   F-statistic:                     808.8\nDate:                                        Mon, 08 Apr 2024   Prob (F-statistic):          2.05e-142\nTime:                                                01:59:26   Log-Likelihood:                 2599.8\nNo. Observations:                                        1488   AIC:                            -5196.\nDf Residuals:                                            1486   BIC:                            -5185.\nDf Model:                                                   1                                         \nCovariance Type:                                    nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2791      0.008     36.641      0.000       0.264       0.294\nlogCape       -0.0794      0.003    -28.440      0.000      -0.085      -0.074\n==============================================================================\nOmnibus:                       19.658   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               16.654\nSkew:                          -0.192   Prob(JB):                     0.000242\nKurtosis:                       2.653   Cond. No.                         21.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nC:\\Users\\johns\\AppData\\Local\\Temp\\ipykernel_17472\\80514701.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_schiller['logCape'] = pd.to_numeric(train_schiller['logCape'], errors='coerce')\nC:\\Users\\johns\\AppData\\Local\\Temp\\ipykernel_17472\\80514701.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_schiller['10 Years Annualized Stock Real Return'] = pd.to_numeric(train_schiller['10 Years Annualized Stock Real Return'], errors='coerce')\n\n\n\\[\n\\mathrm{RET}_t=0.28 - 0.08 \\log \\left(\\mathrm{CAPE}_t\\right)+\\varepsilon_t,  \\quad R^2=0.35\n\\]\nWe acknowledge the following concerns : - Endogeneity of regressor : Whereby price appears in the both sides of the equation, therery violating the assumption of 1. - Induced autocorrelation : The residuals are not independent, they are correlated because using the overlapping observations.\nWe will use one of those corrections : - Hansen - Hodrick - Newey - West - Cochrane - Orcutt - Hjalmarsson\nLess use the Newey - West correction.\n\nimport statsmodels.formula.api as smf\n\n# Ensure your DataFrame 'train_schiller' is prepared and available\ntrain_schiller = train_schiller.dropna(subset=['logCape', '10 Years Annualized Stock Real Return'])\n\n# Define and fit the model with Newey-West correction\nmodel = smf.ols(formula='Q(\"10 Years Annualized Stock Real Return\") ~ logCape', data=train_schiller).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n\n# Display the summary of the regression with corrected standard errors\nprint(model.summary())\n\n                                        OLS Regression Results                                        \n======================================================================================================\nDep. Variable:     Q(\"10 Years Annualized Stock Real Return\")   R-squared:                       0.352\nModel:                                                    OLS   Adj. R-squared:                  0.352\nMethod:                                         Least Squares   F-statistic:                     653.9\nDate:                                        Mon, 08 Apr 2024   Prob (F-statistic):          7.96e-120\nTime:                                                01:59:26   Log-Likelihood:                 2599.8\nNo. Observations:                                        1488   AIC:                            -5196.\nDf Residuals:                                            1486   BIC:                            -5185.\nDf Model:                                                   1                                         \nCovariance Type:                                          HAC                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2791      0.009     32.091      0.000       0.262       0.296\nlogCape       -0.0794      0.003    -25.571      0.000      -0.085      -0.073\n==============================================================================\nOmnibus:                       19.658   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               16.654\nSkew:                          -0.192   Prob(JB):                     0.000242\nKurtosis:                       2.653   Cond. No.                         21.5\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 1 lags and without small sample correction\n\n\nThey obtained that the coefficient on the CAPE ratio is highly significant. Furthermore, the \\(R^2\\) is 35%, indicating that the CAPE ratio explains more than a third of the variation of 10-year real equity returns. Using this regression, they are able to estimate and predict returns.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "CAPE Ratio"
    ]
  },
  {
    "objectID": "posts/MarketFinance/schiller.html#conclusion",
    "href": "posts/MarketFinance/schiller.html#conclusion",
    "title": "Data Schiller : CAPE Ratio",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis example shows how the CAPE ratio can be used to predict future stock returns. However, it has some limitations. Critics of the CAPE ratio contend that it is not very useful since it is inherently backward-looking, rather than forward-looking. Another issue is that the ratio relies on GAAP (generally accepted accounting principles) earnings, which have undergone marked changes in recent years.\nIn June 2016, Jeremy Siegel of the Wharton School published a paper in which he said that forecasts of future equity returns using the CAPE ratio might be overly pessimistic because of changes in the way GAAP earnings are calculated. Siegel said that using consistent earnings data such as operating earnings or NIPA (national income and product account) after-tax corporate profits, rather than GAAP earnings, improves the forecasting ability of the CAPE model and forecasts higher U.S. equity returns.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "CAPE Ratio"
    ]
  },
  {
    "objectID": "posts/MasteringFinancialData/index.html",
    "href": "posts/MasteringFinancialData/index.html",
    "title": "Maitrise des données financières avec Python",
    "section": "",
    "text": "Introduction\nLes données financières sont très utilisées dans la filière gestion des risques de l’ENSAI. Avoir rapidement accès est un atout pour les étudiants et les fera gagner du temps. Ces données sont utilisées dans plusieurs cours notamment le cours de séries temporelles, le cours de la théorie de gestion des risques multiples,le cours d’asset pricing, etc. C’est pourquoi j’ai décidé de partager avec vous quelques astuces pour maitriser les données financières avec Python. Nous utiliserons la librairie yfinance pour récupérer les données financières et pandas pour les manipuler et matplotlib pour les visualiser.\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nJ’ai importé la library datetime pour manipuler les dataes. Maintenant, nous pouvons importer les données.\n\n\nImporter les données\n\nimport yfinance as yf\n\naapl = yf.download('AAPL', \n                      start='2012-01-01', \n                      end='2024-01-01',)\naapl.head()                    \n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\nLe code ci-dessus utilise la fonction downloadde la librairir yfinance pour télécharger les données de la société Apple (AAPL) de 2012 à 2024. De la même manière, vous pouvez télécharger les données d’autres sociétés. Par exemple, pour accèder aux données du CAC40, vous pouvez utiliser le code suivant:\n\ncac40 = yf.download('^FCHI', \n                      start='2012-01-01', \n                      end='2024-01-01',)\ncac40.head()\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n3231.429932\n3246.739990\n3193.629883\n3245.399902\n3245.399902\n123415200\n\n\n2012-01-04\n3227.459961\n3242.840088\n3186.479980\n3193.649902\n3193.649902\n114040800\n\n\n2012-01-05\n3197.159912\n3200.149902\n3136.750000\n3144.909912\n3144.909912\n121161600\n\n\n2012-01-06\n3156.419922\n3184.379883\n3122.629883\n3137.360107\n3137.360107\n104492800\n\n\n2012-01-09\n3143.949951\n3157.310059\n3114.449951\n3127.689941\n3127.689941\n96976800\n\n\n\n\n\n\n\nNous savons que pour les données de marché, nous avons les colonnes suivantes: Open, High, Low, Close, Adj Close, Volume. Nous allons maintenant travailler avec la colonne Close qui représente le prix de clôture de l’action. Nous pouvons maintenant visualiser les données.\n\nplt.figure(figsize=(10, 6))\nplt.plot(aapl['Close'], label='AAPL')\nplt.title('Prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Prix de clôture')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCes données ne sont pas stationnaires. Généralement, pour les données financières, nous travaillons avec les rendements car ils sont stationnaires. Si \\(P_t\\) est le prix de l’action à la date \\(t\\), le rendement à la date \\(t\\) est donné par: \\[\nr_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\]\nest équivalent à\n\\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right) = \\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\nDonc nous travaillons avec le logarithme des rendements. Nous pouvons maintenant calculer les rendements. Il existe une fonction pct_change dans la librairie pandas qui permet de calculer les rendements. Nous allons utiliser cette fonction combinée avec la fonction log de la librairie numpy pour calculer les logarithmes des rendements du prix de clôture de l’action AAPL.\n\ndaily_close = aapl[['Close']]\ndaily_close_returns = daily_close.pct_change().apply(lambda x: np.log(1+x))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nDans le code ci-dessus nous avons utilisé l’expression ci-dessous pour calculer les rendements: \\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right)\n\\]\nNous pouvons plutôt utiliser la fonction log de la librairie numpy pour calculer les rendements à partir de l’expression ci-dessous: \\[\n\\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\n\ndaily_close_returns = np.log(daily_close / daily_close.shift(1))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nNous avons les mêmes résultats avec la première valeur qui est NaN. Nous pouvons supprimer cette valeur.\n\ndaily_close_returns = daily_close_returns.dropna()\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n2012-01-10\n0.003574\n\n\n\n\n\n\n\nNous pouvons maintenant visualiser les rendements.\n\ndaily_close_returns.plot(figsize=(10, 6))\nplt.title('Logarithme des rendements du prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Logarithme des rendements')\nplt.show()\n\n\n\n\n\n\n\n\nSi vous voulez, vous pouvez étudier les statistiques descriptives des rendements. Une fonction que j’adore qui donne les statistiques sommaire est la fonction describe de la librairie pandas.\n\ndaily_close_returns.describe()\n\n\n\n\n\n\n\n\nClose\n\n\n\n\ncount\n3017.000000\n\n\nmean\n0.000853\n\n\nstd\n0.017967\n\n\nmin\n-0.137708\n\n\n25%\n-0.007562\n\n\n50%\n0.000765\n\n\n75%\n0.010275\n\n\nmax\n0.113157\n\n\n\n\n\n\n\nNous pouvons aussi rééchantillonner les données pour avoir les log rendements minimums, maximums, moyens, etc. par semaine, par mois, par trimestre,etc. Par exemple pour avoir les log rendements moyens par semaine, nous pouvons utliser la fonction resample de la librairie pandas combinée avec la fonction mean pour avoir les moyennes.\n\nweekly = daily_close_returns.resample('W').mean()\nweekly.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-08\n0.008933\n\n\n2012-01-15\n-0.001230\n\n\n2012-01-22\n0.000292\n\n\n2012-01-29\n0.012443\n\n\n2012-02-05\n0.005469\n\n\n\n\n\n\n\nJe vais terminer ce poste par vous montrer comment télécharger les données de plusieurs sociétés. Par exemple, pour télécharger les données de Apple, Microsoft, Google et Amazon, vous pouvez utiliser le code suivant:\n\nimport yfinance as yf\ntickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n\ndef get_data(tickers, startdate, enddate):\n    def data(ticker):\n        return (yf.download(ticker, start=startdate, end=enddate))\n    datas = map(data, tickers)\n    return(pd.concat(datas,keys= tickers, names=['Ticker', 'Date']))\nall_data = get_data(tickers, '2012-01-01', '2024-01-01')\nall_data.head()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nTicker\nDate\n\n\n\n\n\n\n\n\n\n\nAAPL\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\n\n\nConclusion\nJ’espère que ce poste vous sera utile. Si vous avez des questions, n’hésitez pas à me contacter si chatgpt ne peut pas vous aider.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Mastering Financial Data"
    ]
  },
  {
    "objectID": "posts/obligation/index.html",
    "href": "posts/obligation/index.html",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "",
    "text": "Une obligation zéro coupon est une obligation dont les intérêts sont versés en totalité à l’échéance de l’emprunt après avoir capitalisé sur toute la période.Elle est mise à un prix inférieur à sa valeur nominale et remboursée à sa valeur nominale complète à l’échéance.Par exemple, une obligation de valeur nominale 1000€ et de maturité pourrait être mise à 680 €. Ce prix inférieur reflète le fait que l’investisseur ne recevra pas d’intérêt annuel. À l’échéance, l’investisseur recevra 1000 €. Il est donc important de savoir calculer le prix d’une obligation zéro. Nous noterons B(t,T) le prix d’une obligation zéro coupon de maturité T à l’instant t. Il a plusieurs applications : - Il va permettre de construire la courbe de taux zéro coupon. - Il joue le rôle de facteur d’actualisation. - Il est aussi utilisé dans la modélisation du modèle de Hull et White.\nLa première section de ce document se focalisera sur l’élaboration de la courbe relative aux obligations zéro coupon, à partir de laquelle nous déduirons aisément la courbe des taux zéro coupon. Dans la 2nde partie, nous allons valoriser une obligation. Dans la seconde partie, nous procéderons à la valorisation d’une obligation, en mettant l’accent sur le concept de dirty price et de clean price. Ensuite nous étudier la sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit. Enfin, la troisième partie sera consacrée à l’étude d’un modèle simple de Hull et White.\n\n\nL’objectif de cette partie est de construire la courbe spot des taux zéro coupon \\(r_T\\) à partir des données de marché. Il est donné par la formule suivante : \\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\nLa calibration de la courbe de taux se fait par bootstrapping à partir des cotations du marché : - CT = cash rates (les cash flow) : Ici c’est la courbe de taux qui est constatée sur le marché interbancaire - FRA = Forward Rate Agreement (les taux forward) - SWAP = Interest Rate Swap (les taux swap)\n\n\nCode\n#!pip install openpyxl\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom scipy import interpolate\n#importer un fichier xlsx : la première feuille\n\n#courses = pd.read_csv('courses.csv')\ntaux_BBG = pd.read_excel('Taux_BBG.xlsx', sheet_name='Feuil1') \n\n\n\n\n\n\nCode\n# calcul du Mid qui est la moyenne entre le bid et l'ask\n\nnouvelle_ligne = pd.DataFrame({\n    'Term': [5],  # Supposons que vous voulez ajouter une période de 5 mois au début\n    'Unit': ['MO'],\n    'Ticker': ['EUR005M'],  # Supposons un ticker pour la nouvelle ligne\n    'Bid': [3.9],  # Exemple de valeur Bid\n    'Ask': [3.9],  # Exemple de valeur Ask\n    'Spread': [None],\n    'Bid Spr Val': [0],\n    'Ask Spr Val': [0],\n    'Final Bid Rate': [None],\n    'Final Ask Rate': [None],\n    'Rate Type': [None],\n    'Daycount': [None],\n    'Freq': [None]\n}, index=[0])\ndf = pd.concat([nouvelle_ligne, taux_BBG], ignore_index=True)\n\n\n\n\n\nPour calculer la courbe de taux à court terme, nous avons besoin du Mid qui est la moyenne entre le bid et l’ask. Nous nous le considérons comme proxy des taux interbancaires par exemple le LIBOR. Il était important de considérer la liquidité de l’instrument. Les écarts plus larges entre le bid et l’ask indiquent une liquidité plus faible.Ce qui pourrait influencer l’approximation du Libor.Pour des raisons de simplicité, nous avons, nous n’avons pas pris en compte la liquidité des instruments.\n\n\nCode\ndf['Mid'] = (df['Bid'] + df['Ask']) / 2\n\n\n\n\nCode\n# Initialisation des colonnes\ndf['T'] = 0\ndf['T1'] = 0\ndf['T2'] = 0\ndf['B(0,T)'] = 0\n\n\n\n\n\n\nCode\n# calcule du premier élement de B(0,T)\ndf['B(0,T)'].astype(int)[0] = 1\n\n\n\n\n\n\nSa formule est donnée par :\n\\[B(0,T) = \\frac{1}{(1+\\delta*L )}\\]\nOù \\(L\\) est le taux de prêt interbancaire(Libor par exemple) et \\(\\delta\\) est la fraction de l’année.\n\n\nCode\n# Conversion des termes en années\n\ndf['T2'] = df['Term'].astype(int)/12\n\n\n\n\n\n\n\nCode\n\ndf['T1'] = df['T2'] - 0.5\n\n\n\n\n\n\n\nCode\n# df['T'] = df['Term'].astype(int)/12 if df['Unit'] == 'MO' else df['Term'].astype(int)\n# With apply function\ndf['T'] = df.apply(lambda x: x['Term']/12 if x['Unit'] == 'MO' else x['Term'], axis=1)\ndf[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n5\nMO\n3.900\n0.416667\n-0.083333\n0.416667\n0\n\n\n1\n6\nMO\n3.832\n0.500000\n0.000000\n0.500000\n0\n\n\n\n\n\n\n\n\n\nCode\ndf['B(0,T1)'] =0\ndf['T'].astype(int)[0] = 0\n\ndf.loc[0, 'B(0,T)'] = 1\n\n\ndf.loc[1,'B(0,T)'] = 1/(1+ df.loc[1,'Mid']/100*0.5)\n\ndf[[\"Term\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\",\"B(0,T1)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nMid\nT\nT1\nT2\nB(0,T)\nB(0,T1)\n\n\n\n\n0\n5\n3.900\n0.416667\n-0.083333\n0.416667\n1.0000\n0\n\n\n1\n6\n3.832\n0.500000\n0.000000\n0.500000\n0.9812\n0\n\n\n\n\n\n\n\nDans la partie ci-dessus, nous avons calculé la courbe spot à partir des cash rates. Nous allons maintenant calculer la courbe spot à partir des taux forward et les taux swap. Nous pourrons ainsi prolonger la courbe spot jusqu’à 50 ans par des interpolations linéaires.\n\n\n\n\n\nCode\n# creation d'un dataframe qui va  prolonger le dataframe data_month jusqu'à 50 ans\n\nimport pandas as pd\nNombre_year = 51\nterms = [0,6, 7, 8, 9, 10, 11, 12, 15, 18] + list(range(2,Nombre_year))\nunits = ['MO' if i &lt; 10 else 'YR' for i in range(len(terms))]\n\ndata_50 = pd.DataFrame({\n    'Term': terms,\n    'Unit': units\n})\n\ndata_final = pd.merge(data_50, df, how='left', on=['Term', 'Unit'])\n\ndata_final \n\n#data_final['B(0,T)'][0] =1\ndata_final.loc[0, 'B(0,T)'] = 1\n#data_final['T'][0] = 0\ndata_final.loc[0, 'T'] = 0\ndata_final[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n0\nMO\nNaN\n0.0\nNaN\nNaN\n1.0000\n\n\n1\n6\nMO\n3.832\n0.5\n0.0\n0.5\n0.9812\n\n\n\n\n\n\n\n\n\n\nNous avons utilisé le Mid comme proxy des taux interbancaires. Il représente la moyenne arithmétique entre le “ask” et le “bid”.Nous avons utilisé la méthode de l’interpolation linéaire pour remplir les valeurs manquantes.\n\n\nCode\nfrom scipy.interpolate import interp1d\n\nnan_indices = data_final['Mid'][data_final['Mid'].isna()].index\nvalid_data = data_final.dropna(subset=['Mid'])\nfunction = interp1d(valid_data.index, valid_data['Mid'], fill_value='extrapolate')\nfor nan_index in nan_indices:\n    data_final.at[nan_index, 'Mid'] = function(nan_index)\n   \n\n\nLe code ci-dessous va permettre de prolonger la courbe des obligations spot jusqu’à 50 ans. Nous avons ainsi propagé des valeurs de la colonne T jusqu’à 50 ans.\n\n\nCode\n\n\nlast_t = data_final['T'].iloc[0]\nfor i in range(len(data_final)):\n    if pd.isna(data_final.loc[i, 'T']):\n        last_t += 1\n        data_final.loc[i, 'T'] = last_t\n    else:\n        last_t = data_final.loc[i, 'T']\n\n\n\n\n\nEn moyen terme, la courbe spot est donnée à partir des taux forward par la formule suivante:\n\\[B(0,T+\\delta) = \\frac{B(0,T)}{(1+\\delta*f )}\\]\noù \\(f\\) est le taux forward et \\(\\delta\\) est la fraction de l’année. Un prêt forward est un engagement entre 2 parties de vendre ou d’acheter un instrument financier à une date ultérieure et à un prix déterminé à l’avance.\nEn long terme, la courbe spot est donnée à partir des taux swap.Un contrat swap est un contrat d’échange de flux financiers entre deux parties. Un swap de taux est utilisé pour échanger des taux d’intérêt entre deux parties. La courbe spot est donnée par la formule suivante:\n\\[B(0,T_n) = \\frac{1 -S_{T_n} \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1+\\delta S_{T_n}}\\]\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\nimport pandas as pd\n\nimport pandas as pd\n\ndef calculate_B0T(df):\n    \"\"\"\n    Calculate B(0,T) by interpolating B(0,T1) \n    based on given B(0,T) values in a DataFrame.\n    \n    Parameters:\n    - df: DataFrame containing columns 'T', 'B(0,T)',\n    'T1', and 'Mid'.\n    \n    Returns:\n    - Updated DataFrame with interpolated 'B(0,T1)' \n    and recalculated 'B(0,T)'.\n    \"\"\"\n    \n    df['PVBP'] = 0\n    for i, row in df.iterrows():\n        if i in [0,1]:\n            continue\n        t1 = row['T1']\n        \n        if row['Unit']==\"MO\":\n            \n            ## On vérifie que l'interpolation est possible\n            \n            try:\n                # On cherche les valeurs les plus proches de t1 dans la colonne T\n                \n                valid_points = df.sort_values('T')\n                interp = interpolate.interp1d(valid_points['T'], \n                                              valid_points['B(0,T)'],\n                                              kind='linear', fill_value='extrapolate')\n                b1 = interp(t1)\n                \n                # Update the DataFrame with the interpolated value\n                df.at[i, 'B(0,T1)'] = b1  # Use 'at' for scalar value updates\n                \n                # Calcule du forward rate\n                df.at[i, 'B(0,T)'] = b1 / (1 + 0.5 * row['Mid'] / 100)\n                # Initialisation de la PVBP\n                df.at[i, 'PVBP'] = 0\n                \n                            \n            except ValueError:\n                print(f\"Impossible d'interpoler la valeur à l'index {i} avec t1 = {t1}\")\n                continue\n\n        else:\n            # Calcul de PVBP et calcul des swaps\n            df.at[i, 'PVBP'] = df.at[i-1, 'B(0,T1)'] + df.at[i-1, 'PVBP']\n            df.at[i, 'B(0,T)'] = (1- row['Mid']/100* df.at[i, 'PVBP'] )/(1+row['Mid']/100)\n            df.at[i, 'B(0,T1)'] = df.at[i, 'B(0,T)']\n    return df\n\nessai = calculate_B0T(data_final)\n\n\n\n\nLa fonction ci-dessus nous a permis de terminer la construction de notre couple. Maintenant, nous pouvons passer à une représentation graphique de la courbe spot.\n\n\nCode\n#pip install matplotlib\nimport matplotlib.pyplot as plt\nplt.plot(essai['T'], essai['B(0,T)'])\nplt.xlabel('T')\nplt.ylabel('B(0,T)')\nplt.title(\"Courbe d'obligation zéro coupon spot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLa courbe de taux zéro coupon spot est donnée par la formule suivante:\n\\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\n\n\nCode\nessai['RT'] = -np.log(essai['B(0,T)'])/essai['T']\n\n#plot the cure\n\nplt.plot(essai['T'], essai['RT'])\nplt.xlabel('Maturité en années')\nplt.ylabel('RT')\nplt.title('Courbe de taux spot sur 50 ans')\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Modèles de Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#préparation-des-données-et-calcul-de-la-courbe-des-taux-de-zéro-coupon.",
    "href": "posts/obligation/index.html#préparation-des-données-et-calcul-de-la-courbe-des-taux-de-zéro-coupon.",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "",
    "text": "L’objectif de cette partie est de construire la courbe spot des taux zéro coupon \\(r_T\\) à partir des données de marché. Il est donné par la formule suivante : \\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\nLa calibration de la courbe de taux se fait par bootstrapping à partir des cotations du marché : - CT = cash rates (les cash flow) : Ici c’est la courbe de taux qui est constatée sur le marché interbancaire - FRA = Forward Rate Agreement (les taux forward) - SWAP = Interest Rate Swap (les taux swap)\n\n\nCode\n#!pip install openpyxl\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom scipy import interpolate\n#importer un fichier xlsx : la première feuille\n\n#courses = pd.read_csv('courses.csv')\ntaux_BBG = pd.read_excel('Taux_BBG.xlsx', sheet_name='Feuil1') \n\n\n\n\n\n\nCode\n# calcul du Mid qui est la moyenne entre le bid et l'ask\n\nnouvelle_ligne = pd.DataFrame({\n    'Term': [5],  # Supposons que vous voulez ajouter une période de 5 mois au début\n    'Unit': ['MO'],\n    'Ticker': ['EUR005M'],  # Supposons un ticker pour la nouvelle ligne\n    'Bid': [3.9],  # Exemple de valeur Bid\n    'Ask': [3.9],  # Exemple de valeur Ask\n    'Spread': [None],\n    'Bid Spr Val': [0],\n    'Ask Spr Val': [0],\n    'Final Bid Rate': [None],\n    'Final Ask Rate': [None],\n    'Rate Type': [None],\n    'Daycount': [None],\n    'Freq': [None]\n}, index=[0])\ndf = pd.concat([nouvelle_ligne, taux_BBG], ignore_index=True)\n\n\n\n\n\nPour calculer la courbe de taux à court terme, nous avons besoin du Mid qui est la moyenne entre le bid et l’ask. Nous nous le considérons comme proxy des taux interbancaires par exemple le LIBOR. Il était important de considérer la liquidité de l’instrument. Les écarts plus larges entre le bid et l’ask indiquent une liquidité plus faible.Ce qui pourrait influencer l’approximation du Libor.Pour des raisons de simplicité, nous avons, nous n’avons pas pris en compte la liquidité des instruments.\n\n\nCode\ndf['Mid'] = (df['Bid'] + df['Ask']) / 2\n\n\n\n\nCode\n# Initialisation des colonnes\ndf['T'] = 0\ndf['T1'] = 0\ndf['T2'] = 0\ndf['B(0,T)'] = 0\n\n\n\n\n\n\nCode\n# calcule du premier élement de B(0,T)\ndf['B(0,T)'].astype(int)[0] = 1\n\n\n\n\n\n\nSa formule est donnée par :\n\\[B(0,T) = \\frac{1}{(1+\\delta*L )}\\]\nOù \\(L\\) est le taux de prêt interbancaire(Libor par exemple) et \\(\\delta\\) est la fraction de l’année.\n\n\nCode\n# Conversion des termes en années\n\ndf['T2'] = df['Term'].astype(int)/12\n\n\n\n\n\n\n\nCode\n\ndf['T1'] = df['T2'] - 0.5\n\n\n\n\n\n\n\nCode\n# df['T'] = df['Term'].astype(int)/12 if df['Unit'] == 'MO' else df['Term'].astype(int)\n# With apply function\ndf['T'] = df.apply(lambda x: x['Term']/12 if x['Unit'] == 'MO' else x['Term'], axis=1)\ndf[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n5\nMO\n3.900\n0.416667\n-0.083333\n0.416667\n0\n\n\n1\n6\nMO\n3.832\n0.500000\n0.000000\n0.500000\n0\n\n\n\n\n\n\n\n\n\nCode\ndf['B(0,T1)'] =0\ndf['T'].astype(int)[0] = 0\n\ndf.loc[0, 'B(0,T)'] = 1\n\n\ndf.loc[1,'B(0,T)'] = 1/(1+ df.loc[1,'Mid']/100*0.5)\n\ndf[[\"Term\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\",\"B(0,T1)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nMid\nT\nT1\nT2\nB(0,T)\nB(0,T1)\n\n\n\n\n0\n5\n3.900\n0.416667\n-0.083333\n0.416667\n1.0000\n0\n\n\n1\n6\n3.832\n0.500000\n0.000000\n0.500000\n0.9812\n0\n\n\n\n\n\n\n\nDans la partie ci-dessus, nous avons calculé la courbe spot à partir des cash rates. Nous allons maintenant calculer la courbe spot à partir des taux forward et les taux swap. Nous pourrons ainsi prolonger la courbe spot jusqu’à 50 ans par des interpolations linéaires.\n\n\n\n\n\nCode\n# creation d'un dataframe qui va  prolonger le dataframe data_month jusqu'à 50 ans\n\nimport pandas as pd\nNombre_year = 51\nterms = [0,6, 7, 8, 9, 10, 11, 12, 15, 18] + list(range(2,Nombre_year))\nunits = ['MO' if i &lt; 10 else 'YR' for i in range(len(terms))]\n\ndata_50 = pd.DataFrame({\n    'Term': terms,\n    'Unit': units\n})\n\ndata_final = pd.merge(data_50, df, how='left', on=['Term', 'Unit'])\n\ndata_final \n\n#data_final['B(0,T)'][0] =1\ndata_final.loc[0, 'B(0,T)'] = 1\n#data_final['T'][0] = 0\ndata_final.loc[0, 'T'] = 0\ndata_final[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n0\nMO\nNaN\n0.0\nNaN\nNaN\n1.0000\n\n\n1\n6\nMO\n3.832\n0.5\n0.0\n0.5\n0.9812\n\n\n\n\n\n\n\n\n\n\nNous avons utilisé le Mid comme proxy des taux interbancaires. Il représente la moyenne arithmétique entre le “ask” et le “bid”.Nous avons utilisé la méthode de l’interpolation linéaire pour remplir les valeurs manquantes.\n\n\nCode\nfrom scipy.interpolate import interp1d\n\nnan_indices = data_final['Mid'][data_final['Mid'].isna()].index\nvalid_data = data_final.dropna(subset=['Mid'])\nfunction = interp1d(valid_data.index, valid_data['Mid'], fill_value='extrapolate')\nfor nan_index in nan_indices:\n    data_final.at[nan_index, 'Mid'] = function(nan_index)\n   \n\n\nLe code ci-dessous va permettre de prolonger la courbe des obligations spot jusqu’à 50 ans. Nous avons ainsi propagé des valeurs de la colonne T jusqu’à 50 ans.\n\n\nCode\n\n\nlast_t = data_final['T'].iloc[0]\nfor i in range(len(data_final)):\n    if pd.isna(data_final.loc[i, 'T']):\n        last_t += 1\n        data_final.loc[i, 'T'] = last_t\n    else:\n        last_t = data_final.loc[i, 'T']\n\n\n\n\n\nEn moyen terme, la courbe spot est donnée à partir des taux forward par la formule suivante:\n\\[B(0,T+\\delta) = \\frac{B(0,T)}{(1+\\delta*f )}\\]\noù \\(f\\) est le taux forward et \\(\\delta\\) est la fraction de l’année. Un prêt forward est un engagement entre 2 parties de vendre ou d’acheter un instrument financier à une date ultérieure et à un prix déterminé à l’avance.\nEn long terme, la courbe spot est donnée à partir des taux swap.Un contrat swap est un contrat d’échange de flux financiers entre deux parties. Un swap de taux est utilisé pour échanger des taux d’intérêt entre deux parties. La courbe spot est donnée par la formule suivante:\n\\[B(0,T_n) = \\frac{1 -S_{T_n} \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1+\\delta S_{T_n}}\\]\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\nimport pandas as pd\n\nimport pandas as pd\n\ndef calculate_B0T(df):\n    \"\"\"\n    Calculate B(0,T) by interpolating B(0,T1) \n    based on given B(0,T) values in a DataFrame.\n    \n    Parameters:\n    - df: DataFrame containing columns 'T', 'B(0,T)',\n    'T1', and 'Mid'.\n    \n    Returns:\n    - Updated DataFrame with interpolated 'B(0,T1)' \n    and recalculated 'B(0,T)'.\n    \"\"\"\n    \n    df['PVBP'] = 0\n    for i, row in df.iterrows():\n        if i in [0,1]:\n            continue\n        t1 = row['T1']\n        \n        if row['Unit']==\"MO\":\n            \n            ## On vérifie que l'interpolation est possible\n            \n            try:\n                # On cherche les valeurs les plus proches de t1 dans la colonne T\n                \n                valid_points = df.sort_values('T')\n                interp = interpolate.interp1d(valid_points['T'], \n                                              valid_points['B(0,T)'],\n                                              kind='linear', fill_value='extrapolate')\n                b1 = interp(t1)\n                \n                # Update the DataFrame with the interpolated value\n                df.at[i, 'B(0,T1)'] = b1  # Use 'at' for scalar value updates\n                \n                # Calcule du forward rate\n                df.at[i, 'B(0,T)'] = b1 / (1 + 0.5 * row['Mid'] / 100)\n                # Initialisation de la PVBP\n                df.at[i, 'PVBP'] = 0\n                \n                            \n            except ValueError:\n                print(f\"Impossible d'interpoler la valeur à l'index {i} avec t1 = {t1}\")\n                continue\n\n        else:\n            # Calcul de PVBP et calcul des swaps\n            df.at[i, 'PVBP'] = df.at[i-1, 'B(0,T1)'] + df.at[i-1, 'PVBP']\n            df.at[i, 'B(0,T)'] = (1- row['Mid']/100* df.at[i, 'PVBP'] )/(1+row['Mid']/100)\n            df.at[i, 'B(0,T1)'] = df.at[i, 'B(0,T)']\n    return df\n\nessai = calculate_B0T(data_final)\n\n\n\n\nLa fonction ci-dessus nous a permis de terminer la construction de notre couple. Maintenant, nous pouvons passer à une représentation graphique de la courbe spot.\n\n\nCode\n#pip install matplotlib\nimport matplotlib.pyplot as plt\nplt.plot(essai['T'], essai['B(0,T)'])\nplt.xlabel('T')\nplt.ylabel('B(0,T)')\nplt.title(\"Courbe d'obligation zéro coupon spot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLa courbe de taux zéro coupon spot est donnée par la formule suivante:\n\\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\n\n\nCode\nessai['RT'] = -np.log(essai['B(0,T)'])/essai['T']\n\n#plot the cure\n\nplt.plot(essai['T'], essai['RT'])\nplt.xlabel('Maturité en années')\nplt.ylabel('RT')\nplt.title('Courbe de taux spot sur 50 ans')\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Modèles de Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#sensibilité-du-prix-dune-obligation-en-fonction-du-taux-dintérêt-et-du-spread-de-crédit",
    "href": "posts/obligation/index.html#sensibilité-du-prix-dune-obligation-en-fonction-du-taux-dintérêt-et-du-spread-de-crédit",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "2.1 Sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit",
    "text": "2.1 Sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit\nCette fois-ci, nous allons étudier la sensibilité du prix d’obligations en fonction du taux d’intérêt et le spread de crédit.\n\n\nCode\n# Tracer pt en fonction de t pour c=0.03, s=0.01 et T=10 avec un pas de 0.1\ntemps_initial = 0\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Supposons que vos fonctions et variables sont déjà définies correctement\n\nfig, ax = plt.subplots(1,2, figsize=(8,4))  # Crée une figure et deux subplots (axes)\nplage_h = np.arange(0, 1, 0.001)\n\n\n# Supposons que prix_obligation_spread soit votre fonction personnalisée\nplage_pt_h = [prix_obligation_spread(temps_initial,coupon, spread, maturite, h, essai) for h in plage_h]\nplage_pt__h = [prix_obligation_spread(temps_initial,coupon, spread, maturite, -h, essai) for h in plage_h]\n\n# Utiliser les méthodes de l'objet Axes pour tracer\nax[0].plot(plage_h, plage_pt_h,color='green')\nax[1].plot(plage_h, plage_pt__h,color='green')\n\nax[0].set_xlabel('h')\nax[0].set_ylabel('p0')\nax[1].set_xlabel('h')\nax[1].set_ylabel('p0')\n\nax[0].set_title('Sensibilité au taux pour h positifs')\nax[1].set_title('Sensibilité  au taux pour h négatifs')\n\nplt.tight_layout()  # Ajuste automatiquement le layout pour éviter le chevauchement\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Tracer pt en fonction de t pour c=0.03, s=0.01 et T=10 avec un pas de 0.1\ntemp_initial = 0\nfig,ax = plt.subplots(1,2, figsize=(8,4))\nplage_h = np.arange(0, 1, 0.001)\nplage_pt_h = [prix_obligation_spread(temp_initial, coupon, spread, maturite, h, essai) for h in plage_h]\nplage__pt_h = [prix_obligation_spread(temp_initial, coupon, spread, maturite, -h, essai) for h in plage_h]\nax[0].plot(plage_h, plage_pt_h,color='r')\nax[1].plot(plage_h, plage__pt_h,color='r')\n\n\nax[0].set_xlabel('h')\nax[0].set_ylabel('p0')\nax[1].set_xlabel('h')\nax[1].set_ylabel('p0')\n\nax[0].set_title('Sensibilité au spread pour h positifs')\nax[1].set_title('Sensibilité  au spread pour h négatifs')\n\nplt.tight_layout()  # Ajuste automatiquement le layout pour éviter le chevauchement\nplt.show()\n\n\n\n\n\n\n\n\n\nIl en ressort des graphiques ci-dessus qu’une augmentation(diminution) du taux d’intérêt ou du spread de crédit conduit à une augmentation(diminution) du prix de l’obligation zéro coupon.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Modèles de Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#valorisation-dun-produit-exotique-avec-hull-white",
    "href": "posts/obligation/index.html#valorisation-dun-produit-exotique-avec-hull-white",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "3.1 Valorisation d’un produit Exotique avec Hull White",
    "text": "3.1 Valorisation d’un produit Exotique avec Hull White\nDans cette partie, nous allons dans un premier temps définir le prix de n’importe quel zéro-coupon c’est-à-dire trouver une forme analytique de B(t,T)-qui est le prix d’une obligation sans coupon définie par la valeur en t d’une unité monétaire payée à l’échéance. Ensuite, nous allons nous hedger contre le risque de taux d’intérêt en utilisant les caplets. Un caplet est une option d’achat sur un taux d’intérêt. Il est utilisé pour se protéger contre une hausse des taux d’intérêt. Nous allons calibrer sa volatilité. Enfin, nous utiliserons cette volatilité et des simulations de Monte Carlo pour calibrer les niveaux de strike d’un caplet pour se protéger de la hausse d’un Libor. Nous savons par exemple que si le Libor est supérieur au strike, l’option est exercée.\n\n3.1.1 Calcul de B(t,T) avec Hull White\nLe modèle de Hull et White est une extension du modèle de Vasicek. Sous le modèle de Hull et White, le taux court est donné par la formule suivante: \\[dr_t=\\lambda (\\theta -  r_t)dt+\\sigma dW_t\\]\nNous allons étudier la partie simplifiée avec \\(\\lambda=0\\)\nMontrer tout d’abord que : La formule pour calculer (B(t,T)) est donnée par :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\noù l’équation différentielle stochastique pour (X_t) est :\n\\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\nNous avons déjà montrer dans le cours que :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -\\beta(t,T) X_t - \\frac{1}{2} \\beta(t,T)^2 \\phi(t) \\right)\\]\nAvec : \\[\\phi(t) =  \\exp(-2\\lambda t)*\\int_0^t \\sigma^2 \\exp(2\\lambda s) ds = \\frac{\\sigma^2}{2\\lambda} (1-\\exp(-2\\lambda t))\\] et \\[\\beta(t,T) = \\frac{1}{\\lambda} (1-\\exp(-\\lambda(T-t)))\\]\n\\[dX_t = (\\phi(t)- \\lambda X_t)dt + \\sigma dW_t\\]\nLorsque \\(\\lambda=0\\), nous avons : \\(\\phi(t) = \\sigma^2 t\\) et \\(\\beta(t,T) = T-t\\)\nDonc la formule pour calculer (B(t,T)) est donnée par :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\] et \\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\n\n\n3.1.2 Calibration de la volatilité pour un strike K=4%, une maturité T=1 et \\(\\delta\\)(la fraction de l’année) = 1/2\nLe prix du caplet dans le modèle de Hull et White est donné par la formule suivante:\n\\[\\text{Caplet}^{HW1F} = B(0, T + \\delta) \\cdot \\text{Black}\\left(\\text{Fwd} = \\frac{B(0, T)}{B(0, T + \\delta)}, \\text{maturité} = T, \\text{vol} = \\sigma\\delta, \\text{Strike} = 1 + \\delta K\\right)\\]\nEt le prix du caplet du marché est obtenu par la formule suivante:\n\\[\\text{Caplet}^{\\text{Marché}} = \\delta \\cdot B(0, T + \\delta) \\cdot \\text{Black}\\left(\\text{Fwd} = \\frac{1}{\\delta} \\left( \\frac{B(0, T)}{B(0, T + \\delta)} - 1 \\right), \\text{maturité} = T, \\text{vol} = \\text{vol\\_implicite}, \\text{Strike} = K\\right)\\]\nBlack est la formule de Black Scholes qui est utilisée pour calculer le prix d’une option européenne et la vol_implicite est la volatilité implicite qui sera considérée égale à 25%.\nCalibrer \\(\\sigma\\), revient à déterminer la volatilité qui fait correspondre le prix du caplet du marché au prix du caplet dans le modèle de Hull et White. Nous utiliserons pour ce fait la méthode de Newton.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import newton\n\n# Ecrire une fonction qui calcule le prix d'un call européen\n\ndef blackScholesCall(S,K,T,r,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\n\ndef prix_caplet(K,T,delta,sigma,data= essai):\n    f_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T_delta = f_interpolation((T + delta))\n    B_0_T = f_interpolation(T)\n    fwd = B_0_T/B_0_T_delta\n    vol = sigma* delta\n    strike = 1+delta*K\n    black = blackScholesCall(S=fwd, K=strike, T=T, r=0, sigma=vol)\n    prix_caplet = B_0_T_delta  * black\n    return prix_caplet\n    \ndef prix_caplet_maket(K,T,delta,sigma,data= essai):\n    f_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T_delta = f_interpolation((T + delta))\n    B_0_T = f_interpolation(T)\n    fwd = (B_0_T/B_0_T_delta-1)*(1/delta)\n    black = blackScholesCall(S=fwd, K=K, T=T, r=0, sigma=sigma)\n    prix_caplet_maket = B_0_T_delta  * black*delta\n    return prix_caplet_maket\n\ndef implied_volatility(K,T,delta,sigma,data,prix_caplet_maket):\n    difference = lambda sigma:np.abs( prix_caplet_maket - prix_caplet(K,T,delta,sigma,data))\n    return newton(difference, sigma)\n\n    \n\n\n\n\nCode\nstrike = 0.04\ndelta = 0.5\nsigma = 0.25\nprix_caplet_maket = prix_caplet_maket(K=strike,T=maturite,delta =delta,sigma=sigma,data = essai)\n\n\n\n\nCode\nprint(f\"Le prix du caplet est {prix_caplet_maket:.6f}\")\n\n\nLe prix du caplet est 0.002126\n\n\n\n\nCode\n\nsigma_calibre = implied_volatility(K=strike,T=maturite,delta =delta,sigma=sigma,data = essai,prix_caplet_maket = prix_caplet_maket)\n\nprint(f\"La volatilité implicite est {sigma_calibre*100:.2f}%\")\n\n\nLa volatilité implicite est 0.81%\n\n\n\n\n3.1.3 Simulation de Monte Carlo pour calibrer les niveaux de seuil\nNous savons que le Libor entre deux instants i et i+1 est donné par la formule suivante:\n\\[L(i+1) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0, {i+1})} - 1 \\right)\\]\nSi nous utilisons la formule de générale de B(t,T) calculé précédemment avec t=i et T= i+1, nous obtenons la formule suivante:\n\\[L(i,i+1) = \\frac{1}{\\delta} \\left( \\frac{B(0,i)}{B(0, {i+1})} \\exp(X_i + \\frac{1}{2} \\sigma^2 i) - 1 \\right)\\]\n\\(\\sigma\\) est la volatilité de Hull et White que nous avons calibré précédemment. Nous remarquons que, calibrer L revient à calibrer X qui est donnée par la formule suivante:\n\\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\] Nous aurons pu calibrer X en prenant des pas très petits, comme dans le graphique ci-dessous, mais nous avons décidé de calibrer X avec des pas de 1 :\n\\[X_{i+1} = X_i + \\sigma^2 (i+0.5) + \\sigma Z\\]\nOù Z est une variable aléatoire suivant une loi normale centrée réduite.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Paramètres de simulation\nT = 10  # Temps final\nN = 1000  # Nombre de pas\ndt = T/N  # Pas de temps\nsigma = sigma_calibre  # Volatilité\n\n# Initialisation\nt = np.linspace(0, T, N+1)\nX = np.zeros(N+1)\nZ = np.random.normal(0, 1, N)  # Génère N réalisations de N(0,1)\n\n# Simulation par différences finies\nfor i in range(N):\n    X[i+1] = X[i] + sigma**2 * t[i] * dt + sigma * np.sqrt(dt) * Z[i]\n\n# Affichage\nplt.plot(t, X)\nplt.xlabel('Temps')\nplt.ylabel('$X_t$')\nplt.title('Simulation de $X_t$ par différences finies')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ndef simulation_monte_carlo(delta,sigma,essai,M,alpha):\n    \"\"\"La fonction simule le prix d'un caplet par la méthode de Monte Carlo\n\n    Args:\n        delta (float): Fréquence de paiement\n        sigma (float): Volatilité\n        essai (dataframe): ce dataframe contient les données des zéro coupon\n        M (float): Nombre de simulations\n        alpha (float): Taux de référence\n\n    Returns:\n        float: prix du caplet\n    \"\"\"\n    mean_price = []\n    for i in range(M):\n        Z = np.random.normal(0, 1, 10)  # Génère N réalisations de N(0,1)\n        X = np.zeros(11)\n        f_interpolation = interpolate.interp1d(essai['T'], essai['B(0,T)'])\n        L_i_iplus1 = np.zeros(10)\n        for j in range(0,10):\n            X[j+1] = X[j] + sigma**2*(j+0.5) + sigma*Z[j]\n            \n        j_values = np.arange(1, 11)  \n        B_0_i = f_interpolation(j_values)\n        B_0_i_plus_1 = f_interpolation(j_values + 1)\n        \n        exp_X_i_plus_sigma = np.exp(X[1:] + sigma**2*0.5*j_values)\n        L_i_iplus1 = ((B_0_i / B_0_i_plus_1) * exp_X_i_plus_sigma - 1) * (1 / delta)\n        meanl = np.mean(L_i_iplus1&gt;alpha)\n        mean_price.append(meanl)\n    B_0_T = f_interpolation(10)\n    return np.mean(mean_price)*B_0_T\n        \n\n\n\n\nCode\n\ndelta = 0.5\nsigma = sigma_calibre\ndata = essai\nM = 10000\nalpha = 0.03\n\nsimulation_monte_carlo(delta,sigma,data,M,alpha)\n\nprint(f\"Pour un seuil de {alpha}, le prix du flux résultant de la simulation Monte Carlo\\\n est : {simulation_monte_carlo(delta, sigma, data, M, alpha):.2f}\")\n\n\nPour un seuil de 0.03, le prix du flux résultant de la simulation Monte Carlo est : 0.58\n\n\n\n3.1.3.1 variation du prix du flux en fonction du seuil alpha\nNous avons représenté ci-dessous le prix du flux en fonction de différents niveaux de seuil alpha. Il est ressort que le prix est décroissant en fonction du seuil alpha. Le produit est donc plus cher lorsque le seuil alpha est plus bas -inférieur à 4% et plus cher lorsque le strile est plus élevé -supérieur à 4%.\nCette analyse peut permettre de développer des stratégies d’investissement pour les investisseurs et les émetteurs.\n\n\nCode\nplage_alpha = np.arange(0, 0.11, 0.01)\nplage_prix = [simulation_monte_carlo(delta,sigma,data,M,alpha) for alpha in plage_alpha]\n\nplt.plot(plage_alpha, plage_prix)\nplt.xlabel('strike')\nplt.ylabel('prix du flux')\nplt.title('Prix du flux en fonction du strike')\nplt.axhline(y=0.5, color='r', linestyle='--')\nplt.axvline(x=0.04, color='r', linestyle='--')\nplt.xticks([0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1])\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Modèles de Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#valorisation-dune-obligation-avec-clause-de-rappel-avec-hull-white.",
    "href": "posts/obligation/index.html#valorisation-dune-obligation-avec-clause-de-rappel-avec-hull-white.",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "3.2 Valorisation d’une obligation avec clause de rappel avec Hull White.",
    "text": "3.2 Valorisation d’une obligation avec clause de rappel avec Hull White.\nLa valorisation d’une obligation avec une clause de rappel implique l’évaluation d’une obligation qui donne à l’émetteur le droit, mais pas l’obligation de rembourser le principal de l’obligation avant l’échéance à un prix prédéterminé, appelé prix de rappel, cette caractéristique offre à l’émetteur de l’obligation, la flexibilité de refinancer sa dette à un coût inférieur si les taux d’intérêt du marché baissent.\nDans cette partie, avec un exemple simple, nous allons valoriser le prix d’une option sur une obligation avec clause de rappel. Nous emploierons pour cela des simulations de Monte-Carlo afin de déterminer le prix de l’obligation avec clause de rappel. Par la suite, nous comparerons cette obligation à une obligation classique sans clause de rappel, afin de déterminer le prix d’une option de rappel elle-même. Enfin, nous procéderons à diverses analyses de sensibilité pour évaluer l’impact de différents facteurs sur la valorisation.\n\n3.2.1 Modèle de Hull et White\nNous avons déjà montrer dans le cours que :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left(-(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\nAvec : \\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\nsi on pose \\(X_0 = 0\\), nous avons : En intégrant l’équation différentielle stochastique pour \\(X_t\\), nous obtenons la formule suivante: \\[X_t = \\sigma^2 \\frac{t^2}{2} + \\sigma W_t\\]\nComme \\(W_t\\) est un mouvement brownien, nous avons que \\(W_t\\) est une variable aléatoire suivant une loi normale centrée de variance t. donc si on pose \\(U =\\sqrt{t}W_t\\), nous avons que U est une variable aléatoire suivant une loi normale centrée de variance 1. Ainsi on trouve que notre modèle de Hull et White s’écrira sous la forme suivante:\n\\[B(t,T,U,\\sigma) = \\frac{B(0,T)}{B(0,t)} \\exp \\left(-(T-t)(\\frac{1}{2} \\sigma^2 t^2+\\sigma \\sqrt(t)U)- \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\nAinsi nous pouvons calibrer par monte-Carlo le prix de l’obligation avec clause de rappel. Dans un prix temps la fonction de valorisation de l’obligation est donnée par la formule suivante: \\[P(t, T, U, \\sigma) = \\sum_{i=1}^{T} \\left[ c \\cdot B(t, i, U, \\sigma) \\cdot PS(i) \\cdot \\mathbf{1}_{(t &lt; i)} \\right] + B(t, T, U, \\sigma) \\cdot PS(T) \\cdot \\mathbf{1}_{(t &lt; T)}\\]\navec \\(PS(t)\\) la fonction de survie qui est donnée par la formule suivante: \\[PS(i)=\\exp(-s \\cdot(i-t))\\]\nLe prix de l’obligation avec clause de rappel est donné par la formule suivante:\ncomment on écrire le minimum de deux valeurs dans une fonction mathématique en latex?\n\\[P(t,T,\\sigma)^c = \\frac{1}{N} \\sum_{i=1}^{N}  \\min \\left( P(t, T, U, \\sigma) , 1 \\right)\\]\nAvec N le nombre de simulations.\nNous avons implémenté sur prix le code du ci-dessous.\n\n\nCode\nimport numpy as np\n\ndef B(t, T, U, sigma,data,h_taux,h_sigma):\n    \"\"\"\n    Calcule la valeur de B(t, T, U, sigma) selon la formule spécifiée.\n\n    Paramètres:\n    - t : Temps actuel.\n    - T : Temps final.\n    - U : Variable aléatoire ou paramètre spécifique au modèle.\n    - sigma : Volatilité.\n    \n    Retourne:\n    - La valeur calculée de B(t, T, U, sigma).\n    \"\"\"\n    f_inte = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T = f_inte(T)*np.exp(-h_taux*T)\n    B_0_t = f_inte(t)*np.exp(-h_taux*t)\n    sigma+=h_sigma\n    exp_part = np.exp(-1 * (T - t) * (0.5 * sigma**2 * t**2 + sigma * np.sqrt(t) * U) - 0.5 * (T - t)**2 * sigma**2 * t)\n    return (B_0_T / B_0_t) * exp_part\n\n\n  \n\ndef prix_obligation_Hull_White(c,s,t,T,U,sigma,data,h_taux,h_sigma):\n    \"\"\"\n    Calcule le prix d'une obligation.\n    paramètres:\n    - t: Taux d'intérêt sans risque\n    - c: Coupon de l'obligation\n    - s:spread\n    - T: Maturité de l'obligation\n    - U : Variable aléatoire ou paramètre spécifique au modèle.\n    - sigma : Volatilité.\n    - data : dataframe contenant les données\n    Returns:\n    - Prix de l'obligation\n\n    \"\"\"\n    prix = 0\n    for i in  range(1,T+1):\n        if t&lt;i:\n            ps_i = np.exp(-s*(i-t))\n            b_t_i = B(t,i,U,sigma,data,h_taux,h_sigma)\n            prix += c*b_t_i*ps_i\n    if t&lt;T:\n        ps_T = np.exp(-s*(T-t))\n        b_t_T = B(t,T,U,sigma,data,h_taux,h_sigma)\n        prix += b_t_T*ps_T\n    return prix   \n    \n# simulation des prix\n\ndef simulation_prix(c,s,t,T,sigma,data,N,h_taux,h_sigma):\n    liste_prix = []\n    for i in range(N):\n        U = np.random.normal(0,1)\n        prix = prix_obligation_Hull_White(c,s,t,T,U,sigma,data,h_taux,h_sigma)\n        liste_prix.append(prix)\n    prix_ajuste = np.minimum(liste_prix,1)\n    \n    \n    return np.mean(prix_ajuste)\n\n\n\n\nCode\nNombre_simulations = 10000\ntemps_exercice = 5\nsigma =0.01\n\nprix_c =simulation_prix(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0.01,h_sigma=0.01)\nprint(f\"Le prix de l'obligation de rappel est de  {prix_c:.2f}\")\n\n\nLe prix de l'obligation de rappel est de  0.87\n\n\n\n\n3.2.2 Calcul du prix de l’option de rappel\nNous avons initialement estimé la valeur d’une obligation zéro-coupon, sa clause de rappel Par la suite, la valorisation d’une obligation incorporant une clause de rappel a été réalisée.Afin de quantifier le coût de l’option de rappel, nous procéderons à la soustraction des 2 prix précédemment obtenus, puis à l’actualisation du résultat à la date d’exercices. Le prix de l’option de rappel est donné par la formule suivante:\n\\[\\text{option} = B(0,t) \\left[ P(t,T) - P(t,T,\\sigma)^c \\right]\\]\nRappelons que P(t,T) est donné par la formule suivante:\n\\[P_t = \\sum_{i=1}^{T} \\left[ c \\cdot B(t,i) \\cdot PS(i) \\cdot \\mathbf{1}_{(t &lt; i)} \\right] + B(t,T) \\cdot PS(T) \\cdot \\mathbf{1}_{(t &lt; T)}\n\\]\nNous avons implémenté le prix de cette option dans le code ci-dessous.\n\n\nCode\n\n\n\n\n# Créeons une fonction qui calcule le prix d'une option\n\ndef calcule_option(c,s,t,T,sigma,data,N,h_taux=0,h_sigma=0):\n    fonction_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_t = fonction_interpolation(t)\n    prix_c =simulation_prix(c,s,t,T,sigma,data,N,h_taux,h_sigma)\n    prix = prix_obligation_shift(t, c, s,T, h_taux,data) \n    option = B_0_t * (prix - prix_c)\n    return option\n\noption = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0.01,h_sigma=0.01)\nprint(f\"Le prix de l'option est {option}\")\n\n\nLe prix de l'option est 0.05313592167256735\n\n\n\n\nCode\nplage_t=np.arange(1,10)\nplage_option = [calcule_option(c=coupon,s=spread,t=t,T=maturite,sigma=sigma_calibre,data=essai,N=Nombre_simulations) for t in plage_t]\n    \nplt.plot(plage_t, plage_option)\nplt.xlabel('t')\nplt.ylabel('Option')\n\nplt.title('Prix de l\\'option en fonction de t')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nLe graphique ci-dessus présente le prix de l’option du rappel en fonction du temps. Nous remarquons que si l’émetteur exerce son option prématurément, il devra payer un prix plus élevé. Cependant, lorsqu’on se rapproche de la maturité de l’obligation, le prix de l’option de rappel diminue.\n\n\n3.2.3 Sensibilité du prix de l’option de rappel en fonction de la date d’exercie, de la volatilité et du taux d’intérêt.\n\n3.2.3.1 Sensibilité du prix de l’option de rappel en fonction de la volatilité.\nLe code ci-dessous représente la sensibilité du prix de l’option en fonction de la volatilité. Comme nous pouvons le constater, une augmentation de la volatilité conduit à une augmentation du prix de l’option de rappel.\n\n\nCode\n# Je veux une plage de volatilité {0%,0.25%,0.5%,...,5%}\npas = 0.25 / 100\nplage_sigma =  np.arange(0, 10 * pas, pas)\nplage_option_sigma = [calcule_option(c=0.03,s=0.01,t=5,T=10,sigma=sigma,data=essai,N=10000) for sigma in plage_sigma]\nplt.plot(plage_sigma, plage_option_sigma)\nplt.xlabel('Volatilité')\nplt.ylabel('Option')\nplt.title('Prix de l\\'option en fonction de la volatilité')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2.3.2 Sensibilité du prix de l’option de rappel en fonction du taux d’intérêt.\nLe code ci-dessous représente la sensibilité du prix de l’option en fonction du taux d’intérêt. Comme nous pouvons le constater, une augmentation infinitésimale du taux d’intérêt conduit à une diminution du prix de l’option de rappel.\n\n\nCode\nplage_h_taux = np.arange(0, 0.11, 0.01)\nplage_option_h_taux = [calcule_option(c=0.03,s=0.01,t=5,T=10,sigma=0.01,data=essai,N=10000,h_taux=h_taux) for h_taux in plage_h_taux]\nplt.plot(plage_h_taux, plage_option_h_taux)\n\nplt.ylabel('Option')\nplt.title(\"Prix de l'option en fonction d'une variation du taux d'intérêt\")\nplt.xticks([0,0.05,0.1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2.3.3 Sensibilité du prix de l’option : Delta et Vega.\n\nLe delta est la sensibilité du prix de l’option par rapport au sous-jacent qui est ici le taux d’intérêt. Il est donné par la formule suivante:\n\n\\[\\Delta =\\frac{option_{h}-option}{h}\\]\nOù \\(option_{h}\\) est le prix de l’option avec un taux d’intérêt augmenté de h.\nIl peut aussi être utilisé pour se couvrir contre le risque de taux d’intérêt.\n\nLe Vega est la sensibilité du prix de l’option par rapport à la volatilité. Il est donné par la formule suivante:\n\n\\[\\text{Vega} =\\frac{option_{\\sigma+h}-option_{\\sigma}}{h}\\]\nOù \\(option_{\\sigma+h}\\) est le prix de l’option avec une volatilité augmentée de h.\nUne obligation avec un Vega null est moins sensible à la volatilité du taux d’intérêt.\nLe code ci-dessous permet d’implémenter le calcul du delta et du vega.\nNous observons qu’une augmentation infinitésimale du taux d’intérêt et de la volatilité conduit à une diminution du prix de l’option de rappel.\n\n\nCode\nsigma = 0.01\nh =0.001\noption_h = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=h,h_sigma=0.01)\noption_0 = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=0.01)\ndelta = (option_h - option_0) /h\nprint(f\"Le delta de l'option est {delta:.2f}\")\n\n\nLe delta de l'option est -3.16\n\n\n\n\nCode\n# Calcul des Vega\nh = 0.0001\noption_haut = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=h)\noption_bas = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=0)\n                            \n                            \nvega = (option_haut - option_bas) / h\nprint(f\"Le vega de l'option est {vega:.2f}\")\n\n\nLe vega de l'option est -16.03",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Modèles de Courbe de taux"
    ]
  },
  {
    "objectID": "posts/Simulation/index.html",
    "href": "posts/Simulation/index.html",
    "title": "Simulation of a real and continuous random variable",
    "section": "",
    "text": "Introduction\nIn the context of a project on copula theory, I realized that simulating random variables is not as simple as it seems. I have therefore decided to delve into the subject. In this paper, we will focus on the simulation of a real and continuous random variable X. We will use the strengths of the Python language to empirically address (with observed data) the distribution of a random variable, the law of large numbers, and the central limit theorem. I will be inspired by the book “The R Software: Mastering the Language - Performing Statistical Analyses” by Pierre Lafaye de Micheaux, Rémy Drouilhet and Benoit Liquet. I think this book is very interesting for any beginner in statistics and programming. It is very well written and very pedagogical. I highly recommend it.\n\n\nThe foundation of simulation: the uniform distribution on [0, 1]\nI have often wondered how computers generate random numbers, and another question that bothered me was how to generate random numbers that follow a given distribution. This question is easy, you’ll see that it all comes down to the uniform distribution. And knowing how to simulate a uniform distribution will allow you to simulate any random variable distribution for which you know the cumulative distribution function. Let me explain why.\nThe uniform distribution is a continuous probability distribution on the interval . It is defined by the following cumulative distribution function: \\(F(x) = x\\) if \\(x \\in [0, 1]\\) and \\(F(x) = 0\\) if \\(x &lt; 0\\) et \\(F(x) = 1\\) if \\(x &gt; 1\\). The probability density is given by \\(f(x) = 1\\) if \\(x \\in [0, 1]\\) and \\(f(x) = 0\\) otherwise. Furthermore, the cumulative distribution function is increasing and continuous.\nNow, let’s consider a real continuous random variable X, that is, one defined on a measurable space \\((\\Omega, \\mathcal{F}, P)\\) with a real-valued cumulative distribution function F defined by : \\[F(x) = P(X \\leq x)\\]\nAn important result in simulation is that the random variable F(X) follows a uniform distribution. Indeed, we have: \\[P(F(X) \\leq x) = P(X \\leq F^{-1}(x)) = F(F^{-1}(x)) = x\\]\nThis means that we can write \\(F(X) = U\\), equivalent to \\(X = F^(-1)(U)\\), where U follows a uniform distribution on . This means that if we know how to simulate a uniform distribution, we can simulate any random variable distribution. This is what we will do in the following.\n\n\nSimulation of the uniform distribution on [0, 1]\nTo simulate a uniform distribution on , we will use the random function from the Python random module. This function generates random numbers following a uniform distribution on . We will generate 1000 random numbers following a uniform distribution on and represent them graphically.\n\nimport random\nimport matplotlib.pyplot as plt\n\n# Simulation de la loi uniforme sur [0, 1]\nn = 1000\nu = [random.random() for i in range(n)]\n\n# Représentation graphique\nplt.plot(u, 'o', label=\"Loi uniforme sur [0, 1]\")\nplt.title(\"Simulation de la loi uniforme sur [0, 1]\")\nplt.show()\n\n\n\n\n\n\n\n\nWe notice that the values are uniformly distributed on the interval . This means that the simulation is correct. We will now simulate an exponential distribution with parameter \\(\\lambda = 1\\). Its cumulative distribution function is given by: \\[F(x) = 1 - e^{-x}\\] And its inverse function is given by: \\[F^{-1}(x) = -\\ln(1-x)\\]\nWe will simulate 1000 values of the exponential distribution with parameter \\(\\lambda = 1\\) and represent them graphically.\n\nimport numpy as np\n\n# Simulation de la loi exponentielle de paramètre lambda = 1\nx = [-np.log(1 - u[i]) for i in range(n)]\n\n# Représentation graphique\nplt.hist(x, bins=30, density=True, label=\"Loi exponentielle de paramètre lambda = 1\")\nplt.title(\"Simulation de la loi exponentielle de paramètre lambda = 1\")\nplt.show()\n\n\n\n\n\n\n\n\nWe notice that the values are well distributed according to an exponential distribution with parameter \\(\\lambda = 1\\). We will superimpose the density of an exponential distribution with parameter equal to 1 to show how perfect our simulation is.\n\nimport scipy.stats as stats\n\n# Représentation graphique\nplt.hist(x, bins=30, density=True, label=\"Loi exponentielle de paramètre lambda = 1\")\nplt.plot(np.linspace(0, 6, 100), stats.expon.pdf(np.linspace(0, 6, 100)), label=\"Densité de la loi exponentielle de paramètre lambda = 1\")\nplt.title(\"Simulation de la loi exponentielle de paramètre lambda = 1\")\nplt.show()\n\n\n\n\n\n\n\n\nThank you for reading. If you have any questions or comments, please do not hesitate to contact me. I will be happy to answer you."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/PSI_V_cramer.html",
    "href": "posts/TechniquesMethodesScoring/PSI_V_cramer.html",
    "title": "PSI and CRAMER’S V",
    "section": "",
    "text": "When dealing with credit risk models, we sometimes want to assess the representiveness between two databases Let’s assume that you have divided your database into a training set and it test set. In this case, we can use the population stability index(PSI) and CRAMER’s V to assess the representativeness between the two databases. This document will be divided into three parts. In the first part, I will explain how the PSI works. In the second part of your explain how Cramer’s V works. Finally, I will guide you through how implementing PSI and Cramer’s V in Python using real data."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#introduction",
    "href": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#introduction",
    "title": "PSI and CRAMER’S V",
    "section": "",
    "text": "When dealing with credit risk models, we sometimes want to assess the representiveness between two databases Let’s assume that you have divided your database into a training set and it test set. In this case, we can use the population stability index(PSI) and CRAMER’s V to assess the representativeness between the two databases. This document will be divided into three parts. In the first part, I will explain how the PSI works. In the second part of your explain how Cramer’s V works. Finally, I will guide you through how implementing PSI and Cramer’s V in Python using real data."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#population-stability-index-psi.",
    "href": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#population-stability-index-psi.",
    "title": "PSI and CRAMER’S V",
    "section": "2 Population Stability Index (PSI).",
    "text": "2 Population Stability Index (PSI).\nWe can define the PSI As a measure of how much a population have shifted over time, or between two different samples of a population (e.g., training and test data). In the case of two different samples of a population , the training and the test set, the PSI can be calculated as follows: \\[\nPSI = \\sum_{bucket} (P_{train} - P_{test}) \\times \\log(\\frac{P_{train}}{P_{test}})\n\\] where \\(P_{train}\\) and \\(P_{test}\\) are the proportion of the population in each bucket for the training and test set, respectively."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#cramers-v",
    "href": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#cramers-v",
    "title": "PSI and CRAMER’S V",
    "section": "3 CRAMER’s V",
    "text": "3 CRAMER’s V\nThe Cramer’s V test it is goodness of fit test applied to binned data. Continuous knows that time must be bind into a fixed number of buckets. This number should be determined statically using clustering methodology. The segmentation defined must be propagated on all the historical data.\nThe Cramer’s V Consider the size of the portfolios and the number of classes, there is a link between \\(\\chi^2\\) statistics and Cramer’s V: \\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(c, r) - 1)}}\n\\] where \\(c\\) is the number of columns, \\(r\\) is the number of rows, and \\(n\\) is the total number of observations.\nConsidering a simple. using the binned value of a variable in the training set and binned value of the same variable in the test set using the same segmentation. We will use the value of the Cramer’s V statistic. - If the value is low. It’s mean there’s no link between the variable and the belonging to the training or test set. - If the value is high, they say link between the variable and the belonging to the training or test set."
  },
  {
    "objectID": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#implementation",
    "href": "posts/TechniquesMethodesScoring/PSI_V_cramer.html#implementation",
    "title": "PSI and CRAMER’S V",
    "section": "4 Implementation",
    "text": "4 Implementation\nBefore implementing. ensure that you have done the same The same treatment in the test set as you have done in the training set.\n\n\n\n\n\n\nAlgorithm to compute the p-value under the bootstrap method\n\n\n\n\nVariables :\n\n( x_1, …, x_n ) : the observations\n( t_{obs} ) : the observed value of the test statistic\nB : the number of bootstrap samples\n\nBegin :\n\nFor b in 1 to B : - Sample with replacement m observations from ( x_1, …, x_m ) to get ( x_1^b, …, x_m^b ) - Compute the test statistic \\(T_b\\) on the bootstrap sample ( x_1^b, …, x_n^b ) - Compute the p-value : ( p-value = {i=1}^{B} (T_i &gt; t{obs}) )"
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "title": "Modélisation de la value at risk",
    "section": "VaR historique :",
    "text": "VaR historique :\nIci on estime la distribution des rendements R par la fonction de répartition empirique du vecteur d’observations. La VaR est alors donnée par le quantile empirique d’ordre \\(1-\\alpha\\) :\n\\(\\hat{VaR}_h(\\alpha) = \\hat{F_n^{-1}}(1-\\alpha)\\) avec \\(\\hat{F_n}(1-\\alpha)= \\frac{1}{n} \\sum_{i=1}^{n} 1_{R_i \\leq (1-\\alpha)}\\)\nNous utiliserons ainsi la fonction numpy.percentile pour calculer la VaR historique.\n\ndef hist_var(returns, index, fenetre, seuil):\n    \"\"\"Cette fonction calcule la Value at Risk (VaR) historique d'une série temporelle de log rendements\n\n    Args:\n        returns (numpy_array ): serie de log rendements\n        index (int): indice maximal de la série à considérer pour le calcul\n        fenetre (int): nombre de jours sur lesquels on calcule la VaR\n        seuil (float): niveau de confiance de la VaR\n\n    Return:\n        float: VaR historique\n    \"\"\"\n    return np.percentile(returns[index-fenetre:index], 100*(1-seuil))\n\n\nvar_hist= hist_var(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_hist)\n\n-0.04320825141346711",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "title": "Modélisation de la value at risk",
    "section": "Backtesting",
    "text": "Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist for i in range(test_size)], label=\"Non parametric VaR\", color = 'red')\nlist_exceptions_np = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist]\nplt.scatter(test_close.index[list_exceptions_np], test_close['log_return'][list_exceptions_np], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)/test_size}\")\n\nLe nombre d'exceptions pour la VaR non paramétrique est: 10\nLe pourcentage d'exceptions pour la VaR non paramétrique est: 0.0046146746654360865\n\n\nNous allons maintenant vérifier si la probabilité d’exception est statiquement égale à \\(\\alpha\\).\n\nfrom scipy import stats\n\ntest_except_np = stats.binomtest(len(list_exceptions_np), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np.pvalue}')\n\nla p-value du test binomial est: 0.009043369613659823\n\n\nLa pvalue du test est inférieure au seuil de 5%. On rejette donc l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR n’est pas satisfaisante.\n\nES_np = np.mean([r for r in train_close['log_return'] if r&lt;var_hist])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.054387464763168906",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "title": "Modélisation de la value at risk",
    "section": "VaR historique Bootstrap",
    "text": "VaR historique Bootstrap\nPour la VaR historique boostrap, nous allons construire B réplications bootstrap de la série des log rendements. Pour chaque réplication(b), nous allons calculer la VaR historique. La VaR historique bootstrap est alors donnée par la moyenne des VaR historiques des B réplications bootstrap.\n\\(\\hat{VaR}_{h,bootstrap}(\\alpha) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{VaR}_{h,b}(\\alpha)\\)\nNous avons aussi calculé l’intervalle de confiance à 95% de la VaR historique bootstrap. Cet intervalle est donné par les quantiles empiriques d’ordre 2.5% et 97.5% des VaR historiques des B réplications bootstrap.\n\n\ndef VaR_Hist_Bootstrap(returns, seuil, num_simulations, alpha_IC, n_B):\n\n\n    VaRs_boot = np.zeros(num_simulations)\n\n    for i in range(num_simulations):\n        sample = np.random.choice(returns, n_B, replace=True)\n        VaRs_boot[i] = hist_var(sample, len(sample), len(sample), seuil)\n\n    VaR = np.mean(VaRs_boot)\n\n    lower_bound = np.percentile(VaRs_boot, 100 * (1-alpha_IC) / 2)\n    upper_bound = np.percentile(VaRs_boot, 100 * (1 - (1-alpha_IC) / 2))\n    IC = (lower_bound, upper_bound)\n\n    return VaR, IC\n\n\nseuil = 0.99\nseuil_IC = 0.9\nnum_simulations = 5000\nn_B = 251*10 #on utilise 10 ans de données comme taille d'échantillon bootstrap\nvar_hist_boot, IC_hist_boot = VaR_Hist_Bootstrap(train_close[\"log_return\"], seuil, num_simulations,\n                                                 seuil_IC, n_B) \nprint(f\"La VaR historique bootstrap: {var_hist_boot}\")\nprint(f\"L'intervalle de confiance associé est: {IC_hist_boot}\")\n\nLa VaR historique bootstrap: -0.03986492710978063\nL'intervalle de confiance associé est: (-0.04339186830234299, -0.036106484963320654)\n\n\nNotre estimation bootstrap de la VaR se trouve bien dans l’intervalle de confiance à 90%. Nous pouvons donc conclure que notre estimation de la VaR est satisfaisante.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "title": "Modélisation de la value at risk",
    "section": "Backtesting",
    "text": "Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist_boot for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist_boot]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['log_return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Bootstrap non paramétrique est: 16\nLe pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: 0.007383479464697739\n\n\n\ntest_except_np_boot = stats.binomtest(len(list_exceptions_np_boot), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np_boot.pvalue}')\n\nla p-value du test binomial est: 0.2783933759125071\n\n\nLe test binomial vient confirmer les conclusions faites à partir de l’intervalle de confiance estimé.\n\nES_np_boot = np.mean([r for r in train_close['log_return'] if r&lt;var_hist_boot])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np_boot}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.05028777316523479",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "title": "Modélisation de la value at risk",
    "section": "VaR gaussienne",
    "text": "VaR gaussienne\nLa VaR gaussienne suppose que les rendements suivent une loi normale. Dans ce cas, on a \\(P(R&lt;VaR_h(\\alpha)) = 1-\\alpha\\) qui est équivalent à \\(P(\\frac{R-\\mu}{\\sigma} &lt; \\frac{VaR_h(\\alpha)-\\mu}{\\sigma}) = 1-\\alpha\\) en supposant que R suit une loi normale de moyenne \\(\\mu\\) et d’écart type \\(\\sigma\\). On a alors \\(VaR_h(\\alpha) = \\mu + \\sigma \\Phi^{-1}(1-\\alpha)\\). On peut estimer les paramètres à partir de l’échantillon. On a alors \\(\\hat{VaR}_h(\\alpha) = \\hat{\\mu} + \\hat{\\sigma} \\Phi^{-1}(1-\\alpha)\\).\n\nfrom scipy import stats\ndef var_gaussienne(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR = mean_returns + sd_returns * stats.norm.ppf(1-seuil)\n    return VaR\n\n\n## VaR gaussienne sur base d'apprentissage\n\nvar_gaus = var_gaussienne(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_gaus)\n\n-0.034948710985183845\n\n\n\nValidation\n\n# analyse graphique avec les densités des distributions\nplt.figure(figsize = (12,8))\nplt.hist(train_close[\"log_return\"], bins=30, density=True, color='blue', label = 'log rendements')\nplt.axvline(var_gaus, color='red', linestyle='dashed', linewidth=2, label=f'VaR Gaussienne ({var_gaus})')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"]))\nplt.plot(x, p, label = 'Loi normale', linewidth=2)\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOn remarque que la queue de la loi normale n’est pas assez lourde pour nos données. La loi normale aura donc tendance à mal estimer les queues de distribution.\n\n## Analyse graphique avec le QQ-plot\n\nplt.figure(figsize=(12, 8))\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\nNous constatons que lorsqu’on utilise la loi normale pour la modélisation de la VaR, au niveau des queues de distribution,les quantiles théoriques sont moins élévés que les quantiles empiriques à gauche et plus élevés à droite. Cela signifie que la loi normale sous-estime la probabilité d’exception. La loi normale semble donc ne pas être adaptée pour modéliser la VaR.\n\n## Test d'adéquation\n\n# Test de Kolmogorov-Smirnov\nks_statistic, ks_p_value = stats.kstest(train_close[\"log_return\"], 'norm', args = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])))\nprint(f\"Test de Kolmogorov-Smirnov - Statistique : {ks_statistic},\\nP-value : {ks_p_value}\")\n\nTest de Kolmogorov-Smirnov - Statistique : 0.05703854924225127,\nP-value : 7.95942063378766e-19\n\n\nOn rejette l’hypothèse nulle selon laquelle les log rendements suivent une distribution normale.\n\n\nReprésentation graphique\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_gaus for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_gaus]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['log_return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnalyse des exceptions\n\nprint(f\"Le nombre d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)/test_size}\")\n\nLe nombre d'exceptions pour la VaR gaussienne est: 24\nLe pourcentage d'exceptions pour la VaR gaussienne est: 0.011075219197046609\n\n\n\ntest_except_gaus = stats.binomtest(len(list_exceptions_gaus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus.pvalue}')\n\nla p-value du test binomial est: 0.5883566372303766\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%.\n\n\nVaR gaussienne à 10 jours\n\n## Var gaussienne à 10 jours par la méthode de scaling\n\nperiode = 10\nvar_gaus_scaling = np.sqrt(periode)*var_gaus\nprint(f\"La VaR gaussienne à 10 jours par la méthode de scaling est: {var_gaus_scaling}\")\n\nLa VaR gaussienne à 10 jours par la méthode de scaling est: -0.11051752800012811\n\n\n\n## VaR gaussienne à 10 jours par la méthode de diffusion\nfrom numpy import random\n\nperiode = 10\nn_simul = 10000\nS0 = train_close['Close'].iloc[-1]\nmean_returns = np.mean(train_close[\"log_return\"])\nsd_returns = np.std(train_close[\"log_return\"])\nsimulations = []\nfor k in range(n_simul):\n    simul_k=[S0]\n    for _ in range(periode):\n        Z = random.standard_normal()\n        dS = simul_k[-1]*mean_returns + simul_k[-1]*sd_returns*Z\n        simul_k.append(simul_k[-1]+dS)\n    simulations.append(simul_k)\n\nrend10 = [np.log(simul[10] / S0) for simul in simulations]\n\nalpha = 0.99\nvar_gaus_diff = np.percentile(rend10, 100*(1-alpha))\nprint(f\"La VaR gaussienne à 10 jours par la méthode de diffusion est: {var_gaus_diff}\")\n\nLa VaR gaussienne à 10 jours par la méthode de diffusion est: -0.10788230585355442\n\n\n\n\nVaR gaussienne pondérée :\nUne façon de corriger la VaR, est de pondérer la moyenne et l’écart type des rendements. On peut utiliser une moyenne mobile pondérée.\n\nfrom scipy import stats\ndef var_gaussienne_ewma(returns, index, fenetre, seuil, lambd):\n    rendements = returns[index-fenetre:index]\n    n = len(rendements)\n    poids = [(lambd**i)*(1-lambd) for i in range(n)]\n    denom = sum(poids)\n    poids_pond = [poid/denom for poid in poids]\n    moy_pond = np.sum([poids_pond[i]*rendements[n-i-1] for i in range(n)])\n    variance_pond = np.sum([poids_pond[i]*(rendements[n-i-1]-moy_pond)**2 for i in range(n)])\n    VaR = moy_pond + np.sqrt(variance_pond) * stats.norm.ppf(1-seuil)\n    return moy_pond, variance_pond, VaR\n\n\n## Calcul \n\nlambd1= 0.9\nlambd2 = 0.95\nlambd3 = 0.99\nmoy_pond1, variance_pond1, VaR1 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd1)\nmoy_pond2, variance_pond2, VaR2 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd2)\nmoy_pond3, variance_pond3, VaR3 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd3)\n\nlist_exceptions_gaus1 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR1]\nprint(100*\"_\"+ f\"\\nlambda = {lambd1}\\nMoyenne pondérée: {moy_pond1}\\nVariance pondérée: {variance_pond1}\\nVaR gaussienne EWMA: {VaR1}\\nNombre d'exceptions: {len(list_exceptions_gaus1)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus1)/test_size}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Premier graphique\naxes[0].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[0].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[0].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[0].plot(ts_close.index[train_size:], [VaR1 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd1})\", color='red')\naxes[0].scatter(test_close.index[list_exceptions_gaus1], [test_close['log_return'][i] for i in list_exceptions_gaus1], color='red', label='Exceptions')\naxes[0].set_title('Lambda = 0.9')\naxes[0].legend()\n\nlist_exceptions_gaus2 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR2]\nprint(100*\"_\"+ f\"\\nlambda = {lambd2}\\nMoyenne pondérée: {moy_pond2}\\nVariance pondérée: {variance_pond2}\\nVaR gaussienne EWMA: {VaR2}\\nNombre d'exceptions: {len(list_exceptions_gaus2)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus2)/test_size}\")\n\n# Deuxième graphique\naxes[1].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[1].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[1].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[1].plot(ts_close.index[train_size:], [VaR2 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd2})\", color='red')\naxes[1].scatter(test_close.index[list_exceptions_gaus2], [test_close['log_return'][i] for i in list_exceptions_gaus2], color='red', label='Exceptions')\naxes[1].set_title('Lambda = 0.95')\naxes[1].legend()\n\n\nlist_exceptions_gaus3 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR3]\nprint(100*\"_\"+ f\"\\nlambda = {lambd3}\\nMoyenne pondérée: {moy_pond3}\\nVariance pondérée: {variance_pond3}\\nVaR gaussienne EWMA: {VaR3}\\nNombre d'exceptions: {len(list_exceptions_gaus3)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus3)/test_size}\")\n\n# Troisième graphique\naxes[2].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[2].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[2].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[2].plot(ts_close.index[train_size:], [VaR3 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd3})\", color='red')\naxes[2].scatter(test_close.index[list_exceptions_gaus3], [test_close['log_return'][i] for i in list_exceptions_gaus3], color='red', label='Exceptions')\naxes[2].set_title('Lambda = 0.99')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n____________________________________________________________________________________________________\nlambda = 0.9\nMoyenne pondérée: 0.003249804834415138\nVariance pondérée: 0.00010705135281489561\nVaR gaussienne EWMA: -0.020819898531762165\nNombre d'exceptions: 67\nPourcentage d'exceptions: 0.03091832025842178\n____________________________________________________________________________________________________\nlambda = 0.95\nMoyenne pondérée: 0.0022669059482574804\nVariance pondérée: 0.00018626179534529804\nVaR gaussienne EWMA: -0.029482569211930626\nNombre d'exceptions: 35\nPourcentage d'exceptions: 0.016151361329026302\n____________________________________________________________________________________________________\nlambda = 0.99\nMoyenne pondérée: 0.0004258509321423379\nVariance pondérée: 0.00022231763612603746\nVaR gaussienne EWMA: -0.034260739803697746\nNombre d'exceptions: 26\nPourcentage d'exceptions: 0.011998154130133826\n\n\n\n\n\n\n\n\n\n\ntest_except_gaus1 = stats.binomtest(len(list_exceptions_gaus1), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus1.pvalue}')\n\nla p-value du test binomial est: 3.0751996180426066e-15\n\n\n\ntest_except_gaus2 = stats.binomtest(len(list_exceptions_gaus2), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus2.pvalue}')\n\nla p-value du test binomial est: 0.006672492499143829\n\n\n\ntest_except_gaus3 = stats.binomtest(len(list_exceptions_gaus3), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus3.pvalue}')\n\nla p-value du test binomial est: 0.3304256909292766\n\n\nSeule la VaR estimée avec \\(\\lambda\\) = 0.99 a une p value supérieure au seuil de 5%.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "title": "Modélisation de la value at risk",
    "section": "Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements",
    "text": "Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements\n\n\nx_values = np.linspace(min(test_close['log_return']), max(test_close['log_return']), 1000)\n\nmu, sigma, skew, df =est_params\n\ntheoretical_density = SkStudentPdf(x_values, mu, sigma, skew, df)\nplt.figure(figsize = (10,8))\nplt.hist(test_close['log_return'], bins=30, density=True, alpha=0.5, label='Données empiriques')\n\nplt.plot(x_values, theoretical_density, label='Densité Skew Student', color='red')\n\n# Personnalisation du graphique\nplt.xlabel('Rendements')\nplt.ylabel('Densité')\nplt.title('Comparaison entre les données et la densité théorique')\nplt.legend()\n\n# Affichage du graphique\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densité théorique de la skew-student est très proche de la densité empirique des log rendements. Nous pouvons donc conclure que la skew-student est une bonne modélisation des log rendements.\n\nFonction de repartition de la skew-student et fonction quantile\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\n\ndef integrale_SkewStudent(x):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: SkStudentPdf(x, mu_est, sigma_est, gamma_est, nu_est), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha):\n    value = integrale_SkewStudent(x)-alpha\n    return abs(value)\n\ndef theoretical_quantile(alpha):\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha))\n        return resultat_minimisation.x\n\n\nLe code ci-dessus nous a permis de construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l’inverse de cette fonction de repartition. Cette fonction quantile est le coeur de la modélisation de la VaR.\n\n\nQQ plot :\nLe graphique de QQ plot nous permettra de discuter de la qualité d’ajustement de la loi skew-student à la série des log rendements.\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\n\nquantiles_empiriques = np.quantile(train_close['log_return'], niveaux_quantiles)\nquantiles_theoriques = [theoretical_quantile(alpha) for alpha in niveaux_quantiles]\n\n\n\n# Créer le QQ plot\nplt.figure(figsize=(8, 8))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles théoriques')\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nNous constatons que la loi skew-student est une bonne approximation de la distribution des log rendements. Même si on constate des écarts aux queues de distribution, la loi skew-student semble bien modéliser la distribution des log rendements.\n\n\ncomparaison entre loi gaussienne et loi de Skew Student\n\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.xlabel('Quantiles théoriques (distribution loi normale)')\nplt.ylabel('Quantiles empiriques')\nplt.title(\"QQ-plot d'une modélisation par loi normale\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi Skew Student\")\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nComme on peut le voir sur le graphique ci-dessus, la loi skew-student est une meilleure approximation de la distribution des log rendements que la loi normale. En effet, les écarts aux queues de distribution sont moins importants pour la loi skew-student que pour la loi normale.\n\n\nCalcul de la VaR Skew Student\n\ndef var_skewstudent(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR =  theoretical_quantile(1-seuil)\n    return VaR\n\n\n## VaR Skew Student sur base d'apprentissage\n\nvar_skew = var_skewstudent(train_close[\"log_return\"], train_size, train_size, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_skew)\n\n-0.03966428515565673\n\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_skew for i in range(test_size)], label=\"VaR Skew Student\", color = 'red')\nlist_exceptions_skew = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_skew]\nplt.scatter(test_close.index[list_exceptions_skew], test_close['log_return'][list_exceptions_skew], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Skew student est: {len(list_exceptions_skew)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Skew Student est: {len(list_exceptions_skew)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Skew student est: 16\nLe pourcentage d'exceptions pour la VaR Skew Student est: 0.007383479464697739\n\n\n\ntest_except_skew = stats.binomtest(len(list_exceptions_skew), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_skew.pvalue}')\n\nla p-value du test binomial est: 0.2783933759125071\n\n\nLa p-value du test binomial est au dessus du seuil de 5%. On ne peut donc pas rejeter l’hypothèse selon laquelle la probabilité d’exception est de 1%. Le modèle VaR Skew Student estimé semble donc satisfaisant.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "title": "Modélisation de la value at risk",
    "section": "Commentaires",
    "text": "Commentaires\nNous avons estimé jusque là 10 valeurs différentes de la VaR. Ces différentes valeurs estimées correspondent à des approches différentes. Nous avons à chaque fois estimer une VaR 99% à horizon 1 jour. Le taux d’exceptions attendu afin de juger de la bonne qualité de la VaR est de 1%. - La VaR dynamique est particulière car, comme son nom l’indique, elle est dynamique donc pas constante au cours du temps. Elle a plus d’exceptions car elle est testée sur plus de données que les autres VaRs. Toutefois son taux d’exceptions se rapproche de celui des autres VaRs. - Le test binomial fournit une p-value inférieure au seuil de 5% pour les VaRs non paramétrique, Gaussienne EWMA (\\(\\lambda\\) = 0.9 et \\(\\lambda\\) = 0.95), dynamique et TVE par maxima par blocs. Ce qui sous entend que le taux d’exception est statistiquement différent du taux attendu de 1%. On en déduit donc que les modèles utilisés sont peu adéquats pour nos données. - La VaR non paramétrique et les VaR Gaussiennes EWMA pour \\(\\lambda\\) faible ont tendance à sous estimer la VaR tandis que la VaR dynamique et la VaR TVE par maxima par blocs ont tendance à sur estimer la vraie valeur de la VaR - De plus, la validation ex-ante de la VaR TVE POT nous indique une inadéquation entre les quantiles théoriques et les quantiles empiriques des excès. - Nous avons également montré que la VaR Skew Student semblait nettement plus valide que la VaR gaussienne. Toutefois, nous avons également estimé une amélioration de la VaR gaussienne qui est la VaR gaussienne pondérée par la méthode EWMA. - Les VARs qui nous semblent donc les plus pertinentes sont la VaR Bootstrap non paramétrique, la VaR gaussienne EWMA (\\(\\lambda\\)=0.99) et la VaR Skew Student.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "title": "Modélisation de la value at risk",
    "section": "Modélisation du GARCH sur les résidus.",
    "text": "Modélisation du GARCH sur les résidus.\nNous allons appliquer un modèle GARCH(1,1) sur les résidus.\n\ngarch_final_model = arch_model(residus, vol='Garch', p=1, q=1).fit(disp='off')\nprint(garch_final_model.summary())\n\n                     Constant Mean - GARCH Model Results                      \n==============================================================================\nDep. Variable:                   None   R-squared:                       0.000\nMean Model:             Constant Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                19340.9\nDistribution:                  Normal   AIC:                          -38673.8\nMethod:            Maximum Likelihood   BIC:                          -38646.6\n                                        No. Observations:                 6500\nDate:                Wed, Apr 24 2024   Df Residuals:                     6499\nTime:                        01:52:11   Df Model:                            1\n                                 Mean Model                                 \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nmu         3.3282e-04  1.553e-04      2.144  3.207e-02 [2.851e-05,6.371e-04]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9475e-06  9.753e-13  4.047e+06      0.000 [3.948e-06,3.948e-06]\nalpha[1]       0.1000  3.135e-03     31.894 3.255e-223   [9.385e-02,  0.106]\nbeta[1]        0.8800  3.953e-03    222.603      0.000     [  0.872,  0.888]\n============================================================================\n\nCovariance estimator: robust\n\n\nLes résultats du modèle GARCH(1,1) se trouvent dans le tableau ci-dessus. Nous pouvons constater que les paramètres sont tous significatifs. Nous allons maintenant vérifier la qualité d’ajustement du modèle GARCH(1,1) aux résidus.\n\nAnalyse des résidus du modèle GARCH\nLa figure ci-dessous représente les différents graphiques : - Le graphique des résidus du modèle GARCH - La volatilité conditionnelle estimée - Le graphique des résidus standardisés\nLe résidu dont l’analyse de la blancheur nous intéresse est le résidu standardisé. Il est le résultat du rapport entre le résidu du modèle GARCH et la volatilité conditionnelle estimée. Nous allons donc vérifier si ce résidu est un bruit blanc. C’est ce résidu que nous allons utiliser pour modéliser la VaR.\n\nstd_residus = garch_final_model.resid / garch_final_model.conditional_volatility\nfig, axes = plt.subplots(3, 1, figsize=(15, 6))\n\n\naxes[0].plot(garch_final_model.resid)\naxes[0].set_title(\"Résidus\")\naxes[1].plot(garch_final_model.conditional_volatility)\naxes[1].set_title(\"Volatilité conditionnelle\")\naxes[2].plot(std_residus)\naxes[2].set_title(\"Résidus standardisés\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nVérifions que les résidus de ce modèle GARCH sont des bruits blancs.\nNous allons faire le test de Ljung-Box pour vérifier l’autocorrélation des résidus. Et nous allons également faire ce test sur les carrés des résidus pour vérifier l’absence d’autocorrélation des résidus au carré d’où l’absence d’hetéroscedasticité.\n\n# test de Ljung-Box sur les résidus et les résidus au carré\n\ntest_lyungbow = acorr_ljungbox(std_residus, lags=[10], return_df=False)\nprint(test_lyungbow)\ntest_lyungbow_squared = acorr_ljungbox(std_residus**2, lags=[10], return_df=False)\nprint(test_lyungbow_squared)\n\n      lb_stat  lb_pvalue\n10  16.281508   0.091852\n     lb_stat  lb_pvalue\n10  9.337446   0.500406\n\n\nDans les deux cas, la p-value est supèrieur à 5%. Nous ne pouvons donc pas rejeter l’hypothèse nulle selon laquelle les résidus sont des bruits blancs. Nous pouvons donc conclure que le modèle GARCH(1,1) est satisfaisant. Nous pouvons faire d’autres tests pour vérifier la qualité de notre modèle tel que le test de Jarque-Bera et le QQ plot.\nLes estimations des paramètres du modèle AR(1)-GARCH(1,1) sont significatifs. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements. Les paramètres estimés sont : - alpha = 0.0858 - beta = 0.8978 - omega = 0.0322 - phi = -0.0092",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "title": "Modélisation de la value at risk",
    "section": "Estimation des paramètres du modèle AR(1)-GARCH(1,1).",
    "text": "Estimation des paramètres du modèle AR(1)-GARCH(1,1).\nNous pouvons directement estimer les paramètres du modèle AR(1)-GARCH(1,1) en utilisant la fonction arch_model de la bibliothèque arch. C’est ce que nous allons faire dans la cellule suivante.\n\n## estimation des paramètres du modèle AR(1) -GARCH(1,1)\n\nar_garch_model = arch_model(train_close['log_return'], vol='Garch', p=1, q=1, mean='AR', lags=1).fit(disp='off')\nprint(ar_garch_model.summary())\n\n                           AR - GARCH Model Results                           \n==============================================================================\nDep. Variable:             log_return   R-squared:                      -0.000\nMean Model:                        AR   Adj. R-squared:                 -0.001\nVol Model:                      GARCH   Log-Likelihood:                19340.9\nDistribution:                  Normal   AIC:                          -38671.8\nMethod:            Maximum Likelihood   BIC:                          -38637.9\n                                        No. Observations:                 6500\nDate:                Wed, Apr 24 2024   Df Residuals:                     6498\nTime:                        01:54:07   Df Model:                            2\n                                    Mean Model                                   \n=================================================================================\n                     coef    std err          t      P&gt;|t|       95.0% Conf. Int.\n---------------------------------------------------------------------------------\nConst          4.8457e-04  1.423e-04      3.404  6.636e-04  [2.056e-04,7.636e-04]\nlog_return[1] -9.9994e-03  1.272e-02     -0.786      0.432 [-3.494e-02,1.494e-02]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9495e-06  7.237e-12  5.457e+05      0.000 [3.949e-06,3.950e-06]\nalpha[1]       0.1000  6.899e-03     14.495  1.305e-47   [8.648e-02,  0.114]\nbeta[1]        0.8800  6.474e-03    135.923      0.000     [  0.867,  0.893]\n============================================================================\n\nCovariance estimator: robust\n\n\n\n# les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants\n\nprint(f\"Les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\\n{ar_garch_model.params}\")\nconstance,phi, omega, alpha, beta = ar_garch_model.params\n\nLes paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\nConst            0.000485\nlog_return[1]   -0.009999\nomega            0.000004\nalpha[1]         0.100000\nbeta[1]          0.880000\nName: params, dtype: float64\n\n\nLes résultats ci-dessus donnent les mêmes résultats que ceux obtenus précédemment. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "title": "Modélisation de la value at risk",
    "section": "Backtesting",
    "text": "Backtesting\n\nlist_exceptions_TVE_residus = [i for i in range(train_size,len(ts_close['log_return'])) if ts_close['log_return'][i]&lt;var_dyn_TVE_residus[i]]\nlen(list_exceptions_TVE_residus)\n\n25\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)/test_size*100.:0.2f}%\")\n\nLe nombre d'exceptions pour la VaR TVE est: 25\nLe pourcentage d'exceptions pour la VaR TVE est: 1.15%\n\n\n\ntest_except_TVE_residus = stats.binomtest(len(list_exceptions_TVE_residus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_TVE_residus.pvalue:.2f}')\n\nla p-value du test binomial est: 0.45\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR dynamique est satisfaisante.\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index, ts_close['log_return'], label=\"historical log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], var_dyn_TVE_residus[train_size:], label='VaR dynamique', color = 'red')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.scatter(ts_close.index[list_exceptions_TVE_residus], ts_close['log_return'][list_exceptions_TVE_residus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Statistiques des risques extrêmes"
    ]
  }
]