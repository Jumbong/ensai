[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "",
    "text": "# L’avènement de l’intelligence artificielle (IA) : un sujet controversé à l’échelle mondiale\nLes débats autour de l’intelligence artificielle (IA) sont partagés. Alors que certains craignent que l’IA ne supprime de nombreux emplois et ait un impact négatif sur l’environnement, l’importance de parvenir à un consensus sur la manière de manipuler cet outil qui évolue à une vitesse exponentielle est cruciale. C’est dans cet esprit que le Royaume-Uni a organisé le tout premier sommet mondial sur l’IA."
  },
  {
    "objectID": "posts/welcome/index.html#déroulement-du-sommet",
    "href": "posts/welcome/index.html#déroulement-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Déroulement du sommet",
    "text": "Déroulement du sommet\nCe sommet, qui s’est tenu sur deux jours (1er et 2 novembre), a réuni des sommités en IA, des entrepreneurs, des chercheurs et des représentants politiques de haut niveau. Parmi eux se trouvaient Antonio Guterres, Secrétaire général de l’ONU; Kamala Harris, vice-présidente des États-Unis; Giorgia Meloni, Première ministre italienne et seule dirigeante du G7 présente; et Elon Musk, CEO de X (ex Twitter)."
  },
  {
    "objectID": "posts/welcome/index.html#résultats-du-sommet",
    "href": "posts/welcome/index.html#résultats-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Résultats du sommet",
    "text": "Résultats du sommet\n\nUn accord sur la sécurité des systèmes d’IA a été signé par les grandes puissances mondiales telles que la Chine, les États-Unis et l’Union européenne, soulignant une responsabilité partagée dans la régulation de l’IA.\nUn premier rapport sur l’IA a été confié à Yoshua Bengio, scientifique canadien récompensé par le prix Turing, chargé d’évaluer les risques et avantages de l’IA.\nAntonio Guterres a plaidé pour une approche universelle : il a exhorté à une gestion de l’IA collective et éthique, alignée avec les principes fondamentaux et les droits de l’homme énoncés dans la charte des Nations Unies.\nElon Musk, lors d’un échange avec Rishi Sunak, a mis en lumière les risques de l’IA, même s’il envisage une “ère d’abondance”, alertant sur le fait que les robots humanoïdes pourraient un jour dépasser les capacités humaines.\n\nL’IA peut certes faciliter le travail et accélérer la résolution des problèmes, mais elle soulève également des inquiétudes. Si un robot peut penser et agir à la place d’une équipe de quatre personnes, que deviendront ces travailleurs ? C’est l’une des questions cruciales que soulève l’essor de l’IA."
  },
  {
    "objectID": "posts/welcome/index.html#prochaines-étapes",
    "href": "posts/welcome/index.html#prochaines-étapes",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Prochaines étapes",
    "text": "Prochaines étapes\nLe prochain sommet se déroulera à Paris, se concentrant sur des problèmes immédiats tels que la désinformation et l’impact sur l’emploi, avec une date encore à déterminer."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html",
    "href": "posts/MarketRiskGen/index.html",
    "title": "Market risk generalities",
    "section": "",
    "text": "A market risk is a risk of loss an investment value due to a variation of the market factors. The market factors are the parameters that influence the price of a financial instrument. The most common market factors are:\n\nInterest rates\nEquity prices\nForeign exchange rates\nCommodity prices\n\nThe interest rate risk is the risk that the value of a financial instrument(ex:bond) will decline due to an increase of the interest rates.\nC’est donc une offre dont la valeur va baisser si les taux augmentent. C’est le cas des obligations.\nExemple : A zero coupon bond with a face value of 1000€ and a maturity of 10 years. The interest rate is 5%. The value of the bond is p = \\frac{1000}{(1+0.05)^{10}} = 613.\nIf the interest rate increases to 6%, the value of the bond is p = \\frac{1000}{(1+0.06)^{10}} = 558\nUne autre chose qu’il faut remarquer est que si les taux d’intérêts augmentent, la valeur des obligations diminue. Qu’elle est l’interprétation économique de cela ?\n\n\nSi les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue.\n\n\n\nUne banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "href": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "title": "Market risk generalities",
    "section": "",
    "text": "Si les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#curve-risk",
    "href": "posts/MarketRiskGen/index.html#curve-risk",
    "title": "Market risk generalities",
    "section": "",
    "text": "Une banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "href": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "title": "Market risk generalities",
    "section": "Sensitivity-based methods",
    "text": "Sensitivity-based methods\n\nDuration\nLa duration exprime comment un porte-feuille est sensible aux variations des taux d’intérêts.\nExemple: Un porte-feuille avec une duration de 5 ans signifie que si les taux d’intérêts augmentent de 1%, la valeur du porte-feuille va diminuer de 5%.\nDonc une duration élevée signifie un risque élevé.\nMaintenant si on a deux obligations(bonds): Le premier de nominal 1000, maturité 10 ans, de prix 900 et de coupon 5%. Le deuxième de nominal 1000, maturité 10 ans, de prix 1100 et de coupon 7%.\nSur quels bond va t-on investir ?\nPour répondre à cette question, nous avons besoin d’un outil: the yield to maturity.\nThe yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre. Mathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\noù C_i = nominal* coupon i!=N C_N = nominal + coupon*nominal\nIl faut deux hypothèses pour calculer le yield to maturity: - Le coupon est réinvesti au taux d’intérêt du marché. - Le titre est détenu jusqu’à la maturité. On a défini le yield to maturity, on peut définir la duration.\nLa duration est la sensibilité de la valeur d’un titre par rapport au yield to maturity. Mathématiquement,\nD = -\\frac{1}{P}\\frac{dP}{dy}.\nConséquence de la duration:\n\nSi le taux d’intérêt augmente diminue, nous allons investir dans un porte-feuille avec une duration élevée.\nSoit une obligation dont le prix est de 90 qui paye des coupons annnuels de 5% de nominal 1000 et de maturité 5 ans.\n\nDeterminons son yield to maturity.\nOn sait que le yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre.\nMathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\nPour déterminer la duration, on a besoin de calculer la dérivée de P par rapport à y.\non sait que la dérivée peut être calculée comme une limite de la formule suivante:\n\\frac{dP}{dy} = \\lim_{\\Delta y \\to 0} \\frac{P(y+\\Delta y) - P(y- \\Delta y)}{2 \\Delta y}\nAinsi si on a déterminé le yield to maturity, on peut déterminer la duration. Il faut pour cela, calculer le prix de l’obligation pour deux valeurs de y: y+\\Delta et y-\\Delta y. Prenons \\Delta = 0.01 Pour y1 = y+\\Delta P(y+\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y+\\Delta)^i}\npour y2 = y-\\Delta P(y-\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y-\\Delta)^i}\nFinalement, D = -\\frac{1}{P}\\frac{P(y+\\Delta) - P(y- \\Delta y)}{2 \\Delta y}\nIl y’a une hypothèse forte derrière la duration: Une courbe de déplacement parallèle. C’est à dire que si les taux d’intérêts augmentent de 1%, la courbe des taux d’intérêts va se déplacer parallèlement vers le haut."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#the-greeks",
    "href": "posts/MarketRiskGen/index.html#the-greeks",
    "title": "Market risk generalities",
    "section": "The Greeks",
    "text": "The Greeks\nLa valeur d’un porte-feuille dépend de plusieurs facteurs, qui peuvent interagir entre eux. Pour mesurer l’impact de ces facteurs sur la valeur du porte-feuille, on utilise les lettres grecques : delta, gamma, vega, theta, rho.\n\nDelta\nLe delta exprime le changement entre un changement de la valeur du porte-feuille et un changement de la valeur du sous-jacent. Exemple : Soit C_0 le prix d’une option de maturité T et de strike k. Soit S_t le prix du sous-jacent à la date t.\npayoff de l’option : max(S_T - k, 0), le prix de l’option est le payoff actualisé : C_0 = \\frac{max(S_T - k, 0)}{(1+r)^T}\nLes facteurs de risques de cette option sont : le prix du sous-jacent, le taux d’intérêt et la volatilité.\nLe delta de cette option est : \\frac{\\partial C_0}{\\partial S_t}\nComment se couvrir de la variation du prix du sous-jacent ?\nIl faut construire un porte-feuille qui a un delta égal à 0. C’est à dire que si le prix du sous-jacent augmente, la valeur du porte-feuille ne change pas.\non peut par exemple considéré un porte-feuille de prix P^{'} = C_0 + \\alpha S_t. La nouvelle option est de sensibilité nulle par rapport au prix du sous-jacent si \\alpha = -\\frac{\\partial C_0}{\\partial S_t}. Ceci revient à dire que pour se couvrir du risque de la variation de prix du sous-jacent, il faut vendre à decouvert c’est-à-dire short selling \\frac{\\partial C_0}{\\partial S_t} unités du sous-jacent. Le delta d’une action est 1.\nUn portefeuille avec un delta de 0 est option risque neutre. Il est construit pour (hedging purposes) se couvrir du risque de la variation du prix du sous-jacent.\n\n\nGamma\nLe gamma est la dérivée seconde du prix de l’option par rapport au prix du sous-jacent. Il mesure la sensibilité du delta par rapport au prix du sous-jacent. Son implication dans le cas d’un portefeuille delta-neutre est qu’il a besoin d’être réajusté régulièrement si son gamma est élevé. Le gamma est donc de mesurer le degré d’exposition au risque qu’une position couverte(hedged position) dévellopera si la couverture(hedge) n’est pas réajustée.\n\n\nVega\nLe vega mesure le changement du prix de l’option par rapport à la volatilité du sous-jacent. Il mesure la sensibilité du prix de l’option par rapport à la volatilité du sous-jacent.\nUn portefeuille avec un vega de 0 est vega-neutre. Il est construit pour se couvrir du risque de la variation de la volatilité du sous-jacent. C’est-à-dire qu’il est insensitive à la variation de la volatilité du sous-jacent.\n\n\nTheta and Rho\nLe Theta et le rho déterminent respectivement le taux de changement de la valeur d’un portefeuille par rapport au temps to maturity et par rapport au taux d’intérêt.\nEn practive, les sensibilités d’un portefeuille sont calculées intensivement, intra-day(à l’intérieur de la journée) et quotidiennement pour chaque traded product. Ils sont utilisées par les traders afin de gérer leur risque, their position(hedging) et par les managers pour expliquer leur perte et leur profit(P&L). Cependant, elles souffrent de plusieurs limitations: elles ne peuvent pas être comparées entre plusieurs activités pour conclure qu’une activité est plus risquée qu’une autre."
  },
  {
    "objectID": "posts/machineLearning3A/index.html",
    "href": "posts/machineLearning3A/index.html",
    "title": "Examen de machine learning",
    "section": "",
    "text": "Je ne sais pas si c’est Da Veiga qui vous donnera cours, mais je vais vous donner un petit aperçu de ce que vous pourriez avoir comme examen. Je pense que c’est important parce que nous prenons souvent à la légère ce genre d’activité surtout qu’il permet l’utilisation des générateurs de texte(chatgpt). Cette année, peu d’étudiant ont terminé le projet, parce que chargé les données était long et fastidieux. Je vous conseille de vous y prendre à l’avance. Créer des fonctions qui font certaines tâches, je vous expliquerai progressivement."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#box-plot",
    "href": "posts/machineLearning3A/index.html#box-plot",
    "title": "Examen de machine learning",
    "section": "Box plot",
    "text": "Box plot\nLe graphique ci-dessous montre la distribution de chaque variable en fonction de la variable cible.\n\n# box plot\n# Transformed Cover_Type to categorical\nY_sampled = Y_sampled.astype('category')\n\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterprétation: Nous allons nous concentrer sur la variable Elevation.\nOn peut voir que la variable Elevation est très discriminante.\nLes types de couverture 1, 2, et 7 montrent des médianes relativement élevées pour l’élévation, avec 7 ayant la médiane la plus élevée, suivie par 1 et 2."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html",
    "href": "posts/InstrumentsFinanciers/index.html",
    "title": "Les intruments financiers à un bébé",
    "section": "",
    "text": "Les instruments financiers\nDans le cadre d’un cours d’asset pricing à l’ENSAI, je suis totalement perdu dans la compréhension des instruments financiers.\nCe décrochage m’empêche de suivre le cours et je suis donc totalement perdu sur les autres notions telles que pricing, hedging, etc.\nJ’ai donc décidé de retourner à la base c’est-à-dire de comprendre les instruments financiers.\nDans mes recherches sur chatgpt, j’ai trouvé les différents instruments financiers et leurs définitions."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "href": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les obligations.",
    "text": "Les obligations.\nJ’ai été vraiment déçu de la définition et de sa complexité. Une obligation est définie comme un titre de dette émis par une entreprise, un gouvernement, etc. Les détenteurs de ces obligations reçoivent des intérêts à intervalles réguliers et remboursent le montant total à l’échéance.\nQu’est-ce que cela signifie concrètement ?\nPrenons un exemple. J’ai faim, j’ai besoin d’argent pour manger à l’école mais je n’ai pas d’argent. Je vais donc voir un ami pour lui expliquer ma situation. Il me prête de l’argent de l’argent avec des intérêts sur une durée de cinq jours et il me demande de lui verser une partie de cette argent(qui correspond au produit entre le taux d’intérêt et la somme dont je lui est empruntée) chaque jour et de lui rembourser le montant total à la fin des cinq jours. Pour être sûr que je vais lui rembourser, il me demande un papier signé par moi et par lui. Ce papier est une obligation.\nIl existe deux types d’obligations: - Les obligations 0 coupon: Ce sont des prêts que l’on fait généralement au lycée. C’est-à-dire que l’on part voir un ami pour lui demander de l’argent avec un intérêt de x% pour cinq jours. On lui remboursera à la fin des cinq jours la somme empruntée plus les intérêts.\n\nLes obligations couponnées: Ce sont des obligations qui versent des intérêts à intervalles réguliers. C’est-à-dire que chaque mois nous verserons une partie de la somme empruntée(la somme empruntée multipliée par le taux d’intérêt) et à la fin de la période, nous rembourserons la somme empruntée plus les intérêts."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "href": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les actions.",
    "text": "Les actions.\nEn anglais les actions sont appelées les equities. Ce sont des titres de propriété d’une entreprise."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "href": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les produits dérivés.",
    "text": "Les produits dérivés.\nCe sont les intruments financiers qui pose actuellement beaucoup de difficultés.\nCe sont des intruments financiers dont la valeur dépend de la valeur d’un autre actif qui est appelé sous-jacent. On compte parmi les produits dérivés les options, les futures, les swaps, etc.\n\nLes options.\nUne option est un contrat qui donne à son détenteur le droit mais pas l’obligation d’acheter ou de vendre un actif à un prix fixé à l’avance et à une date donnée.\nIl y’a deux choses à retenir dans cette définition: le prix fixé à l’avance et la date donnée.\nOn signe ce type de contrat lorsque pense que le prix de l’actif va augmenter.\nPar exemple je pense que le prix de l’Iphone va augmenter dans les prochains jours. Je vois un ami qui vend des Iphones. Je lui donne un montant afin qu’il me reserve cet Iphone à un prix k fixé aujourd’hui. Si le prix de l’Iphone augmente, je pourrais l’acheter à un prix inférieur à celui du marché. Si le prix de l’Iphone diminue, je ne l’achèterai pas et je perdrai le montant que j’ai donné à mon ami.\n\n\nLes futures."
  },
  {
    "objectID": "posts/entretienCreditAgricole/index.html",
    "href": "posts/entretienCreditAgricole/index.html",
    "title": "Compte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.",
    "section": "",
    "text": "Sommet mondial IA\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\nListe des personnes présentes :\n\n\n\n\nIntervenants\n\n\nPrénom\n\n\nNom\n\n\n\n\nRecruteur\n\n\nprenom\n\n\nnom\n\n\n\n\nRecruteur\n\n\nMarie\n\n\nnom\n\n\n\n\nCandidat\n\n\nJunior\n\n\nJUMBONG\n\n\n\n\nDéroulement de l’Entretien :\nL’entretien du jeudi, 30 novembre pour le poste de data scientist - Auditeur a duré 1 heure et a eu pour objectif d’évaluer les compétences du candidat et la compréhension du poste de stage.\n\nParcours de l’Étudiant :\n\nLa discussion a débuté par un tour de table permettant à chacun de se présenter. Le candidat a notamment présenté son parcours, et mis en avant quelques compétences acquises durant ses différents projets universitaires.\n\nPrésentation de Crédit Agricole CACIB :\n\nCrédit Agricole CACIB est la banque de financement et d’investissement du groupe Crédit Agricole. Elle offre une gamme étendue de produits et services en banque d’investissement, financements structurés, banque commerciale, et marchés financiers.\n\nMissions du Stage :\n\nÉquipe Méthode Support - Data Scientist :\nUne première mission dans l’équipe méthode support où le candidat, s’il est retenu, travaillera sur la modélisation des risques de la banque. Il devra utiliser les compétences théoriques et pratiques acquises lors des formations précédentes, notamment en clustering et modélisation statistique.\nMission d’Inspection - Encadrement :\nLa deuxième mission en tant qu’inspecteur consistera à utiliser des techniques basées sur la donnée, l’analyse quantitative, et à proposer de nouvelles techniques afin de contribuer à la réussite de la mission. Les méthodologies utilisées, telles que la collecte de la donnée ,son exploration, sa préparation, son analyse ainsi que la diffusion des résultats, ont été précisément présentées.\n\n\nConclusion :\nLe processus de recrutement est actuellement en cours. Le candidat recevra une réponse finale d’ici quelques semaines, une fois que tous les entretiens seront terminés avec les autres candidats.\nIl lui a été demandé d’informer s’il est engagé dans d’autres processus de recrutement parallèle. Sa transparence est importante afin de permettre aux recruteurs d’adapter leurs réponses dans le cas échéant."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "menu",
    "section": "",
    "text": "We can notice that the data is labelised so the model that can be applied in the data is classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nLes intruments financiers à un bébé\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nExamen de machine learning\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nTBATS : N’a pas de contraintes de saisonnalité\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nUn entretien raté\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nLeadership : Pouvoir, autorité et légitimité\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nMarket risk generalities\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n  \n\n\n\n\nIA : Eléments clés du premier sommet mondial\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ClimateScenario/index.html",
    "href": "posts/ClimateScenario/index.html",
    "title": "Analyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.",
    "section": "",
    "text": "Risques physiques et climatiques\n\n\n\nObjectif :\nL’objectif est d’intégrer les données climatiques dans le cadre d’allocation d’actifs.\nIl s’agit plus simplement de comprendre comment intégrer les scénarions climatiques et les risques climatiques dans les projections financières futures.\nComprendre comment ceux-ci impactent les performances financières d’investissement.\nCet article de Luca Bongiorno et al. met à lumière les impacts potentiels du changement climatique sur les marchés financiers, en se concentrant sur leurs effets à long terme.\nElle utilise un outil de modélisation top-down dévéloppé par Ortec finance qui intégre la science climatique aux effets macroéconomiques et financiers pour examiner les impacts possibles de trois scénarios plausible et non extrèmes.\nL’article analyse l’impact sur le PIB, il en ressort une réduction de PIB dans les trois scénarios, avec un impact très sévère dans le cadre d’une transition ratée où les objectifs climatiques de l’accord de Paris ne sont pas atteints.\nL’impact sur les marchés financiers est également analysé. Il en ressort une baisse des rendements cumulés des actions mondiales\nGIEC : groupe d’experts intergouvernemental sur l’évolution du climat.\nCes risques climatiques commencent à se manifester à travers les changements de conditions climatiques(variations de température, régimes pluviométriques, etc.), l’élévation du niveau de la mer, les conditions météorologiques extrêmes, les incendies de forêt, les sécheresses, les inondations, les ouragans, les cyclones, les tempêtes de neige, les vagues de chaleur, les vagues de froid, etc. Ces événements ont des impacts négatifs significatifs et pertubent divers secteurs notamment l’agriculture, la sylviculture, l’énergie, les infrastructures.\nLes actions menant à limiter l’ampleur du changement climatique en reduisant les émissions de gaz à effet de serre, principalement en diminuant l’utilisation de l’énergie fossile, en utilisant l’énergie renouvelable.\nLe changement climatique est un risque systémique car il affecte tous les secteurs de l’économie de la même manière et en même temps.\nLes changements climatiques posent des risques pour l’économie et le système financier.\nLes risques climatiques sont classés en deux catégories : les risques physiques et les risques de transition.\nles risques physiques découlent des changements des conditions climatiques et des événements météorologiques extrêmes. Ils peuvent être aigus ou chroniques.\nles risques de transition résultent du passage à une économie à faible émission de carbone, à l’utilisation d’une technologie propre et à l’adoption de pratiques commerciales durables. Ils peuvent être liés à la politique, à la technologie, à la marché et aux risques légaux.\nBonjour, merci de me donner l’opportunité de m’exprimer aujourd’hui. Je m’appelle Junior Jumbong. Mon intérêt"
  },
  {
    "objectID": "posts/forumEnsai/index.html",
    "href": "posts/forumEnsai/index.html",
    "title": "Un entretien raté",
    "section": "",
    "text": "entretien"
  },
  {
    "objectID": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "href": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "title": "Un entretien raté",
    "section": "Comment raté son entretien de stage ?",
    "text": "Comment raté son entretien de stage ?\nAujourd’hui, a eu lieu le forum des ingénieurs de l’ENSAI où tout étudiant rêve de décrocher un stage. C’est l’occasion pour les étudiants de discuter avec les entreprises de leur parler d’eux.\nIl y’a une première étape, où les étudiants doivent s’inscrire sur une platform pour facililer la prise de rendez-vous. Ensuite, les étudiants doivent se présenter à l’heure à l’entretien. Ce n’est pas grave si un étudiant n’arrive pas à décrocher un entretien, il peut toujours se présenter à l’entreprise pour discuter avec les recruteurs.\nJ’ai pu obtenir quelques entretiens\nJe me suis lévé aujourd’hui à 9 heures, j’ai pris mon petit déjeuné, je me suis habillé, direction l’ENSAI. Enfin d’être présentable, j’ai porté une chémise.\nQuand je suis arrivé, j’ai localisé les différent"
  },
  {
    "objectID": "posts/Leadership/index.html",
    "href": "posts/Leadership/index.html",
    "title": "Leadership : Pouvoir, autorité et légitimité",
    "section": "",
    "text": "Leadership : Pouvoir, autorité et légitimité\n\n\n\nLe Pouvoir se reçoit, l’autorité se construit.\nComme l’éminent penseur Christian Monjou, débutons avec une citation marquante. Niccolò Machiavel, une figure clé dans l’étude du leadership et de la stratégie politique, affirmait : ‘Les qualités nécessaires pour conserver le pouvoir ne sont pas les mêmes que celles pour l’acquérir.’ Cette citation soulève un point crucial dans le domaine de la gestion du leadership et de l’autorité en entreprise. Lorsqu’on crée une entreprise, devenir un patron ou un chef d’entreprise confère du pouvoir, mais pas nécessairement de l’autorité. L’autorité, dans le contexte de la leadership efficace, est une reconnaissance octroyée par autrui ; elle repose sur la légitimité perçue par les collaborateurs. Un leader en cours de construction de son autorité n’est pas automatiquement reconnu comme légitime. En effet, la légitimité en leadership et en gestion d’équipe se manifeste à travers le respect et la confiance que l’on inspire à son entourage. Souvent, ceux qui doivent affirmer leur propre légitimité en sont en réalité dépourvus. La construction de l’autorité est donc un élément essentiel à l’efficacité du leadership dans le monde des affaires et la gestion d’équipe.\n\n\nLa construction de l’autorité\nL’essence du leadership transformationnel réside dans la capacité à favoriser la croissance et l’épanouissement de la communauté à laquelle on appartient. Cela implique d’encourager et de soutenir le développement personnel et professionnel des membres de l’équipe. Le concept de développement personnel est crucial dans le cadre du leadership efficace. En effet, un leader qui cesse de s’investir dans son propre développement personnel risque de perdre la légitimité aux yeux de ses collaborateurs. Dans la construction de l’autorité, la notion de style personnel joue également un rôle clé. Un style de leadership rigide ou répétitif peut s’avérer contre-productif. Il est important de rester adaptable et innovant, quelle que soit la situation. Cela soulève une question fondamentale : le leadership est-il inné ou acquis ? La réponse réside souvent dans la capacité d’un individu à évoluer et à s’adapter continuellement, renforçant ainsi son autorité et son influence au sein de son organisation.\n\n\nL’origine du leadership\nLa question de savoir si le leadership est inné ou acquis est un débat central dans le domaine du développement du leadership. Bien qu’il puisse sembler que certaines personnes naissent leaders, cette idée comporte des risques. En effet, se reposer uniquement sur des compétences innées en leadership peut conduire à une stagnation, limitant l’innovation et l’adaptabilité. Comme le soulignait le théoricien du leadership, Berson, ‘Plaquer du mécanique sur du vivant, c’est risquer le ridicule.’ Un leader efficace ne se contente pas de compter sur des compétences innées, mais s’interroge continuellement sur l’adéquation de son style de leadership avec les besoins actuels de sa communauté ou de son organisation. Un exemple éloquent est celui du leadership féminin, où l’adaptabilité et l’empathie jouent souvent un rôle clé dans la réussite et l’impact du leadership.\n\n\nElisabeth I er\nLa figure que vous voyez est celle d’Elisabeth Ière elisabeth., reine d’Angleterre ayant régné durant 45 ans (de 1558 à 1603). Fille de Henri VIII et Anne Boleyn, elle fut la dernière souveraine de la dynastie des Tudors et la première femme à régner sur l’Angleterre et l’Irlande. Son ascension au trône, perçue initialement comme un coup du hasard, est un exemple remarquable de la manière dont un leader peut transformer une opportunité imprévue en un règne influent et significatif. Sa présence imposante, souvent décrite comme une ‘hypertrophie de signe’, reflétait une stratégie de communication visant à établir sa légitimité et son autorité. En effet, Elisabeth Ière comprenait l’importance de l’image et du symbolisme dans la construction du leadership. Elle incarnait la dualité du ‘signe et du sens’ – un aspect essentiel dans la représentation du leadership. Son règne illustre parfaitement la nécessité pour un leader de maintenir un équilibre entre son identité privée et sa persona publique. Finalement, l’efficacité de son leadership résidait dans sa capacité à maîtriser l’art de la communication – un aspect crucial pour tout leader aspirant à un impact durable.\n\n\nLa qualité de la parole\nDans le contexte actuel de l’éducation et du leadership, une transformation notable s’est opérée par rapport aux méthodes traditionnelles. Autrefois, l’entrée d’un enseignant dans une salle de classe était synonyme de respect absolu et d’une reconnaissance incontestée de son savoir. Cependant, dans le monde moderne, marqué par l’accès instantané à l’information via la technologie numérique, cette dynamique a évolué. Imaginez un professeur mentionnant un pourcentage spécifique, comme 2.23 %, devant des étudiants équipés de smartphones. Il est probable qu’ils vérifient immédiatement cette information, remettant en question l’exactitude des données présentées. Cette situation illustre un changement fondamental : la légitimité dans le domaine de l’éducation et du leadership ne repose plus exclusivement sur la transmission du savoir, mais de plus en plus sur la qualité de la communication et de l’engagement. L’ère numérique, en économisant du temps, nous incite non pas à intensifier notre présence numérique, mais plutôt à valoriser davantage le temps consacré à la communication directe et significative. Selon Marcel Mauss, ce principe d’échange symbolique, où la qualité de la communication exige en retour un engagement – ici sous forme de loyauté – est essentiel. Enfin, la vraie mesure d’un leader réside dans sa capacité à anticiper les idées potentiellement nuisibles à sa communauté et à persuader celle-ci d’adopter de nouvelles perspectives pour éviter les pièges futurs. Mener une communauté du connu vers l’inconnu est une des responsabilités les plus ardues d’un leader.\n\n\nUn monde de changement\nNous vivons dans une ère de transformation rapide, caractérisée par un marché mondial en constante évolution. À l’ère numérique et face aux défis de la transition écologique, les communautés et organisations qui ne reconnaissent pas ce changement de paradigme risquent l’obsolescence. Cette transformation s’illustre particulièrement avec l’avènement de l’intelligence artificielle, comme en témoigne le développement de technologies telles que ChatGPT. De plus, les appels urgents du GIEC et des communautés scientifiques, notamment des astrophysiciens, mettent en lumière l’imminence d’une crise écologique mondiale. Il est impératif pour les entreprises, et plus spécifiquement pour les banques, d’évaluer et de gérer ces risques. L’enjeu n’est pas simplement financier, mais il s’agit de la préservation de la vie sur Terre dans toutes ses formes. La responsabilité sociale des entreprises et la durabilité environnementale sont devenues des facteurs clés dans la stratégie d’entreprise moderne, dictant une nouvelle approche vers un avenir plus durable et conscient."
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html",
    "href": "posts/machineLearning3A/Jumbong_Junior.html",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "",
    "text": "import pandas as pd\nimport chardet\nimport warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\n\n\nwith open('Dataset1.csv', 'rb') as f:\n    result = chardet.detect(f.read())  # or readline if the file is large\n#print(result['encoding'])\n\nDataset1 = pd.read_csv('Dataset1.csv', delimiter=\",\",decimal = \".\",encoding=result['encoding'])\nDataset1.head()\n\n\n\n\n\n\n\n\nVegetation_Type_2\nGroundwater_Level_1\nDrainage_Quality_1\nSlope\nHillshade_9am\nHillshade_Noon\nPollution_Level_1\nWater_Source_Distance_2\nTerrain_Roughness_2\nUrban_Proximity_Index_1\n...\nSoil_Moisture_Level_2\nHorizontal_Distance_To_Hydrology\nWind_Speed_Average_2\nElevation_Range_1\nPollution_Level_2\nVegetation_Type_1\nTemperature_Average_1\nCanopy_Cover_2\nElevation\nCover_Type\n\n\n\n\n0\n450.684968\n2054.426315\n229.994980\n3\n221\n232\n181.000000\n227.161401\n216.000000\n14.0\n...\n418.375833\n258\n116.000000\n219.000000\n224.000000\n0.000000\n9.000000\n503.586301\n2596\n5\n\n\n1\n889.885392\n997.573983\n208.000000\n2\n220\n235\n166.000000\n933.851870\n251.000000\n16.0\n...\n2488.726342\n212\n179.025367\n236.000000\n224.000000\n69.000000\n17.000000\n537.000000\n2590\n5\n\n\n2\n1232.738700\n676.352745\n214.313227\n9\n234\n238\n205.000000\n4235.282432\n176.898683\n13.0\n...\n1852.423116\n268\n156.000000\n200.872578\n230.415218\n120.357544\n10.000000\n376.000000\n2804\n2\n\n\n3\n3359.512595\n4720.481538\n230.000000\n18\n238\n238\n176.002696\n1066.935784\n220.000000\n11.0\n...\n5388.528248\n242\n156.000000\n230.000000\n243.000000\n15.000000\n12.933234\n30.000000\n2785\n2\n\n\n4\n1907.275049\n2187.627994\n221.000000\n2\n220\n234\n109.000000\n1182.489702\n184.859965\n9.0\n...\n304.080537\n153\n112.000000\n232.000000\n213.000000\n39.000000\n14.000000\n330.000000\n2595\n5\n\n\n\n\n5 rows × 51 columns\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\n\nX = Dataset1.drop(['Cover_Type'], axis=1).copy()\nY = Dataset1['Cover_Type'].copy()\nY.value_counts()\n\nCover_Type\n2    283301\n1    211840\n3     35754\n7     20510\n6     17367\n5      9493\n4      2747\nName: count, dtype: int64\nfrom sklearn.feature_selection import mutual_info_classif\n\nfrom sklearn.feature_selection import mutual_info_classif\nimport pandas as pd\n\n\n\n\n\n\nmutual_info = mutual_info_classif(X, Y)\n\n\nmutual_info_df = pd.DataFrame(mutual_info, index=X.columns, columns=['Mutual Information'])\n\n# Filtrer les caractéristiques avec une information mutuelle supérieure à 0.01\nrelevant_features = mutual_info_df[mutual_info_df['Mutual Information'] &gt; 0.01].index\n\n# Afficher les caractéristiques pertinentes\nprint(\"Caractéristiques pertinentes (Information Mutuelle &gt; 0.01) :\")\nprint(relevant_features)\n\nCaractéristiques pertinentes (Information Mutuelle &gt; 0.01) :\nIndex(['Slope', 'Hillshade_9am', 'Hillshade_Noon',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_3pm',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points',\n       'Aspect', 'Horizontal_Distance_To_Hydrology', 'Elevation'],\n      dtype='object')\nX = X[relevant_features]\nX.head()\n\n\n\n\n\n\n\n\nSlope\nHillshade_9am\nHillshade_Noon\nHorizontal_Distance_To_Roadways\nHillshade_3pm\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Fire_Points\nAspect\nHorizontal_Distance_To_Hydrology\nElevation\n\n\n\n\n0\n3\n221\n232\n510\n148\n0\n6279\n51\n258\n2596\n\n\n1\n2\n220\n235\n390\n151\n-6\n6225\n56\n212\n2590\n\n\n2\n9\n234\n238\n3180\n135\n65\n6121\n139\n268\n2804\n\n\n3\n18\n238\n238\n3090\n122\n118\n6211\n155\n242\n2785\n\n\n4\n2\n220\n234\n391\n150\n-1\n6172\n45\n153\n2595\nX.describe()\n\n\n\n\n\n\n\n\nSlope\nHillshade_9am\nHillshade_Noon\nHorizontal_Distance_To_Roadways\nHillshade_3pm\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Fire_Points\nAspect\nHorizontal_Distance_To_Hydrology\nElevation\n\n\n\n\ncount\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n581012.000000\n\n\nmean\n14.103704\n212.146049\n223.318716\n2350.146611\n142.528263\n46.418855\n1980.291226\n155.656807\n269.428217\n2959.365301\n\n\nstd\n7.488242\n26.769889\n19.768697\n1559.254870\n38.274529\n58.295232\n1324.195210\n111.913721\n212.549356\n279.984734\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-173.000000\n0.000000\n0.000000\n0.000000\n1859.000000\n\n\n25%\n9.000000\n198.000000\n213.000000\n1106.000000\n119.000000\n7.000000\n1024.000000\n58.000000\n108.000000\n2809.000000\n\n\n50%\n13.000000\n218.000000\n226.000000\n1997.000000\n143.000000\n30.000000\n1710.000000\n127.000000\n218.000000\n2996.000000\n\n\n75%\n18.000000\n231.000000\n237.000000\n3328.000000\n168.000000\n69.000000\n2550.000000\n260.000000\n384.000000\n3163.000000\n\n\nmax\n66.000000\n254.000000\n254.000000\n7117.000000\n254.000000\n601.000000\n7173.000000\n360.000000\n1397.000000\n3858.000000\nsample_size = 1000 # Adjust this based on your dataset size\nX_sampled = X.sample(n=sample_size, random_state=42)\nY_sampled = Y.loc[X_sampled.index]\nX_sampled.describe()\n\n\n\n\n\n\n\n\nSlope\nHillshade_9am\nHillshade_Noon\nHorizontal_Distance_To_Roadways\nHillshade_3pm\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Fire_Points\nAspect\nHorizontal_Distance_To_Hydrology\nElevation\n\n\n\n\ncount\n1000.000000\n1000.00000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n14.239000\n211.96100\n222.901000\n2337.541000\n142.274000\n45.694000\n1955.295000\n155.262000\n265.347000\n2943.082000\n\n\nstd\n7.613004\n28.15678\n19.640951\n1562.373354\n39.159244\n58.644604\n1325.619385\n112.038618\n215.242347\n284.695287\n\n\nmin\n1.000000\n75.00000\n98.000000\n0.000000\n0.000000\n-152.000000\n30.000000\n0.000000\n0.000000\n1920.000000\n\n\n25%\n9.000000\n198.00000\n213.000000\n1082.000000\n119.000000\n7.000000\n1019.750000\n58.750000\n104.750000\n2791.750000\n\n\n50%\n13.000000\n218.00000\n225.000000\n1969.000000\n142.000000\n29.000000\n1698.000000\n123.000000\n214.000000\n2980.500000\n\n\n75%\n18.000000\n232.00000\n236.000000\n3266.250000\n168.250000\n66.000000\n2473.250000\n260.000000\n365.000000\n3146.000000\n\n\nmax\n47.000000\n253.00000\n254.000000\n7078.000000\n246.000000\n387.000000\n6769.000000\n359.000000\n1237.000000\n3702.000000\n_ = X_sampled.hist(figsize=(20, 14))\nsns.pairplot(X_sampled)\n# Correlation matrix\nimport seaborn as sns\ncorr_matrix = X_sampled.corr()\n\n# Heatmap of the correlation matrix\n\n_= sns.heatmap(corr_matrix, annot=True)"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#the-graphic-show-that-it-seems-dont-exist-a",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#the-graphic-show-that-it-seems-dont-exist-a",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "The graphic show that it seems don’t exist a",
    "text": "The graphic show that it seems don’t exist a\n\n# box plot\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#boxplot-is-a-good-tool-to-visualise-a-effect-of-the-numerical-variable-in-the-categorical-target",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#boxplot-is-a-good-tool-to-visualise-a-effect-of-the-numerical-variable-in-the-categorical-target",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Boxplot is a good tool to visualise a effect of the numerical variable in the categorical target",
    "text": "Boxplot is a good tool to visualise a effect of the numerical variable in the categorical target\n\nwith open('Dataset2.csv', 'rb') as f:\n    result = chardet.detect(f.read())  # or readline if the file is large\n#print(result['encoding'])\n\nDataset2 = pd.read_csv('Dataset2.csv', delimiter=\",\",decimal = \".\",encoding=result['encoding'])\nDataset2.head()\n\n\n\n\n\n\n\n\nWilderness_Area\nSoil_Type\n\n\n\n\n0\n1\n29\n\n\n1\n1\n29\n\n\n2\n1\n12\n\n\n3\n1\n30\n\n\n4\n1\n29\n\n\n\n\n\n\n\n\nprint(Dataset2.dtypes)\n\nWilderness_Area    int64\nSoil_Type          int64\ndtype: object\n\n\n\nDataset2[['Wilderness_Area']].value_counts()\n\nWilderness_Area\n1                  260796\n3                  253364\n4                   36968\n2                   29884\nName: count, dtype: int64\n\n\n\n_=Dataset2[['Soil_Type']].hist(figsize=(20, 14))"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#wilderness_area-is-categorical-variables-and-soil_type-is-numerical-variable.",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#wilderness_area-is-categorical-variables-and-soil_type-is-numerical-variable.",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Wilderness_Area is categorical variables and Soil_Type is numerical variable.",
    "text": "Wilderness_Area is categorical variables and Soil_Type is numerical variable.\n\n# Concatenate the data\n\ncombined_X = pd.concat([X, Dataset2], axis=1)\ncombined_X.head()\n\n\n\n\n\n\n\n\nSlope\nHillshade_9am\nHillshade_Noon\nHorizontal_Distance_To_Roadways\nHillshade_3pm\nVertical_Distance_To_Hydrology\nHorizontal_Distance_To_Fire_Points\nAspect\nHorizontal_Distance_To_Hydrology\nElevation\nWilderness_Area\nSoil_Type\n\n\n\n\n0\n3\n221\n232\n510\n148\n0\n6279\n51\n258\n2596\n1\n29\n\n\n1\n2\n220\n235\n390\n151\n-6\n6225\n56\n212\n2590\n1\n29\n\n\n2\n9\n234\n238\n3180\n135\n65\n6121\n139\n268\n2804\n1\n12\n\n\n3\n18\n238\n238\n3090\n122\n118\n6211\n155\n242\n2785\n1\n30\n\n\n4\n2\n220\n234\n391\n150\n-1\n6172\n45\n153\n2595\n1\n29\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(combined_X, Y,test_size=0.2, random_state=12345)\n\nX_train.shape\nX_test.shape\nprint(X_train.shape,X_test.shape)\n\n(464809, 12) (116203, 12)"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#elastic-net-principle.",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#elastic-net-principle.",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Elastic net principle.",
    "text": "Elastic net principle.\nThe elastic net penalty is a regularization technique used in machine learnings to prevent overfitting. It combines properties of both L1 (Lasso) and L2 (Ridge) regularizatio andbines both L1 and L2 regularization penalt \\[ies:\nE_ast_c Net Pena\\alpha ty _ α*L1 Penalty\\alpha+ (1_α)*L2 P\\]enalty\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclf_l1l2_LR = LogisticRegressionCV(penalty='elasticnet', l1_ratios=[0.5], \n                                   cv=5, multi_class=\"multinomial\", \n                                 solver=\"saga\",tol=0.01, random_state=12345)\n\nmodel = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\naccuracy_LR = accuracy_score(Y_test, prediction)\n\nprint(\"Accuracy of Logistic Regression :\",\"%.3f\" % accuracy_LR)\n\n\nAccuracy of Logistic Regression : 0.714\n\n\n\nfrom sklearn.metrics import confusion_matrix\n# Confusion matrix\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(Y_test, prediction)\n\n# Display the confusion matrix using Seaborn's heatmap\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted--example",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted--example",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:- Example :",
    "text": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:- Example :\nClass 129 52953 time s Class 24 784492 tim es Class5643173 ti mes Clas2114: 95"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For",
    "text": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For\n\nexample: 1095041 instances of Class 1 were misclassified as Class 2 836401 instances of Class 1 were misclassified as Class 287 527 instances of Class 3 were misclassified as Class 2, and so on.times\n\nSome classes like Class 0 and Class 1 have a high number of correct predictions(29529,44 784 ), suggesting the model performs well on these classes. Other classes, such as Class 3 and Class 4, have very few correct predictions, which may indicate that the model struggles with these classes, or they are underrepresented in the training data. There seems to be a considerable number of instances where Class 0 is misclassified as Class 1 and vice versa, indicating possible class confusion or similarity in feature patterns for these classes.\nwhile the model is quite accurate for certain classes: mes), it performs poorly on ot()hers. This could be due to various factors, including class imbalance, lack of distinguishable features, or insufficient model complexity to capture the nuances between classes."
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#part-3",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#part-3",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Part 3",
    "text": "Part 3\nOOB error is an internal validation measure for random forests, calculated by aggregating prediction performance on the data not used during training of each constituent tree. It serves as an estimate of generalization capability without external validation.\n\n## Training Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndepths = [10, 20, 30]\noob_errors = []\nmodels = []\nbest_oob_error = float('inf')\nbest_model = None\ni = 0\nfor depth in depths:\n    print(i)\n    model = RandomForestClassifier(max_depth=depth, oob_score=True, random_state=42,\n                                   n_estimators=100,  \n                                   warm_start=True  # This allows us to add more estimators later if needed\n                                  )\n    model.fit(X_train, Y_train)\n    oob_error = 1 - model.oob_score_\n    oob_errors.append(oob_error)\n    models.append(model)\n    if oob_error &lt; best_oob_error:\n        best_oob_error = oob_error\n        best_model = model\n    i = i+1\n    print(\"Done\")\n\n# Print OOB errors for each model\nfor depth, error in zip(depths, oob_errors):\n    print(f\"Depth: {depth}, OOB Error: {error}\")\n\n\nDepth: 10, OOB Error: 0.21510340806653916\nDepth: 20, OOB Error: 0.057580640650245596\nDepth: 30, OOB Error: 0.03892996908407542\n\n\n\n# Compute the accuracy of the best random forest model\npredictions = best_model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy of the Best Random Forest: {:.3f}\".format(accuracy))\n\n# Display the confusion matrix\nconf_matrix = confusion_matrix(Y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\nAccuracy of the Best Random Forest: 0.963"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:",
    "text": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:\n\nExample : Class 1: 40 425 times Class 2: 55494 times Class 3: 68343 times Class 4: 486 times"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for-1",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for-1",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For",
    "text": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For\n\nexample:\n9,641 instances of Class 1 were misclassified as Class 2. 401 instances of Class 1 were misclassified as Class 4. 527 instances of Class 3 were misclassified as Class 2, and so on."
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted-1",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#diagonal-values-the-numbers-along-the-diagonal-of-the-matrix-represent-correct-predictions-for-each-class.-the-model-has-correctly-predicted-1",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:",
    "text": "Diagonal Values: The numbers along the diagonal of the matrix represent correct predictions for each class. The model has correctly predicted:\n\nExample : Class 1: 32153 times Class 2: 47492 times Class 3: 6173 times Class 4: 95 times"
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for-2",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#off-diagonal-values-these-numbers-indicate-the-instances-where-the-model-has-made-incorrect-predictions.-for-2",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For",
    "text": "Off-Diagonal Values: These numbers indicate the instances where the model has made incorrect predictions. For\n\nexample:\n9641 instances of Class 1 were misclassified as Class 2. 401 instances of Class 1 were misclassified as Class 4. 527 instances of Class 3 were misclassified as Class 2, and so on."
  },
  {
    "objectID": "posts/machineLearning3A/Jumbong_Junior.html#this-model-is-very-good-but-do-more-error-that-a-random-forest-but-more-accurate-that-logistic-regression-even-do-it-is-not-accurate.",
    "href": "posts/machineLearning3A/Jumbong_Junior.html#this-model-is-very-good-but-do-more-error-that-a-random-forest-but-more-accurate-that-logistic-regression-even-do-it-is-not-accurate.",
    "title": "We can notice that the data is labelised so the model that can be applied in the data is classification",
    "section": "This model is very good but do more error that a random forest but more accurate that logistic regression even do it is not accurate.",
    "text": "This model is very good but do more error that a random forest but more accurate that logistic regression even do it is not accurate.\n\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\ndef compute_roc_auc(models, X, Y_test):\n    \"\"\"\n    Compute ROC AUC for a list of models.\n    \n    Args:\n    models (dict): A dictionary of models with their names as keys.\n    X_test (array-like): Test features.\n    Y_test (array-like): True labels for the test set.\n    \n    Returns:\n    dict: A dictionary containing FPR, TPR, and ROC AUC for each model.\n    \"\"\"\n    Y_classes = np.unique(Y_test)\n    Y_test_binarized = label_binarize(Y_test, classes=Y_classes)\n    n_classes = len(Y_classes)\n    \n    results = {}\n\n    for model_name, model in models.items():\n        if model_name == 'Logistic':\n            model = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n            model.fit(X_train,Y_train)\n            score = model.predict_proba(X_test)\n        else:\n          \n            score = model.predict_proba(X_test)\n        \n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        # Compute ROC for each class\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(Y_test_binarized[:, i], score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_binarized.ravel(), score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        results[model_name] = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc, 'score':score}\n\n    return results\n\nmodels = {\n    'Logistic': clf_l1l2_LR,\n      # Assurez-vous que clf_svm est entraîné sur des données mises à l'échelle si nécessaire\n    'Random Forests': best_model,\n    'XGBOOST': clf_xgb,\n    \n}\n\nroc_results = compute_roc_auc(models, X_test, Y_test)\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k--')  # Ligne diagonale\n\nfor model_name, metrics in roc_results.items():\n    plt.plot(metrics['fpr']['micro'], metrics['tpr']['micro'], label=f'{model_name} Micro (area = {metrics[\"roc_auc\"][\"micro\"]:.2f})')\n\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title('Micro-Average Receiver Operating Characteristic')\nplt.show()"
  },
  {
    "objectID": "posts/TBATS/index.html",
    "href": "posts/TBATS/index.html",
    "title": "TBATS : N’a pas de contraintes de saisonnalité",
    "section": "",
    "text": "Dans notre quête pour décrypter les tendances de consommation énergétique en France, tout au long de notre projet de série temporelle, nous avons été confrontés à un défi de taille : prédire efficacement la consommation future d’énergie dans un contexte de double saisonnalité. En explorant au-delà des méthodes classiques telles que les modèles SARIMA et ARIMA, nous avons rencontré des obstacles liés à l’estimation des variations saisonnières hebdomadaires et annuelles. Ces complexités nous ont conduits vers une solution innovante : le modèle TBATS.\nLe modèle TBATS, une avancée significative dans l’analyse des séries temporelles, embrasse la multifacette de la saisonnalité grâce à une élégante synthèse de fonctions trigonométriques. Contrairement aux modèles ARIMA et SARIMA qui peinaient à capturer les subtilités de nos données, TBATS a brillé par sa capacité à intégrer des périodicités multiples.\nDans cet article, nous plongeons au cœur de la méthodologie mathématique du modèle TBATS avant de le mettre en application sur nos données de consommation énergétique. Rejoignez-nous pour une exploration de la modélisation prédictive avec Python et la librairie tbats, et découvrez comment nous avons illuminé le chemin vers des prévisions énergétiques plus précises.\n\nPrésentation du modèle TBATS\nLe modèle TBATS ou encore (Trigonometric, Box-Cox transform, ARMA errors, Trend and Seasonal components)(De Livera, Hyndman, and Snyder 2011) a pour paramètres TBATS(\\(\\omega\\), {p,q}, \\(\\phi\\), \\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\)) où :\n\n\\(\\omega\\) correspond à la transformation de Box-Cox.\n{p,q} correspond aux paramètres de l’ARMA.\n\\(\\phi\\) correspond à la tendance.\n\\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\) correspond aux paramètres de saisonnalité.\n\\(k_1,...,k_n\\) correspond aux nombres de Fourier de séries pairs.\n\nLe model s’écrit de la manière suivante :\n\\[\ny_t(\\omega) = l_{t-1} + \\phi b_{t-1} + \\sum_{i=1}^{T} s_{t-1}(i) + \\alpha d_t\n\\] \\[\nb_t = b_{t-1} + \\beta d_t\n\\] \\[\ns_t(i) = \\sum_{j=1}^{k_i} s_{j,t}(i)\n\\] \\[\ns_{j,t}(i) = s_{j,t-1}(i) \\cos \\lambda_j(i) + s^{*}_{j,t-1}(i) \\sin \\lambda_j(i) + \\gamma^{(i)}_1 d_t\n\\] \\[\ns^{*}_{j,t}(i) = -s_{j,t-1}(i) \\sin \\lambda_j(i) + s^{*}_{j,t-1}(i) \\cos \\lambda_j(i) + \\gamma^{(i)}_2 d_t\n\\] \\[\n\\lambda_j(i) = \\frac{2\\pi j}{m}\n\\]\noù :\n\n\\(i = 1, \\ldots, T\\)\n\\(d_t\\) est un processus ARMA ( p, q ),\n\\(\\alpha\\), \\(\\beta\\), \\(\\gamma_1\\) et \\(\\gamma_2\\) sont des paramètres de lissage,\n\\(l_0\\) est le niveau initial,\n\\(b_0\\) est la valeur de la pente.\n\nLes erreurs de prévisions seront modélisées par les indicateurs de qualité suivants :\n\nMean Squared Error (MSE) : \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\\)\nRoot Mean Squared Error (RMSE) : \\(RMSE = \\sqrt{MSE}\\)\nMean Absolute Error (MAE) : \\(MAE = \\frac{1}{n} \\sum_{i=1}^{n} |e_i|\\)\nMean Absolute Percentage Error (MAPE) : \\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{e_i}{y_i} \\right| \\cdot 100\\%\\)\nMean Error (ME) : \\(ME = \\frac{1}{n} \\sum_{i=1}^{n} e_i\\)\nMean Percentage Error (MPE) : $MPE = _{i=1}^{n} % $\nMean Absolute Scaled Error (MASE): \\(MASE = \\frac{n}{n-m} \\frac{\\sum_{i=1}^{n} |e_i|}{\\sum_{i=m+1}^{n} |y_i - y_{i-m}|}\\)\nAutocorrelation function of errors at lag 1 (ACF1) : \\(ACF1 = \\frac{\\sum_{i=1}^{n-1} (e_i - ME) \\cdot (e_{i+1} - ME)}{\\sum_{i=1}^{n} (e_i - ME)^2}\\)\n\noù : \\(e_t\\) is the error \\(e_t = y_t -{ y_t}^*\\)\n\\(y_t\\) est la valeur actuelle, \\(y_t^*\\) est la valeur prédicte , m est la période de saisonalité.\nDans l’analyse des prédictions de modèles statistiques, la compréhension des erreurs est cruciale. En comparant le Mean Error (ME) et le Mean Absolute Error (MAE), nous pouvons déterminer si les valeurs prédites sont systématiquement plus élevées ou plus faibles que les valeurs réelles, indiquant un biais directionnel. De même, la comparaison entre le Mean Percentage Error (MPE) et le Mean Absolute Percentage Error (MAPE) révèle l’ampleur de ce biais en termes de pourcentage.\nMais ce n’est pas tout. L’analyse du Mean Squared Error (MSE) peut révéler la présence de valeurs aberrantes ou d’erreurs exceptionnellement élevées dans les prédictions. Ces erreurs extrêmes se manifestent souvent par un écart significatif entre le MAE et le Root Mean Squared Error (RMSE), où le RMSE, en donnant plus de poids aux grandes erreurs, met en lumière les défauts plus subtils de notre modèle prédictif.\n\n\nPrésentation des données\nDans notre étude appronfondie sur la demande énergétique en france, nous avons plongé au coeur des données de consommation, mésurées en mégawattts, révelatrices des tendances de consommation du pays. Afin d’affiner notre analyse nous avons présenter les données sous forme de séries temporelles journalières. Cette approche nous permettra d’aborder plus aisement l’analyse.\n\n\nLe dendogramme de la série temporelle\nNous avons mis en évidence des motifs saisonniers clés à travers la représentation d’un dendogramme. Cet outil permettra de vérifier que 365 et 7 sont des saisonnalités.\n\nfrom scipy import signal\n\nfrequencies, power_spectral_density = signal.periodogram(df_con_daily['consommation'].values)\n\n\n# Tracé du périodogramme\nplt.figure(figsize=(5, 3))\nplt.plot(frequencies, power_spectral_density)\nplt.title('Périodogramme')\nplt.xlabel('Fréquence')\nplt.ylabel('Densité spectrale de puissance')\n\n# Ajout de la ligne verticale à 1/12\nplt.axvline(x=1/365, color='red', linestyle='--')\nplt.axvline(x=1/7, color='yellow', linestyle='--')\n\nplt.show()\n\n\n\n\ndendrogramme\n\n\nNous observons que les cylces de 365 et 7 se détachent nettement, mettant en évidence une saisonnalité annuelle est hebdommadaire.\nEn plus de l’analyse dendrogramme, une autre méthode efficace pour mettre en lumière les saisonnalités dans nos données énergétiques consiste à décomposer la série temporelle. Cette approche permet d’isoler et d’examiner les composantes saisonnières annuelles et hebdomadaires de manière distincte. L’analyse des fonctions d’autocorrélation et d’autocorrélation partielle offre également des insights précieux. Cependant, pour rester concentrés sur les aspects les plus pertinents de notre étude, nous choisirons de ne pas approfondir cette méthode dans ce cadre. Les paramètres du model TBATS(False,{0,0},0.85,{&lt;7,365&gt;}) correspondent au mieux aux données.\nDans notre démarche analytique, une étape cruciale a été la segmentation de nos données en ensembles d’entraînement et de test. Cette division stratégique est essentielle pour affiner et évaluer la précision de nos modèles prédictifs. La variable date_cutoff joue un rôle clé dans ce processus, définissant le point de séparation entre les périodes d’entraînement et de test.\n\nimport pandas as pd\nfrom sktime.datatypes import check_raise\nfrom datetime import datetime\n\ny = df_con_daily['consommation']\ny.index = pd.to_datetime(y.index)\n\n\n\ndate_cutoff = pd.Timestamp('2019-12-31')\n\n# Ensuite, effectuez la comparaison\ny_train = df_con_daily[df_con_daily.index &lt; date_cutoff]['consommation']\ny_test = df_con_daily[df_con_daily.index &gt;= date_cutoff]['consommation']\ny_train.index = pd.to_datetime(y_train.index)\n\nEnsuite nous avons instancier le modèle TBATS avec les différentes saisonnalités 7 et 365. n_jobs facilite le temps de computation. Pour plus de détails voir [TBATS](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.tbats.TBATS.html). Assurez vous que vos index ou la colonne date soit de type pandas.core.indexes.datetimes.DatetimeIndex. Pour cela vous pouvez vous servir de chatgpt.\n\nforecaster = TBATS(sp = [7, 365], n_jobs = 1)\nmodel = forecaster.fit(y_train)\n\n# Prediction\nfh = len(y_test)\ny_pred = model.forecast(fh)\n\nfig, ax = plt.subplots(figsize = (15,5))  \ny.plot(title = 'TBATS dayly energie consumption', xlabel = '', ax = ax)\ny_pred.plot(ax = ax)\nax.legend(['Actual Values', 'Forecast'])\nplt.show()\n\nUne fonction pour les différentes métriques pour évaluer la qualité du modèle est donnée par :\n\n\n\nprevision\n\n\n\ndef print_metrics(y_true, y_pred, model_name):\n    mae_ = mean_absolute_error(y_true, y_pred)\n    rmse_ = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape_ = mean_absolute_percentage_error(y_true, y_pred)\n    smape_ = mean_absolute_percentage_error(y_true, y_pred, symmetric = True)\n    \n    dict_ = {'MAE': mae_, 'RMSE': rmse_,\n             'MAPE': mape_, 'SMAPE': smape_ }\n    \n    df = pd.DataFrame(dict_, index = [model_name])\n    return(df.round(decimals = 2)) \n\nPour avoir les résultats des performances du modèle, il faut exécuter cette fonction.\n\nprint_metrics(y_test, y_pred, 'TBATS Forecaster')\n\n\n\nRésultats de Prévision avec TBATS\nNous avons essayer de mettre les résultats dans un tableau :\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate \n\ntable = [[\"TBATS Forecaster\",\"290.94\",\"412.76\",\"0.07\",\"0.06\"]]\n\nMarkdown((tabulate(\n    table,\n    headers = [\"\",\"MAE\",\"RMSE\",\"MAPE\",\"SMAPE\"]\n)\n))\n\n\nIndicateurs de performance \n\n\n\nMAE\nRMSE\nMAPE\nSMAPE\n\n\n\n\nTBATS Forecaster\n290.94\n412.76\n0.07\n0.06\n\n\n\n\n\n\n\nConclusion\n\nLe Mean Absolute Error (MAE) : Il montre qu’en moyenne, les prévisions s’écartent de 290.94 unités des valeurs réelles, nous donnant une idée de l’erreur moyenne absolue.\nLe Root Mean Squared Error (RMSE) : Sa valeur de 493.17, n’étant pas très élévée comparé au MAE, nous pouvons conclure que les erreurs de prévision sont relativement faibles.\nLe Mean Absolute Percentage Error (MAPE) et le Symmetric Mean Absolute Percentage Error (SMAPE) : Ils indiquent une erreur moyenne de prédiction de 0.07%, ce qui est considéré comme relativement précis dans notre contexte.\n\nAinsi le modèle TBATS est un modèle qui permet de faire des prévisions acceptables sur les données possèdant des saisonnalités multiples.\n\nReferences\n\n\nBROŻYNA, Jacek, Grzegorz Mentel, Beata Szetela, and Wadim Strielkowski. 2018. “MULTI-SEASONALITY IN THE TBATS MODEL USING DEMAND FOR ELECTRIC ENERGY AS a CASE STUDY.” Economic Computation & Economic Cybernetics Studies & Research 52 (1). https://www.researchgate.net/profile/Grzegorz-Mentel/publication/323868510_Multi-Seasonality_in_the_TBATS_Model_Using_Demand_for_Electric_Energy_as_a_Case_Study/links/5ab0afafaca2721710fe20b8/Multi-Seasonality-in-the-TBATS-Model-Using-Demand-for-Electric-Energy-as-a-Case-Study.pdf.\n\n\nDe Livera, Alysha M., Rob J. Hyndman, and Ralph D. Snyder. 2011. “Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing.” Journal of the American Statistical Association 106 (496): 1513–27. https://doi.org/10.1198/jasa.2011.tm09771."
  }
]