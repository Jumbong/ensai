{
  "hash": "980a7c3ac30a4315678537c7757ed909",
  "result": {
    "markdown": "---\ntitle: \"Examen de machine learning\"\nauthor: \"Jumbong Junior\"\ndate: \"2023-12-08\"\ncategories: [news]\ntoc: true\n---\n\n# Introduction\n\nIl est possible que ce soit Da Veiga qui assure vos cours, mais je vais vous offrir un aperçu de ce à quoi pourrait ressembler votre examen. C'est important, car nous avons tendance à sous-estimer ce type d'activité, surtout lorsqu'il autorise l'utilisation de générateurs de texte tels que ChatGPT. Cette année, peu d'étudiants ont achevé le projet, la charge de données étant longue et fastidieuse. Je vous conseille de vous y prendre en avance. Préparez des fonctions exécutant certaines tâches spécifiques, que je vous expliquerai progressivement.\n\n# Les données\n\nLes données, très larges proposées sont Dataset1 et Dataset2. Vous les trouverez\nsur mon [github]()\n\n\n# Packages\n\nJ'utiliserai les packages suivants: sklearn, pandas, numpy, matplotlib,seaborn pour la visualiation. \n\n# Objectif\n\nL'objectif de ce notebook consistera à une 'analyse exploratoire des données, le prétraitement des données pour la modélisation, et l'application ainsi que l'évaluation de modèles de classification() sur un ensemble de données.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Importation des packages\n\n\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport chardet\nimport warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import mutual_info_classif\n\nfrom sklearn.feature_selection import mutual_info_classif\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n```\n:::\n\n\n# Chargement des données\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nwith open('Dataset1.csv', 'rb') as f:\n    result = chardet.detect(f.read())  # or readline if the file is large\n#print(result['encoding'])\n\nDataset1 = pd.read_csv('Dataset1.csv', delimiter=\",\",decimal = \".\",encoding=result['encoding'])\nDataset1.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Vegetation_Type_2</th>\n      <th>Groundwater_Level_1</th>\n      <th>Drainage_Quality_1</th>\n      <th>Slope</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Pollution_Level_1</th>\n      <th>Water_Source_Distance_2</th>\n      <th>Terrain_Roughness_2</th>\n      <th>Urban_Proximity_Index_1</th>\n      <th>...</th>\n      <th>Soil_Moisture_Level_2</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Wind_Speed_Average_2</th>\n      <th>Elevation_Range_1</th>\n      <th>Pollution_Level_2</th>\n      <th>Vegetation_Type_1</th>\n      <th>Temperature_Average_1</th>\n      <th>Canopy_Cover_2</th>\n      <th>Elevation</th>\n      <th>Cover_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>450.684968</td>\n      <td>2054.426315</td>\n      <td>229.994980</td>\n      <td>3</td>\n      <td>221</td>\n      <td>232</td>\n      <td>181.000000</td>\n      <td>227.161401</td>\n      <td>216.000000</td>\n      <td>14.0</td>\n      <td>...</td>\n      <td>418.375833</td>\n      <td>258</td>\n      <td>116.000000</td>\n      <td>219.000000</td>\n      <td>224.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n      <td>503.586301</td>\n      <td>2596</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>889.885392</td>\n      <td>997.573983</td>\n      <td>208.000000</td>\n      <td>2</td>\n      <td>220</td>\n      <td>235</td>\n      <td>166.000000</td>\n      <td>933.851870</td>\n      <td>251.000000</td>\n      <td>16.0</td>\n      <td>...</td>\n      <td>2488.726342</td>\n      <td>212</td>\n      <td>179.025367</td>\n      <td>236.000000</td>\n      <td>224.000000</td>\n      <td>69.000000</td>\n      <td>17.000000</td>\n      <td>537.000000</td>\n      <td>2590</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1232.738700</td>\n      <td>676.352745</td>\n      <td>214.313227</td>\n      <td>9</td>\n      <td>234</td>\n      <td>238</td>\n      <td>205.000000</td>\n      <td>4235.282432</td>\n      <td>176.898683</td>\n      <td>13.0</td>\n      <td>...</td>\n      <td>1852.423116</td>\n      <td>268</td>\n      <td>156.000000</td>\n      <td>200.872578</td>\n      <td>230.415218</td>\n      <td>120.357544</td>\n      <td>10.000000</td>\n      <td>376.000000</td>\n      <td>2804</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3359.512595</td>\n      <td>4720.481538</td>\n      <td>230.000000</td>\n      <td>18</td>\n      <td>238</td>\n      <td>238</td>\n      <td>176.002696</td>\n      <td>1066.935784</td>\n      <td>220.000000</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>5388.528248</td>\n      <td>242</td>\n      <td>156.000000</td>\n      <td>230.000000</td>\n      <td>243.000000</td>\n      <td>15.000000</td>\n      <td>12.933234</td>\n      <td>30.000000</td>\n      <td>2785</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1907.275049</td>\n      <td>2187.627994</td>\n      <td>221.000000</td>\n      <td>2</td>\n      <td>220</td>\n      <td>234</td>\n      <td>109.000000</td>\n      <td>1182.489702</td>\n      <td>184.859965</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>304.080537</td>\n      <td>153</td>\n      <td>112.000000</td>\n      <td>232.000000</td>\n      <td>213.000000</td>\n      <td>39.000000</td>\n      <td>14.000000</td>\n      <td>330.000000</td>\n      <td>2595</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 51 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#info\nDataset1.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 51 columns):\n #   Column                              Non-Null Count   Dtype  \n---  ------                              --------------   -----  \n 0   Vegetation_Type_2                   581012 non-null  float64\n 1   Groundwater_Level_1                 581012 non-null  float64\n 2   Drainage_Quality_1                  581012 non-null  float64\n 3   Slope                               581012 non-null  int64  \n 4   Hillshade_9am                       581012 non-null  int64  \n 5   Hillshade_Noon                      581012 non-null  int64  \n 6   Pollution_Level_1                   581012 non-null  float64\n 7   Water_Source_Distance_2             581012 non-null  float64\n 8   Terrain_Roughness_2                 581012 non-null  float64\n 9   Urban_Proximity_Index_1             581012 non-null  float64\n 10  Sunlight_Intensity_2                581012 non-null  float64\n 11  Rainfall_Index_1                    581012 non-null  float64\n 12  Canopy_Cover_1                      581012 non-null  float64\n 13  Rock_Type_2                         581012 non-null  float64\n 14  Rainfall_Index_2                    581012 non-null  float64\n 15  Terrain_Slope_Angle_2               581012 non-null  float64\n 16  Temperature_Average_2               581012 non-null  float64\n 17  Air_Quality_Index_1                 581012 non-null  float64\n 18  Horizontal_Distance_To_Roadways     581012 non-null  int64  \n 19  Soil_Mineral_Content_2              581012 non-null  float64\n 20  Wildlife_Density_2                  581012 non-null  float64\n 21  Water_Source_Distance_1             581012 non-null  float64\n 22  Air_Quality_Index_2                 581012 non-null  float64\n 23  Wildlife_Density_1                  581012 non-null  float64\n 24  Drainage_Quality_2                  581012 non-null  float64\n 25  Land_Use_Category_2                 581012 non-null  float64\n 26  Groundwater_Level_2                 581012 non-null  float64\n 27  Wind_Speed_Average_1                581012 non-null  float64\n 28  Hillshade_3pm                       581012 non-null  int64  \n 29  Terrain_Slope_Angle_1               581012 non-null  float64\n 30  Land_Use_Category_1                 581012 non-null  float64\n 31  Elevation_Range_2                   581012 non-null  float64\n 32  Vertical_Distance_To_Hydrology      581012 non-null  int64  \n 33  Urban_Proximity_Index_2             581012 non-null  float64\n 34  Horizontal_Distance_To_Fire_Points  581012 non-null  int64  \n 35  Soil_Mineral_Content_1              581012 non-null  float64\n 36  Aspect                              581012 non-null  int64  \n 37  Soil_Moisture_Level_1               581012 non-null  float64\n 38  Terrain_Roughness_1                 581012 non-null  float64\n 39  Rock_Type_1                         581012 non-null  float64\n 40  Sunlight_Intensity_1                581012 non-null  float64\n 41  Soil_Moisture_Level_2               581012 non-null  float64\n 42  Horizontal_Distance_To_Hydrology    581012 non-null  int64  \n 43  Wind_Speed_Average_2                581012 non-null  float64\n 44  Elevation_Range_1                   581012 non-null  float64\n 45  Pollution_Level_2                   581012 non-null  float64\n 46  Vegetation_Type_1                   581012 non-null  float64\n 47  Temperature_Average_1               581012 non-null  float64\n 48  Canopy_Cover_2                      581012 non-null  float64\n 49  Elevation                           581012 non-null  int64  \n 50  Cover_Type                          581012 non-null  int64  \ndtypes: float64(40), int64(11)\nmemory usage: 226.1 MB\n```\n:::\n:::\n\n\nLes données contiennent 51 colonnes et 581012 lignes.\nDans le code ci-dessous, nous allons séparer les données en output(Y) et en features(X). L'output ici est la variable **Cover_Type**.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n # split  data into training and testing sets\nX = Dataset1.drop(['Cover_Type'], axis=1).copy()\nY = Dataset1['Cover_Type'].copy()\n```\n:::\n\n\nLa prémiere chose à faire est de voir la distribution de Y. Afin de vérifier si c'est une variable catégorielle ou continue.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Distribution de Y\nsns.countplot(x='Cover_Type', data=Dataset1)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=618 height=429}\n:::\n:::\n\n\nOn peut voir que la variable est catégorielle. Elle contient 7 classes non équilibrées. On peut s'attendre que le modèle prédira mieux les classes les plus représentées(1 et 2) que les autres.\n\nNous ne traiterons pas ça ici. Les données étant labélisées, nous allons utiliser des modèles de classification.\n\n# Data preprocessing\n\nNous pouvons dans un premier temps vérifier s'il y a des valeurs manquantes dans les données.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Missing values\nDataset1.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\nVegetation_Type_2                     0\nGroundwater_Level_1                   0\nDrainage_Quality_1                    0\nSlope                                 0\nHillshade_9am                         0\nHillshade_Noon                        0\nPollution_Level_1                     0\nWater_Source_Distance_2               0\nTerrain_Roughness_2                   0\nUrban_Proximity_Index_1               0\nSunlight_Intensity_2                  0\nRainfall_Index_1                      0\nCanopy_Cover_1                        0\nRock_Type_2                           0\nRainfall_Index_2                      0\nTerrain_Slope_Angle_2                 0\nTemperature_Average_2                 0\nAir_Quality_Index_1                   0\nHorizontal_Distance_To_Roadways       0\nSoil_Mineral_Content_2                0\nWildlife_Density_2                    0\nWater_Source_Distance_1               0\nAir_Quality_Index_2                   0\nWildlife_Density_1                    0\nDrainage_Quality_2                    0\nLand_Use_Category_2                   0\nGroundwater_Level_2                   0\nWind_Speed_Average_1                  0\nHillshade_3pm                         0\nTerrain_Slope_Angle_1                 0\nLand_Use_Category_1                   0\nElevation_Range_2                     0\nVertical_Distance_To_Hydrology        0\nUrban_Proximity_Index_2               0\nHorizontal_Distance_To_Fire_Points    0\nSoil_Mineral_Content_1                0\nAspect                                0\nSoil_Moisture_Level_1                 0\nTerrain_Roughness_1                   0\nRock_Type_1                           0\nSunlight_Intensity_1                  0\nSoil_Moisture_Level_2                 0\nHorizontal_Distance_To_Hydrology      0\nWind_Speed_Average_2                  0\nElevation_Range_1                     0\nPollution_Level_2                     0\nVegetation_Type_1                     0\nTemperature_Average_1                 0\nCanopy_Cover_2                        0\nElevation                             0\nCover_Type                            0\ndtype: int64\n```\n:::\n:::\n\n\nIl n'y a pas de valeurs manquantes dans les données. \n\n# Features selections.\n\nNous allons supprimer les variables en se basant sur le critère de [l'information mutulle](https://fr.wikipedia.org/wiki/Information_mutuelle) qui n'apportent pas au moins 0.01 d'information à la variable cible.\n\nEn termes simples, l'information mutuelle mesure combien la connaissance d'une variable réduit l'incertitude concernant l'autre. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmutual_info = mutual_info_classif(X, Y)\n\n\nmutual_info_df = pd.DataFrame(mutual_info, index=X.columns, columns=['Mutual Information'])\n\n# Filtrer les caractéristiques avec une information mutuelle supérieure à 0.01\nrelevant_features = mutual_info_df[mutual_info_df['Mutual Information'] > 0.01].index\n\n# Afficher les caractéristiques pertinentes\nprint(\"Caractéristiques pertinentes (Information Mutuelle > 0.01) :\")\nprint(relevant_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCaractéristiques pertinentes (Information Mutuelle > 0.01) :\nIndex(['Slope', 'Hillshade_9am', 'Hillshade_Noon',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_3pm',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points',\n       'Aspect', 'Horizontal_Distance_To_Hydrology', 'Elevation'],\n      dtype='object')\n```\n:::\n:::\n\n\nNous allons supprimer les variables qui ne sont pas pertinentes.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Supprimer les caractéristiques non pertinentes\nX = X[relevant_features]\nX.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 10 columns):\n #   Column                              Non-Null Count   Dtype\n---  ------                              --------------   -----\n 0   Slope                               581012 non-null  int64\n 1   Hillshade_9am                       581012 non-null  int64\n 2   Hillshade_Noon                      581012 non-null  int64\n 3   Horizontal_Distance_To_Roadways     581012 non-null  int64\n 4   Hillshade_3pm                       581012 non-null  int64\n 5   Vertical_Distance_To_Hydrology      581012 non-null  int64\n 6   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n 7   Aspect                              581012 non-null  int64\n 8   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n 9   Elevation                           581012 non-null  int64\ndtypes: int64(10)\nmemory usage: 44.3 MB\n```\n:::\n:::\n\n\nLà on a plus que 10 features et les données sont toutes numériques. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nX.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Slope</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_3pm</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>Aspect</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Elevation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n      <td>581012.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.103704</td>\n      <td>212.146049</td>\n      <td>223.318716</td>\n      <td>2350.146611</td>\n      <td>142.528263</td>\n      <td>46.418855</td>\n      <td>1980.291226</td>\n      <td>155.656807</td>\n      <td>269.428217</td>\n      <td>2959.365301</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.488242</td>\n      <td>26.769889</td>\n      <td>19.768697</td>\n      <td>1559.254870</td>\n      <td>38.274529</td>\n      <td>58.295232</td>\n      <td>1324.195210</td>\n      <td>111.913721</td>\n      <td>212.549356</td>\n      <td>279.984734</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-173.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1859.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>9.000000</td>\n      <td>198.000000</td>\n      <td>213.000000</td>\n      <td>1106.000000</td>\n      <td>119.000000</td>\n      <td>7.000000</td>\n      <td>1024.000000</td>\n      <td>58.000000</td>\n      <td>108.000000</td>\n      <td>2809.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.000000</td>\n      <td>218.000000</td>\n      <td>226.000000</td>\n      <td>1997.000000</td>\n      <td>143.000000</td>\n      <td>30.000000</td>\n      <td>1710.000000</td>\n      <td>127.000000</td>\n      <td>218.000000</td>\n      <td>2996.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>18.000000</td>\n      <td>231.000000</td>\n      <td>237.000000</td>\n      <td>3328.000000</td>\n      <td>168.000000</td>\n      <td>69.000000</td>\n      <td>2550.000000</td>\n      <td>260.000000</td>\n      <td>384.000000</td>\n      <td>3163.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>66.000000</td>\n      <td>254.000000</td>\n      <td>254.000000</td>\n      <td>7117.000000</td>\n      <td>254.000000</td>\n      <td>601.000000</td>\n      <td>7173.000000</td>\n      <td>360.000000</td>\n      <td>1397.000000</td>\n      <td>3858.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# analyse exploratoire des données\n\nPour l'analyse exploratoire, nous allons utiliser qu'un échantillon des données.  Je vais en prendre 500\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsample_size = 500 # Adjust this based on your dataset size\nX_sampled = X.sample(n=sample_size, random_state=42)\nY_sampled = Y.loc[X_sampled.index]\nX_sampled.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Slope</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_3pm</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>Aspect</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Elevation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.00000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.642000</td>\n      <td>211.054000</td>\n      <td>223.252000</td>\n      <td>2313.700000</td>\n      <td>143.338000</td>\n      <td>46.152000</td>\n      <td>1906.97800</td>\n      <td>155.922000</td>\n      <td>271.758000</td>\n      <td>2945.782000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.678265</td>\n      <td>28.780405</td>\n      <td>19.804366</td>\n      <td>1548.527391</td>\n      <td>40.718494</td>\n      <td>60.782744</td>\n      <td>1297.58581</td>\n      <td>111.414732</td>\n      <td>227.100543</td>\n      <td>290.643937</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>75.000000</td>\n      <td>149.000000</td>\n      <td>85.000000</td>\n      <td>0.000000</td>\n      <td>-152.000000</td>\n      <td>42.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1983.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>9.000000</td>\n      <td>197.000000</td>\n      <td>212.750000</td>\n      <td>1086.250000</td>\n      <td>119.000000</td>\n      <td>8.000000</td>\n      <td>982.75000</td>\n      <td>59.000000</td>\n      <td>108.000000</td>\n      <td>2805.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.000000</td>\n      <td>217.000000</td>\n      <td>226.000000</td>\n      <td>1945.500000</td>\n      <td>142.000000</td>\n      <td>29.000000</td>\n      <td>1642.50000</td>\n      <td>125.000000</td>\n      <td>212.000000</td>\n      <td>3001.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>19.000000</td>\n      <td>232.000000</td>\n      <td>237.000000</td>\n      <td>3226.250000</td>\n      <td>171.000000</td>\n      <td>67.000000</td>\n      <td>2411.25000</td>\n      <td>257.000000</td>\n      <td>376.750000</td>\n      <td>3149.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>42.000000</td>\n      <td>253.000000</td>\n      <td>254.000000</td>\n      <td>7078.000000</td>\n      <td>246.000000</td>\n      <td>387.000000</td>\n      <td>6576.00000</td>\n      <td>359.000000</td>\n      <td>1110.000000</td>\n      <td>3529.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNous pouvons tracer la distribution de chaque variable.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n_ = X_sampled.hist(figsize=(20, 14))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=1545 height=1096}\n:::\n:::\n\n\nVoyons si on des dinausores dans les données.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nsns.pairplot(X_sampled)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=2357 height=2358}\n:::\n:::\n\n\nIl semble qu'il n'existe pas de corrélations linéaires entre les variables.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Correlation matrix\nimport seaborn as sns\ncorr_matrix = X_sampled.corr()\n\n# Heatmap of the correlation matrix\n\n_= sns.heatmap(corr_matrix, annot=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=770 height=634}\n:::\n:::\n\n\nCe graphique semble confirmer qu'il n'y a pas de corrélations linéaires entre les variables.\n\n## Box plot\n\nLe graphique ci-dessous montre la distribution de chaque variable en fonction de la variable cible.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# box plot\n# Transformed Cover_Type to categorical\nY_sampled = Y_sampled.astype('category')\n\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=585 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-3.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-4.png){width=601 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-5.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-6.png){width=604 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-7.png){width=601 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-8.png){width=593 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-9.png){width=601 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-10.png){width=602 height=433}\n:::\n:::\n\n\nInterprétation: Nous allons nous concentrer sur la variable **Elevation**.\n\n On peut voir que la variable **Elevation** est très discriminante. \n\nLes types de couverture 1, 2, et 7 montrent des médianes relativement élevées pour l'élévation, avec 7 ayant la médiane la plus élevée, suivie par 1 et 2.\n\n# Chargement des données\nDataset2.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nwith open('Dataset2.csv', 'rb') as f:\n    result = chardet.detect(f.read())  # or readline if the file is large\n#print(result['encoding'])\n\nDataset2 = pd.read_csv('Dataset2.csv', delimiter=\",\",decimal = \".\",encoding=result['encoding'])\nDataset2.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 2 columns):\n #   Column           Non-Null Count   Dtype\n---  ------           --------------   -----\n 0   Wilderness_Area  581012 non-null  int64\n 1   Soil_Type        581012 non-null  int64\ndtypes: int64(2)\nmemory usage: 8.9 MB\n```\n:::\n:::\n\n\nCette fois-ci, nous avons 2 variables et 581012 lignes.\nOn peut voir que les données sont de types int64 Inspectons ces données les pour voir si elles sont numériques ou catégorielles.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Distribution de Y\nsns.countplot(x='Wilderness_Area', data=Dataset2)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=618 height=429}\n:::\n:::\n\n\nOn peut voir que la variable est catégorielle. Elle contient 4 classes équilibrées. Passons à la deuxième variable.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsns.countplot(x='Soil_Type', data=Dataset2)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=620 height=431}\n:::\n:::\n\n\nIci, nous pouvons voir que la variable est plutôt numérique.  Traçons la distribution de la variable.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n_=Dataset2[['Soil_Type']].hist(figsize=(20, 14))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=1567 height=1096}\n:::\n:::\n\n\nComme les deux base de données,on les mêmes lignes, nous allons les concaténer.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ncombined_X = pd.concat([X, Dataset2], axis=1)\ncombined_X.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 12 columns):\n #   Column                              Non-Null Count   Dtype\n---  ------                              --------------   -----\n 0   Slope                               581012 non-null  int64\n 1   Hillshade_9am                       581012 non-null  int64\n 2   Hillshade_Noon                      581012 non-null  int64\n 3   Horizontal_Distance_To_Roadways     581012 non-null  int64\n 4   Hillshade_3pm                       581012 non-null  int64\n 5   Vertical_Distance_To_Hydrology      581012 non-null  int64\n 6   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n 7   Aspect                              581012 non-null  int64\n 8   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n 9   Elevation                           581012 non-null  int64\n 10  Wilderness_Area                     581012 non-null  int64\n 11  Soil_Type                           581012 non-null  int64\ndtypes: int64(12)\nmemory usage: 53.2 MB\n```\n:::\n:::\n\n\nNous pouvons maintenant séparer les données en train et test. Avec stratification sur la variable cible.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# split  data into training and testing sets\nprint(\"Splitting data into training and testing sets...\")\nX_train, X_test, Y_train, Y_test = train_test_split(combined_X, Y, test_size=0.2, random_state=42, stratify=Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSplitting data into training and testing sets...\n```\n:::\n:::\n\n\nAvant de passer à la modélisation, nous allons standardiser les données,  et encoder les variables catégorielles.\n\nD'abord, nous allons séparer les variables numériques et catégorielles.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nX_train.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\nIndex(['Slope', 'Hillshade_9am', 'Hillshade_Noon',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_3pm',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points',\n       'Aspect', 'Horizontal_Distance_To_Hydrology', 'Elevation',\n       'Wilderness_Area', 'Soil_Type'],\n      dtype='object')\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ncategorical_cols = ['Wilderness_Area']\nnumerical_cols = ['Slope', 'Hillshade_9am', 'Hillshade_Noon',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_3pm',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points',\n       'Aspect', 'Horizontal_Distance_To_Hydrology', 'Elevation', 'Soil_Type']\n```\n:::\n\n\nNous allons créer un pipeline pour standardiser les données numériques et encoder les variables catégorielles. J'adore les pipelines.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),  # or median\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)])\n\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)\n```\n:::\n\n\nIci nous n'avons pas de valeurs manquantes, mais nous avons quand même utilisé un imputer pour remplacer les valeurs manquantes par la moyenne pour les variables numériques et par la valeur la plus fréquente pour les variables catégorielles.\nUn premier pipeline pour les variables numériques et un deuxième pour les variables catégorielles. Nous avons utilisé un onehot encoder pour les variables catégorielles. Les données sont maintenant prêtes pour la modélisation.\n\n# Modélisation\n\n## Elastic Net Regression\n\nC'est une méthode de machine learning qui combine la régression Ridge et Lasso. Elle est utilisée pour résoudre le problème de surraprentissage, la multicollinéarité et la sélection de variables. \n\nPassons à sa modélisation :\n\nNous avons utilisé ici un modèle de régression logistique avec une pénalité elasticnet. Nous avons utilisé une validation croisée pour trouver le meilleur paramètre de régularisation. Nous avons utilisé une pénalité elasticnet avec un ratio de 0.5. Nous avons utilisé un solver saga qui est adapté aux problèmes multiclasse. Nous avons utilisé une tolérance de 0.01. Nous avons utilisé un random state de 12345.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nclf_l1l2_LR = LogisticRegressionCV(penalty='elasticnet', l1_ratios=[0.5], \n                                   cv=5, multi_class=\"multinomial\", \n                                 solver=\"saga\",tol=0.01, random_state=12345)\n\nmodel = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\naccuracy_LR = accuracy_score(Y_test, prediction)\n\nprint(\"Accuracy of Logistic Regression :\",\"%.3f\" % accuracy_LR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of Logistic Regression : 0.714\n```\n:::\n:::\n\n\nJ'ai un accuracy de 0.714. Ce n'est pas mal. Nous pouvons voir la matrice de confusion.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Confusion matrix\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(Y_test, prediction)\n\n# Display the confusion matrix using Seaborn's heatmap\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){width=770 height=597}\n:::\n:::\n\n\n- Les valeurs sur la diagonale principale (de haut à gauche à bas à droite) représentent le nombre de prédictions correctes pour chaque classe. Par exemple, il y a 29724 prédictions correctes pour la classe 0, 44616 pour la classe 1, et ainsi de suite.\n- Les valeurs hors de la diagonale indiquent les erreurs de classification. Par exemple, 11947 instances de la classe 0 ont été incorrectement prédites comme appartenant à la classe 1.\n- La classe 0 a le plus grand nombre de faux positifs, c'est-à-dire que de nombreuses instances d'autres classes ont été incorrectement prédites comme appartenant à la classe 0.\n- Les classes avec le moins de prédictions incorrectes (et donc les plus sombres dans la visualisation) sont la classe 3 et la classe 6, avec respectivement 187 et 1993 prédictions correctes.\nLes cases avec un fond plus clair, en dehors de la diagonale, indiquent des erreurs moins fréquentes entre les classes spécifiques.\n\n## Random Forest\n\n### OOB error (Out-of-bag error)\n\nOOB est une méthode de validation croisée pour les forêts aléatoires. Chaque arbre dans la forêt est construit à partir d'un échantillon bootstrap du jeu de données d'entraînement. Certaines observations sont laissées de côté et non utilisées dans la construction d'un arbre donné. Ces observations \"hors sac\" peuvent être utilisées pour évaluer les performances de cet arbre. Du coup on peut utiliser cette méthode pour évaluer la performance de la forêt aléatoire et ajuster les hyperparamètres.\n\nLe code ci-dessous montre comment calculer l'erreur OOB pour un modèle de forêt aléatoire. Il permet en particulier de sélectionner la profondeur de l'arbre dans la forêt aléatoire.\nLe modèle de forêt aléatoire est entraîné avec une profondeur d'arbre de 10, 20 et 30. L'erreur OOB est calculée pour chaque modèle. Le modèle avec la plus petite erreur OOB est sélectionné.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n## Training Random Forest\n\n\n\n\ndepths = [10, 20, 30]\noob_errors = []\nmodels = []\nbest_oob_error = float('inf')\nbest_model = None\ni = 0\nfor depth in depths:\n    print(i)\n    model = RandomForestClassifier(max_depth=depth, oob_score=True, random_state=42,\n                                   n_estimators=100,  \n                                   warm_start=True  # This allows us to add more estimators later if needed\n                                  )\n    model.fit(X_train, Y_train)\n    oob_error = 1 - model.oob_score_\n    oob_errors.append(oob_error)\n    models.append(model)\n    if oob_error < best_oob_error:\n        best_oob_error = oob_error\n        best_model = model\n    i = i+1\n    print(\"Done\")\n\n# Print OOB errors for each model\nfor depth, error in zip(depths, oob_errors):\n    print(f\"Depth: {depth}, OOB Error: {error}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\nDone\n1\nDone\n2\nDone\nDepth: 10, OOB Error: 0.21680518234371537\nDepth: 20, OOB Error: 0.05706860237215716\nDepth: 30, OOB Error: 0.038628770096964526\n```\n:::\n:::\n\n\n### Accuracy et matrice de confusion\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Compute the accuracy of the best random forest model\npredictions = best_model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy of the Best Random Forest: {:.3f}\".format(accuracy))\n\n# Display the confusion matrix\nconf_matrix = confusion_matrix(Y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the Best Random Forest: 0.962\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-2.png){width=770 height=597}\n:::\n:::\n\n\nNous avons un accuracy de 0.962. C'est très bien si on compare avec le modèle de régression logistique qui a un accuracy de 0.714. Nous pouvons voir la matrice de confusion.\n\n# Xgboost\n\nEssayons tout d'abord par expliquer ce qu'est le Xgboost. Xgboost signifie Extreme Gradient Boosting.\nIl combine des weaks models afin de produire des prédictions plus précises.  Il est très rapide et performant. \n\nAvant de passer à la modélisation, il faut transformer les données en un format spécifique à Xgboost.  En effet, le package xgboost ne gère pas les chaînes de caractères pour les étiquettes contrairement à tous les modèles entraînés précédemment, donc vous devez d'abord les encoder en tant qu'entiers. De plus, il encode automatiquement les étiquettes en tant que 0, 1, 2, etc. Cela signifie que si vous avez des étiquettes de classe 1, 2, 3, 4, 5, 6, 7, vous devez les encoder en tant que 0, 1, 2, 3, 4, 5, 6. Il faut le faire pour les labels(Y_train et Y_test) d'entraînement et de test.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# encode string class values as integers\n\n\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(Y)\nlabel_encoded_y_train = label_encoder.transform(Y_train)\nlabel_encoded_y_test = label_encoder.transform(Y_test)\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# Vérifiez les étiquettes uniques dans les ensembles d'entraînement et de test\nunique_train_labels = np.unique(label_encoded_y_train)\nunique_test_labels = np.unique(label_encoded_y_test)\n\nprint(\"Unique labels in training set:\", unique_train_labels)\nprint(\"Unique labels in test set:\", unique_test_labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique labels in training set: [0 1 2 3 4 5 6]\nUnique labels in test set: [0 1 2 3 4 5 6]\n```\n:::\n:::\n\n\nNous avons utilisé un modèle de classification Xgboost avec les paramètres suivants:\n\n- objective='multi:softprob' : Spécifie la fonction objectif de l'entraînement. Ici, 'multi:softprob' est utilisé pour les problèmes de classification multiclasse et retournera une matrice de probabilité estimée pour chaque classe, ce qui est nécessaire pour calculer des scores comme le log-loss.\n\n- seed='12345' : Fournit une graine pour le générateur de nombres aléatoires. Cela garantit que les résultats sont reproductibles. Toutefois, il semble y avoir une petite confusion ici, car la graine devrait être un entier (seed=12345), pas une chaîne de caractères (seed='12345').\n\n- gamma=0 : Paramètre de régularisation qui minimise la complexité du modèle et aide à prévenir le surajustement. La valeur de 0 indique qu'il n'y a pas de régularisation supplémentaire.\n\n- learning_rate=0.05 : C'est le taux d'apprentissage, également connu sous le nom d'eta. Cela contribue à rendre le processus d'apprentissage plus robuste en empêchant les poids de s'ajuster trop fortement à chaque itération. Une valeur plus faible peut nécessiter plus d'arbres pour apprendre les mêmes relations, mais peut améliorer la performance finale du modèle.\n\n- max_depth=5 : Détermine la profondeur maximale de chaque arbre. C'est un autre paramètre qui aide à prévenir le surajustement. Plus la profondeur est grande, plus le modèle est complexe.\n\n- n_estimators=200 : Le nombre d'arbres à construire. Plus il y a d'arbres, plus le modèle peut être précis, mais cela augmente aussi le temps de calcul et le risque de surajustement.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nclf_xgb = XGBClassifier(objective='multi:softprob', seed='12345',\n                       gamma=0, learning_rate=0.05, max_depth=5, n_estimators=200,num_class=7)\nclf_xgb.fit(X_train, label_encoded_y_train)\n\naccuracy_xgb = accuracy_score(label_encoder.transform(Y_test), clf_xgb.predict(X_test))\n\nprint(\"Accuracy of XGBOOST :\",\"%.3f\" % accuracy_xgb)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of XGBOOST : 0.786\n```\n:::\n:::\n\n\nIci on a un accuracy de 0.0.786. C'est mieux que le modèle de régression logistique mais moins bien que le modèle de forêt aléatoire. Nous pouvons voir la matrice de confusion.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# Display the confusion matrix\npredictions = clf_xgb.predict(X_test)\nconf_matrix = confusion_matrix(label_encoded_y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-1.png){width=770 height=597}\n:::\n:::\n\n\n### Courbe ROC\n\nLa courbe ROC est un graphique qui montre la performance d'un modèle de classification à différents seuils de classification. Elle trace le taux de vrais positifs (TPR) en fonction du taux de faux positifs (FPR) à différents seuils de classification. Le TPR est également connu sous le nom de rappel et le FPR est égal à 1 - spécificité.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\ndef compute_roc_auc(models, X, Y_test):\n    \"\"\"\n    Compute ROC AUC for a list of models.\n    \n    Args:\n    models (dict): A dictionary of models with their names as keys.\n    X_test (array-like): Test features.\n    Y_test (array-like): True labels for the test set.\n    \n    Returns:\n    dict: A dictionary containing FPR, TPR, and ROC AUC for each model.\n    \"\"\"\n    Y_classes = np.unique(Y_test)\n    Y_test_binarized = label_binarize(Y_test, classes=Y_classes)\n    n_classes = len(Y_classes)\n    \n    results = {}\n\n    for model_name, model in models.items():\n        if model_name == 'Logistic':\n            model = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n            model.fit(X_train,Y_train)\n            score = model.predict_proba(X_test)\n        else:\n          \n            score = model.predict_proba(X_test)\n        \n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        # Compute ROC for each class\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(Y_test_binarized[:, i], score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_binarized.ravel(), score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        results[model_name] = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc, 'score':score}\n\n    return results\n\nmodels = {\n    'Logistic': clf_l1l2_LR,\n      # Assurez-vous que clf_svm est entraîné sur des données mises à l'échelle si nécessaire\n    'Random Forests': best_model,\n    'XGBOOST': clf_xgb,\n    \n}\n\nroc_results = compute_roc_auc(models, X_test, Y_test)\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k--')  # Ligne diagonale\n\nfor model_name, metrics in roc_results.items():\n    plt.plot(metrics['fpr']['micro'], metrics['tpr']['micro'], label=f'{model_name} Micro (area = {metrics[\"roc_auc\"][\"micro\"]:.2f})')\n\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title('Micro-Average Receiver Operating Characteristic')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-34-output-1.png){width=514 height=523}\n:::\n:::\n\n\nLa courbe ROC montre que le modèle de forêt aléatoire est le meilleur modèle. Il a la plus grande surface sous la courbe (AUC). Le modèle Xgboost est le deuxième meilleur modèle. Le modèle de régression logistique est le moins bon modèle.\n\n# Bonus\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_auc_score\n\n# Affichage des AUC micro-averaged\nprint(\"Area under Roc Curve (micro-average) for:\\n\")\nfor model_name in models.keys():\n    print(f\"- {model_name}: {roc_results[model_name]['roc_auc']['micro']:.3f}\")\n\n# Calcul et affichage des AUC one-vs-one macro-averaged\nprint(\"\\nArea under Roc Curve (one-vs-one macro-average) for:\\n\")\nfor model_name, model in models.items():\n    \n    score = roc_results[model_name]['score']\n    auc_ovo_macro = roc_auc_score(Y_test, score, multi_class=\"ovo\", average=\"macro\")\n    print(f\"- {model_name}: {auc_ovo_macro:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under Roc Curve (micro-average) for:\n\n- Logistic: 0.959\n- Random Forests: 0.999\n- XGBOOST: 0.977\n\nArea under Roc Curve (one-vs-one macro-average) for:\n\n- Logistic: 0.917\n- Random Forests: 0.998\n- XGBOOST: 0.966\n```\n:::\n:::\n\n\n# Conclusion \n\nPour aborder cet exercice de manière efficace, je vous recommande fortement de vous concentrer sur la préparation préalable de vos propres fonctions, plutôt que de vous reposer excessivement sur des générateurs de texte automatiques. La création de vos fonctions en amont vous permettra de gagner un temps précieux. N'oubliez pas que les fonctions et les pipelines que vous avez déjà utilisés ont été préparés avant même de commencer le devoir ; votre tâche consiste essentiellement à les adapter aux besoins spécifiques de la situation.\n\nPar ailleurs, l'utilisation judicieuse de ChatGPT peut s'avérer très bénéfique, notamment dans des tâches telles que la génération de boxplots ou l'implémentation de l'algorithme pour calculer l'erreur out-of-bag (Out-of-bag error). ChatGPT peut fournir des conseils, des exemples de code ou des explications qui peuvent faciliter le processus de développement. Cependant, gardez à l'esprit que l'outil doit être utilisé comme un complément à votre propre travail de programmation et de réflexion, et non comme une solution complète.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}