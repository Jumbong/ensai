[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "",
    "text": "somment mondial IA"
  },
  {
    "objectID": "posts/welcome/index.html#déroulement-du-sommet",
    "href": "posts/welcome/index.html#déroulement-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Déroulement du sommet",
    "text": "Déroulement du sommet\nCe sommet, qui s’est tenu sur deux jours (1er et 2 novembre), a réuni des sommités en IA, des entrepreneurs, des chercheurs et des représentants politiques de haut niveau. Parmi eux se trouvaient Antonio Guterres, Secrétaire général de l’ONU; Kamala Harris, vice-présidente des États-Unis; Giorgia Meloni, Première ministre italienne et seule dirigeante du G7 présente; et Elon Musk, CEO de X (ex Twitter)."
  },
  {
    "objectID": "posts/welcome/index.html#résultats-du-sommet",
    "href": "posts/welcome/index.html#résultats-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Résultats du sommet",
    "text": "Résultats du sommet\n\nUn accord sur la sécurité des systèmes d’IA a été signé par les grandes puissances mondiales telles que la Chine, les États-Unis et l’Union européenne, soulignant une responsabilité partagée dans la régulation de l’IA.\nUn premier rapport sur l’IA a été confié à Yoshua Bengio, scientifique canadien récompensé par le prix Turing, chargé d’évaluer les risques et avantages de l’IA.\nAntonio Guterres a plaidé pour une approche universelle : il a exhorté à une gestion de l’IA collective et éthique, alignée avec les principes fondamentaux et les droits de l’homme énoncés dans la charte des Nations Unies.\nElon Musk, lors d’un échange avec Rishi Sunak, a mis en lumière les risques de l’IA, même s’il envisage une “ère d’abondance”, alertant sur le fait que les robots humanoïdes pourraient un jour dépasser les capacités humaines.\n\nL’IA peut certes faciliter le travail et accélérer la résolution des problèmes, mais elle soulève également des inquiétudes. Si un robot peut penser et agir à la place d’une équipe de quatre personnes, que deviendront ces travailleurs ? C’est l’une des questions cruciales que soulève l’essor de l’IA."
  },
  {
    "objectID": "posts/welcome/index.html#prochaines-étapes",
    "href": "posts/welcome/index.html#prochaines-étapes",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Prochaines étapes",
    "text": "Prochaines étapes\nLe prochain sommet se déroulera à Paris, se concentrant sur des problèmes immédiats tels que la désinformation et l’impact sur l’emploi, avec une date encore à déterminer."
  },
  {
    "objectID": "posts/TBATS/index.html",
    "href": "posts/TBATS/index.html",
    "title": "TBATS : N’a pas de contraintes de saisonnalité",
    "section": "",
    "text": "Dans notre quête pour décrypter les tendances de consommation énergétique en France, tout au long de notre projet de série temporelle, nous avons été confrontés à un défi de taille : prédire efficacement la consommation future d’énergie dans un contexte de double saisonnalité. En explorant au-delà des méthodes classiques telles que les modèles SARIMA et ARIMA, nous avons rencontré des obstacles liés à l’estimation des variations saisonnières hebdomadaires et annuelles. Ces complexités nous ont conduits vers une solution innovante : le modèle TBATS.\nLe modèle TBATS, une avancée significative dans l’analyse des séries temporelles, embrasse la multifacette de la saisonnalité grâce à une élégante synthèse de fonctions trigonométriques. Contrairement aux modèles ARIMA et SARIMA qui peinaient à capturer les subtilités de nos données, TBATS a brillé par sa capacité à intégrer des périodicités multiples.\nDans cet article, nous plongeons au cœur de la méthodologie mathématique du modèle TBATS avant de le mettre en application sur nos données de consommation énergétique. Rejoignez-nous pour une exploration de la modélisation prédictive avec Python et la librairie tbats, et découvrez comment nous avons illuminé le chemin vers des prévisions énergétiques plus précises.\n\nPrésentation du modèle TBATS\nLe modèle TBATS ou encore (Trigonometric, Box-Cox transform, ARMA errors, Trend and Seasonal components)(De Livera, Hyndman, and Snyder 2011) a pour paramètres TBATS(\\(\\omega\\), {p,q}, \\(\\phi\\), \\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\)) où :\n\n\\(\\omega\\) correspond à la transformation de Box-Cox.\n{p,q} correspond aux paramètres de l’ARMA.\n\\(\\phi\\) correspond à la tendance.\n\\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\) correspond aux paramètres de saisonnalité.\n\\(k_1,...,k_n\\) correspond aux nombres de Fourier de séries pairs.\n\nLe model s’écrit de la manière suivante :\n\\[\ny_t(\\omega) = l_{t-1} + \\phi b_{t-1} + \\sum_{i=1}^{T} s_{t-1}(i) + \\alpha d_t\n\\] \\[\nb_t = b_{t-1} + \\beta d_t\n\\] \\[\ns_t(i) = \\sum_{j=1}^{k_i} s_{j,t}(i)\n\\] \\[\ns_{j,t}(i) = s_{j,t-1}(i) \\cos \\lambda_j(i) + s^{*}_{j,t-1}(i) \\sin \\lambda_j(i) + \\gamma^{(i)}_1 d_t\n\\] \\[\ns^{*}_{j,t}(i) = -s_{j,t-1}(i) \\sin \\lambda_j(i) + s^{*}_{j,t-1}(i) \\cos \\lambda_j(i) + \\gamma^{(i)}_2 d_t\n\\] \\[\n\\lambda_j(i) = \\frac{2\\pi j}{m}\n\\]\noù :\n\n\\(i = 1, \\ldots, T\\)\n\\(d_t\\) est un processus ARMA ( p, q ),\n\\(\\alpha\\), \\(\\beta\\), \\(\\gamma_1\\) et \\(\\gamma_2\\) sont des paramètres de lissage,\n\\(l_0\\) est le niveau initial,\n\\(b_0\\) est la valeur de la pente.\n\nLes erreurs de prévisions seront modélisées par les indicateurs de qualité suivants :\n\nMean Squared Error (MSE) : \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\\)\nRoot Mean Squared Error (RMSE) : \\(RMSE = \\sqrt{MSE}\\)\nMean Absolute Error (MAE) : \\(MAE = \\frac{1}{n} \\sum_{i=1}^{n} |e_i|\\)\nMean Absolute Percentage Error (MAPE) : \\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{e_i}{y_i} \\right| \\cdot 100\\%\\)\nMean Error (ME) : \\(ME = \\frac{1}{n} \\sum_{i=1}^{n} e_i\\)\nMean Percentage Error (MPE) : $MPE = _{i=1}^{n} % $\nMean Absolute Scaled Error (MASE): \\(MASE = \\frac{n}{n-m} \\frac{\\sum_{i=1}^{n} |e_i|}{\\sum_{i=m+1}^{n} |y_i - y_{i-m}|}\\)\nAutocorrelation function of errors at lag 1 (ACF1) : \\(ACF1 = \\frac{\\sum_{i=1}^{n-1} (e_i - ME) \\cdot (e_{i+1} - ME)}{\\sum_{i=1}^{n} (e_i - ME)^2}\\)\n\noù : \\(e_t\\) is the error \\(e_t = y_t -{ y_t}^*\\)\n\\(y_t\\) est la valeur actuelle, \\(y_t^*\\) est la valeur prédicte , m est la période de saisonalité.\nDans l’analyse des prédictions de modèles statistiques, la compréhension des erreurs est cruciale. En comparant le Mean Error (ME) et le Mean Absolute Error (MAE), nous pouvons déterminer si les valeurs prédites sont systématiquement plus élevées ou plus faibles que les valeurs réelles, indiquant un biais directionnel. De même, la comparaison entre le Mean Percentage Error (MPE) et le Mean Absolute Percentage Error (MAPE) révèle l’ampleur de ce biais en termes de pourcentage.\nMais ce n’est pas tout. L’analyse du Mean Squared Error (MSE) peut révéler la présence de valeurs aberrantes ou d’erreurs exceptionnellement élevées dans les prédictions. Ces erreurs extrêmes se manifestent souvent par un écart significatif entre le MAE et le Root Mean Squared Error (RMSE), où le RMSE, en donnant plus de poids aux grandes erreurs, met en lumière les défauts plus subtils de notre modèle prédictif.\n\n\nPrésentation des données\nDans notre étude appronfondie sur la demande énergétique en france, nous avons plongé au coeur des données de consommation, mésurées en mégawattts, révelatrices des tendances de consommation du pays. Afin d’affiner notre analyse nous avons présenter les données sous forme de séries temporelles journalières. Cette approche nous permettra d’aborder plus aisement l’analyse.\n\n\nLe dendogramme de la série temporelle\nNous avons mis en évidence des motifs saisonniers clés à travers la représentation d’un dendogramme. Cet outil permettra de vérifier que 365 et 7 sont des saisonnalités.\n\nfrom scipy import signal\n\nfrequencies, power_spectral_density = signal.periodogram(df_con_daily['consommation'].values)\n\n\n# Tracé du périodogramme\nplt.figure(figsize=(5, 3))\nplt.plot(frequencies, power_spectral_density)\nplt.title('Périodogramme')\nplt.xlabel('Fréquence')\nplt.ylabel('Densité spectrale de puissance')\n\n# Ajout de la ligne verticale à 1/12\nplt.axvline(x=1/365, color='red', linestyle='--')\nplt.axvline(x=1/7, color='yellow', linestyle='--')\n\nplt.show()\n\n\n\n\ndendrogramme\n\n\nNous observons que les cylces de 365 et 7 se détachent nettement, mettant en évidence une saisonnalité annuelle est hebdommadaire.\nEn plus de l’analyse dendrogramme, une autre méthode efficace pour mettre en lumière les saisonnalités dans nos données énergétiques consiste à décomposer la série temporelle. Cette approche permet d’isoler et d’examiner les composantes saisonnières annuelles et hebdomadaires de manière distincte. L’analyse des fonctions d’autocorrélation et d’autocorrélation partielle offre également des insights précieux. Cependant, pour rester concentrés sur les aspects les plus pertinents de notre étude, nous choisirons de ne pas approfondir cette méthode dans ce cadre. Les paramètres du model TBATS(False,{0,0},0.85,{&lt;7,365&gt;}) correspondent au mieux aux données.\nDans notre démarche analytique, une étape cruciale a été la segmentation de nos données en ensembles d’entraînement et de test. Cette division stratégique est essentielle pour affiner et évaluer la précision de nos modèles prédictifs. La variable date_cutoff joue un rôle clé dans ce processus, définissant le point de séparation entre les périodes d’entraînement et de test.\n\nimport pandas as pd\nfrom sktime.datatypes import check_raise\nfrom datetime import datetime\n\ny = df_con_daily['consommation']\ny.index = pd.to_datetime(y.index)\n\n\n\ndate_cutoff = pd.Timestamp('2019-12-31')\n\n# Ensuite, effectuez la comparaison\ny_train = df_con_daily[df_con_daily.index &lt; date_cutoff]['consommation']\ny_test = df_con_daily[df_con_daily.index &gt;= date_cutoff]['consommation']\ny_train.index = pd.to_datetime(y_train.index)\n\nEnsuite nous avons instancier le modèle TBATS avec les différentes saisonnalités 7 et 365. n_jobs facilite le temps de computation. Pour plus de détails voir [TBATS](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.tbats.TBATS.html). Assurez vous que vos index ou la colonne date soit de type pandas.core.indexes.datetimes.DatetimeIndex. Pour cela vous pouvez vous servir de chatgpt.\n\nforecaster = TBATS(sp = [7, 365], n_jobs = 1)\nmodel = forecaster.fit(y_train)\n\n# Prediction\nfh = len(y_test)\ny_pred = model.forecast(fh)\n\nfig, ax = plt.subplots(figsize = (15,5))  \ny.plot(title = 'TBATS dayly energie consumption', xlabel = '', ax = ax)\ny_pred.plot(ax = ax)\nax.legend(['Actual Values', 'Forecast'])\nplt.show()\n\nUne fonction pour les différentes métriques pour évaluer la qualité du modèle est donnée par :\n\n\n\nprevision\n\n\n\ndef print_metrics(y_true, y_pred, model_name):\n    mae_ = mean_absolute_error(y_true, y_pred)\n    rmse_ = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape_ = mean_absolute_percentage_error(y_true, y_pred)\n    smape_ = mean_absolute_percentage_error(y_true, y_pred, symmetric = True)\n    \n    dict_ = {'MAE': mae_, 'RMSE': rmse_,\n             'MAPE': mape_, 'SMAPE': smape_ }\n    \n    df = pd.DataFrame(dict_, index = [model_name])\n    return(df.round(decimals = 2)) \n\nPour avoir les résultats des performances du modèle, il faut exécuter cette fonction.\n\nprint_metrics(y_test, y_pred, 'TBATS Forecaster')\n\n\n\nRésultats de Prévision avec TBATS\nNous avons essayer de mettre les résultats dans un tableau :\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate \n\ntable = [[\"TBATS Forecaster\",\"290.94\",\"412.76\",\"0.07\",\"0.06\"]]\n\nMarkdown((tabulate(\n    table,\n    headers = [\"\",\"MAE\",\"RMSE\",\"MAPE\",\"SMAPE\"]\n)\n))\n\n\nIndicateurs de performance\n\n\n\nMAE\nRMSE\nMAPE\nSMAPE\n\n\n\n\nTBATS Forecaster\n290.94\n412.76\n0.07\n0.06\n\n\n\n\n\n\n\nConclusion\n\nLe Mean Absolute Error (MAE) : Il montre qu’en moyenne, les prévisions s’écartent de 290.94 unités des valeurs réelles, nous donnant une idée de l’erreur moyenne absolue.\nLe Root Mean Squared Error (RMSE) : Sa valeur de 493.17, n’étant pas très élévée comparé au MAE, nous pouvons conclure que les erreurs de prévision sont relativement faibles.\nLe Mean Absolute Percentage Error (MAPE) et le Symmetric Mean Absolute Percentage Error (SMAPE) : Ils indiquent une erreur moyenne de prédiction de 0.07%, ce qui est considéré comme relativement précis dans notre contexte.\n\nAinsi le modèle TBATS est un modèle qui permet de faire des prévisions acceptables sur les données possèdant des saisonnalités multiples.\n\nReferences\n\n\nBROŻYNA, Jacek, Grzegorz Mentel, Beata Szetela, and Wadim Strielkowski. 2018. “MULTI-SEASONALITY IN THE TBATS MODEL USING DEMAND FOR ELECTRIC ENERGY AS a CASE STUDY.” Economic Computation & Economic Cybernetics Studies & Research 52 (1). https://www.researchgate.net/profile/Grzegorz-Mentel/publication/323868510_Multi-Seasonality_in_the_TBATS_Model_Using_Demand_for_Electric_Energy_as_a_Case_Study/links/5ab0afafaca2721710fe20b8/Multi-Seasonality-in-the-TBATS-Model-Using-Demand-for-Electric-Energy-as-a-Case-Study.pdf.\n\n\nDe Livera, Alysha M., Rob J. Hyndman, and Ralph D. Snyder. 2011. “Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing.” Journal of the American Statistical Association 106 (496): 1513–27. https://doi.org/10.1198/jasa.2011.tm09771."
  },
  {
    "objectID": "posts/NotionCle/index.html",
    "href": "posts/NotionCle/index.html",
    "title": "Notions clés pour comprendre le réchauffement climatique",
    "section": "",
    "text": "Le réchauffement climatique est une notion d’actualité. Il est important de comprendre les mécanismes qui le sous-tendent, les mécanismes qui servent de base à sa compréhension. Connait-on vraiment l’atmosphère ? Nous avons souvent attendu parler de rayonnement, d’albedo, de gaz à effet de serre et de bilan radioactif. Savons-nous vraiment ce que cela signifie ?\nLa première partie de ce document consistera à présenter brièvement l’atmosphère et ces différentes couches. Ensuite, nous aborderons les notions de rayonnement, d’albedo, de gaz à effet de serre et de bilan radioactif.",
    "crumbs": [
      "About",
      "climat",
      "Notions clés pour comprendre le changement climatique"
    ]
  },
  {
    "objectID": "posts/NotionCle/index.html#les-couches-de-latmosphère-terrestre.",
    "href": "posts/NotionCle/index.html#les-couches-de-latmosphère-terrestre.",
    "title": "Notions clés pour comprendre le réchauffement climatique",
    "section": "Les couches de l’atmosphère terrestre.",
    "text": "Les couches de l’atmosphère terrestre.\nL’atmosphère est constitué de plusieurs couches, dont les plus importantes pour comprendre le réchauffement climatique sont la troposphère et la stratosphère.\n\nLa troposhère est la couche inférieure de l’atmosphère, la couche la plus proche du sol. Elle contient les nuages et la majorité de la vapeur d’eau de l’atmosphère. Elle constitue les 5/6 de la masse totale de l’atmosphère. C’est dans cette couche que se déroule la plupart des phénomènes météorologiques. La température diminue avec l’altitude dans cette couche.\nLa stratosphère compose le niveau supérieur, jusqu’à 50 km d’altitude et héberge la couche d’ozone. La température augmente avec l’altitude dans cette couche, absorbant une grande partie du rayonnement solaire.\n\nJe n’insisterai pas sur les autres couches qui sont la mésosphère, la thermosphère et l’exosphère.",
    "crumbs": [
      "About",
      "climat",
      "Notions clés pour comprendre le changement climatique"
    ]
  },
  {
    "objectID": "posts/MarketRiskGen/index.html",
    "href": "posts/MarketRiskGen/index.html",
    "title": "Market risk generalities",
    "section": "",
    "text": "A market risk is a risk of loss an investment value due to a variation of the market factors. The market factors are the parameters that influence the price of a financial instrument. The most common market factors are:\n\nInterest rates\nEquity prices\nForeign exchange rates\nCommodity prices\n\nThe interest rate risk is the risk that the value of a financial instrument(ex:bond) will decline due to an increase of the interest rates.\nC’est donc une offre dont la valeur va baisser si les taux augmentent. C’est le cas des obligations.\nExemple : A zero coupon bond with a face value of 1000€ and a maturity of 10 years. The interest rate is 5%. The value of the bond is p = \\frac{1000}{(1+0.05)^{10}} = 613.\nIf the interest rate increases to 6%, the value of the bond is p = \\frac{1000}{(1+0.06)^{10}} = 558\nUne autre chose qu’il faut remarquer est que si les taux d’intérêts augmentent, la valeur des obligations diminue. Qu’elle est l’interprétation économique de cela ?\n\n\nSi les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue.\n\n\n\nUne banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "href": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "title": "Market risk generalities",
    "section": "",
    "text": "Si les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#curve-risk",
    "href": "posts/MarketRiskGen/index.html#curve-risk",
    "title": "Market risk generalities",
    "section": "",
    "text": "Une banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "href": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "title": "Market risk generalities",
    "section": "Sensitivity-based methods",
    "text": "Sensitivity-based methods\n\nDuration\nLa duration exprime comment un porte-feuille est sensible aux variations des taux d’intérêts.\nExemple: Un porte-feuille avec une duration de 5 ans signifie que si les taux d’intérêts augmentent de 1%, la valeur du porte-feuille va diminuer de 5%.\nDonc une duration élevée signifie un risque élevé.\nMaintenant si on a deux obligations(bonds): Le premier de nominal 1000, maturité 10 ans, de prix 900 et de coupon 5%. Le deuxième de nominal 1000, maturité 10 ans, de prix 1100 et de coupon 7%.\nSur quels bond va t-on investir ?\nPour répondre à cette question, nous avons besoin d’un outil: the yield to maturity.\nThe yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre. Mathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\noù C_i = nominal* coupon i!=N C_N = nominal + coupon*nominal\nIl faut deux hypothèses pour calculer le yield to maturity: - Le coupon est réinvesti au taux d’intérêt du marché. - Le titre est détenu jusqu’à la maturité. On a défini le yield to maturity, on peut définir la duration.\nLa duration est la sensibilité de la valeur d’un titre par rapport au yield to maturity. Mathématiquement,\nD = -\\frac{1}{P}\\frac{dP}{dy}.\nConséquence de la duration:\n\nSi le taux d’intérêt augmente diminue, nous allons investir dans un porte-feuille avec une duration élevée.\nSoit une obligation dont le prix est de 90 qui paye des coupons annnuels de 5% de nominal 1000 et de maturité 5 ans.\n\nDeterminons son yield to maturity.\nOn sait que le yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre.\nMathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\nPour déterminer la duration, on a besoin de calculer la dérivée de P par rapport à y.\non sait que la dérivée peut être calculée comme une limite de la formule suivante:\n\\frac{dP}{dy} = \\lim_{\\Delta y \\to 0} \\frac{P(y+\\Delta y) - P(y- \\Delta y)}{2 \\Delta y}\nAinsi si on a déterminé le yield to maturity, on peut déterminer la duration. Il faut pour cela, calculer le prix de l’obligation pour deux valeurs de y: y+\\Delta et y-\\Delta y. Prenons \\Delta = 0.01 Pour y1 = y+\\Delta P(y+\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y+\\Delta)^i}\npour y2 = y-\\Delta P(y-\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y-\\Delta)^i}\nFinalement, D = -\\frac{1}{P}\\frac{P(y+\\Delta) - P(y- \\Delta y)}{2 \\Delta y}\nIl y’a une hypothèse forte derrière la duration: Une courbe de déplacement parallèle. C’est à dire que si les taux d’intérêts augmentent de 1%, la courbe des taux d’intérêts va se déplacer parallèlement vers le haut."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#the-greeks",
    "href": "posts/MarketRiskGen/index.html#the-greeks",
    "title": "Market risk generalities",
    "section": "The Greeks",
    "text": "The Greeks\nLa valeur d’un porte-feuille dépend de plusieurs facteurs, qui peuvent interagir entre eux. Pour mesurer l’impact de ces facteurs sur la valeur du porte-feuille, on utilise les lettres grecques : delta, gamma, vega, theta, rho.\n\nDelta\nLe delta exprime le changement entre un changement de la valeur du porte-feuille et un changement de la valeur du sous-jacent. Exemple : Soit C_0 le prix d’une option de maturité T et de strike k. Soit S_t le prix du sous-jacent à la date t.\npayoff de l’option : max(S_T - k, 0), le prix de l’option est le payoff actualisé : C_0 = \\frac{max(S_T - k, 0)}{(1+r)^T}\nLes facteurs de risques de cette option sont : le prix du sous-jacent, le taux d’intérêt et la volatilité.\nLe delta de cette option est : \\frac{\\partial C_0}{\\partial S_t}\nComment se couvrir de la variation du prix du sous-jacent ?\nIl faut construire un porte-feuille qui a un delta égal à 0. C’est à dire que si le prix du sous-jacent augmente, la valeur du porte-feuille ne change pas.\non peut par exemple considéré un porte-feuille de prix P^{'} = C_0 + \\alpha S_t. La nouvelle option est de sensibilité nulle par rapport au prix du sous-jacent si \\alpha = -\\frac{\\partial C_0}{\\partial S_t}. Ceci revient à dire que pour se couvrir du risque de la variation de prix du sous-jacent, il faut vendre à decouvert c’est-à-dire short selling \\frac{\\partial C_0}{\\partial S_t} unités du sous-jacent. Le delta d’une action est 1.\nUn portefeuille avec un delta de 0 est option risque neutre. Il est construit pour (hedging purposes) se couvrir du risque de la variation du prix du sous-jacent.\n\n\nGamma\nLe gamma est la dérivée seconde du prix de l’option par rapport au prix du sous-jacent. Il mesure la sensibilité du delta par rapport au prix du sous-jacent. Son implication dans le cas d’un portefeuille delta-neutre est qu’il a besoin d’être réajusté régulièrement si son gamma est élevé. Le gamma est donc de mesurer le degré d’exposition au risque qu’une position couverte(hedged position) dévellopera si la couverture(hedge) n’est pas réajustée.\n\n\nVega\nLe vega mesure le changement du prix de l’option par rapport à la volatilité du sous-jacent. Il mesure la sensibilité du prix de l’option par rapport à la volatilité du sous-jacent.\nUn portefeuille avec un vega de 0 est vega-neutre. Il est construit pour se couvrir du risque de la variation de la volatilité du sous-jacent. C’est-à-dire qu’il est insensitive à la variation de la volatilité du sous-jacent.\n\n\nTheta and Rho\nLe Theta et le rho déterminent respectivement le taux de changement de la valeur d’un portefeuille par rapport au temps to maturity et par rapport au taux d’intérêt.\nEn practique, les sensibilités d’un portefeuille sont calculées intensivement, intra-day(à l’intérieur de la journée) et quotidiennement pour chaque traded product. Ils sont utilisées par les traders afin de gérer leur risque, their position(hedging) et par les managers pour expliquer leur perte et leur profit(P&L). Cependant, elles souffrent de plusieurs limitations: elles ne peuvent pas être comparées entre plusieurs activités pour conclure qu’une activité est plus risquée qu’une autre."
  },
  {
    "objectID": "posts/machineLearning3A/index.html",
    "href": "posts/machineLearning3A/index.html",
    "title": "Examen de machine learning",
    "section": "",
    "text": "Il est possible que ce soit Da Veiga qui assure vos cours, mais je vais vous offrir un aperçu de ce à quoi pourrait ressembler votre examen. C’est important, car nous avons tendance à sous-estimer ce type d’activité, surtout lorsqu’il autorise l’utilisation de générateurs de texte tels que ChatGPT. Cette année, peu d’étudiants ont achevé le projet, la charge de données étant longue et fastidieuse. Je vous conseille de vous y prendre en avance. Préparez des fonctions exécutant certaines tâches spécifiques, que je vous expliquerai progressivement."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#box-plot",
    "href": "posts/machineLearning3A/index.html#box-plot",
    "title": "Examen de machine learning",
    "section": "Box plot",
    "text": "Box plot\nLe graphique ci-dessous montre la distribution de chaque variable en fonction de la variable cible.\n\n# box plot\n# Transformed Cover_Type to categorical\nY_sampled = Y_sampled.astype('category')\n\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterprétation: Nous allons nous concentrer sur la variable Elevation.\nOn peut voir que la variable Elevation est très discriminante.\nLes types de couverture 1, 2, et 7 montrent des médianes relativement élevées pour l’élévation, avec 7 ayant la médiane la plus élevée, suivie par 1 et 2."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#elastic-net-regression",
    "href": "posts/machineLearning3A/index.html#elastic-net-regression",
    "title": "Examen de machine learning",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nC’est une méthode de machine learning qui combine la régression Ridge et Lasso. Elle est utilisée pour résoudre le problème de surraprentissage, la multicollinéarité et la sélection de variables.\nPassons à sa modélisation :\nNous avons utilisé ici un modèle de régression logistique avec une pénalité elasticnet. Nous avons utilisé une validation croisée pour trouver le meilleur paramètre de régularisation. Nous avons utilisé une pénalité elasticnet avec un ratio de 0.5. Nous avons utilisé un solver saga qui est adapté aux problèmes multiclasse. Nous avons utilisé une tolérance de 0.01. Nous avons utilisé un random state de 12345.\n\nclf_l1l2_LR = LogisticRegressionCV(penalty='elasticnet', l1_ratios=[0.5], \n                                   cv=5, multi_class=\"multinomial\", \n                                 solver=\"saga\",tol=0.01, random_state=12345)\n\nmodel = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\naccuracy_LR = accuracy_score(Y_test, prediction)\n\nprint(\"Accuracy of Logistic Regression :\",\"%.3f\" % accuracy_LR)\n\nAccuracy of Logistic Regression : 0.714\n\n\nJ’ai un accuracy de 0.714. Ce n’est pas mal. Nous pouvons voir la matrice de confusion.\n\n# Confusion matrix\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(Y_test, prediction)\n\n# Display the confusion matrix using Seaborn's heatmap\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\n\n\n\n\n\n\n\n\nLes valeurs sur la diagonale principale (de haut à gauche à bas à droite) représentent le nombre de prédictions correctes pour chaque classe. Par exemple, il y a 29724 prédictions correctes pour la classe 0, 44616 pour la classe 1, et ainsi de suite.\nLes valeurs hors de la diagonale indiquent les erreurs de classification. Par exemple, 11947 instances de la classe 0 ont été incorrectement prédites comme appartenant à la classe 1.\nLa classe 0 a le plus grand nombre de faux positifs, c’est-à-dire que de nombreuses instances d’autres classes ont été incorrectement prédites comme appartenant à la classe 0.\nLes classes avec le moins de prédictions incorrectes (et donc les plus sombres dans la visualisation) sont la classe 3 et la classe 6, avec respectivement 187 et 1993 prédictions correctes. Les cases avec un fond plus clair, en dehors de la diagonale, indiquent des erreurs moins fréquentes entre les classes spécifiques."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#random-forest",
    "href": "posts/machineLearning3A/index.html#random-forest",
    "title": "Examen de machine learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nOOB error (Out-of-bag error)\nOOB est une méthode de validation croisée pour les forêts aléatoires. Chaque arbre dans la forêt est construit à partir d’un échantillon bootstrap du jeu de données d’entraînement. Certaines observations sont laissées de côté et non utilisées dans la construction d’un arbre donné. Ces observations “hors sac” peuvent être utilisées pour évaluer les performances de cet arbre. Du coup on peut utiliser cette méthode pour évaluer la performance de la forêt aléatoire et ajuster les hyperparamètres.\nLe code ci-dessous montre comment calculer l’erreur OOB pour un modèle de forêt aléatoire. Il permet en particulier de sélectionner la profondeur de l’arbre dans la forêt aléatoire. Le modèle de forêt aléatoire est entraîné avec une profondeur d’arbre de 10, 20 et 30. L’erreur OOB est calculée pour chaque modèle. Le modèle avec la plus petite erreur OOB est sélectionné.\n\n## Training Random Forest\n\n\n\n\ndepths = [10, 20, 30]\noob_errors = []\nmodels = []\nbest_oob_error = float('inf')\nbest_model = None\ni = 0\nfor depth in depths:\n    print(i)\n    model = RandomForestClassifier(max_depth=depth, oob_score=True, random_state=42,\n                                   n_estimators=100,  \n                                   warm_start=True  # This allows us to add more estimators later if needed\n                                  )\n    model.fit(X_train, Y_train)\n    oob_error = 1 - model.oob_score_\n    oob_errors.append(oob_error)\n    models.append(model)\n    if oob_error &lt; best_oob_error:\n        best_oob_error = oob_error\n        best_model = model\n    i = i+1\n    print(\"Done\")\n\n# Print OOB errors for each model\nfor depth, error in zip(depths, oob_errors):\n    print(f\"Depth: {depth}, OOB Error: {error}\")\n\n0\nDone\n1\nDone\n2\nDone\nDepth: 10, OOB Error: 0.21680518234371537\nDepth: 20, OOB Error: 0.05706860237215716\nDepth: 30, OOB Error: 0.038628770096964526\n\n\n\n\nAccuracy et matrice de confusion\n\n# Compute the accuracy of the best random forest model\npredictions = best_model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy of the Best Random Forest: {:.3f}\".format(accuracy))\n\n# Display the confusion matrix\nconf_matrix = confusion_matrix(Y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\nAccuracy of the Best Random Forest: 0.962\n\n\n\n\n\n\n\n\n\nNous avons un accuracy de 0.962. C’est très bien si on compare avec le modèle de régression logistique qui a un accuracy de 0.714. Nous pouvons voir la matrice de confusion."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html",
    "href": "posts/InstrumentsFinanciers/index.html",
    "title": "Les intruments financiers à un bébé",
    "section": "",
    "text": "Les instruments financiers\nDans le cadre d’un cours d’asset pricing à l’ENSAI, je suis totalement perdu dans la compréhension des instruments financiers.\nCe décrochage m’empêche de suivre le cours et je suis donc totalement perdu sur les autres notions telles que pricing, hedging, etc.\nJ’ai donc décidé de retourner à la base c’est-à-dire de comprendre les instruments financiers.\nDans mes recherches sur chatgpt, j’ai trouvé les différents instruments financiers et leurs définitions."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "href": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les obligations.",
    "text": "Les obligations.\nJ’ai été vraiment déçu de la définition et de sa complexité. Une obligation est définie comme un titre de dette émis par une entreprise, un gouvernement, etc. Les détenteurs de ces obligations reçoivent des intérêts à intervalles réguliers et remboursent le montant total à l’échéance.\nQu’est-ce que cela signifie concrètement ?\nPrenons un exemple. J’ai faim, j’ai besoin d’argent pour manger à l’école mais je n’ai pas d’argent. Je vais donc voir un ami pour lui expliquer ma situation. Il me prête de l’argent de l’argent avec des intérêts sur une durée de cinq jours et il me demande de lui verser une partie de cette argent(qui correspond au produit entre le taux d’intérêt et la somme dont je lui est empruntée) chaque jour et de lui rembourser le montant total à la fin des cinq jours. Pour être sûr que je vais lui rembourser, il me demande un papier signé par moi et par lui. Ce papier est une obligation.\nIl existe deux types d’obligations: - Les obligations 0 coupon: Ce sont des prêts que l’on fait généralement au lycée. C’est-à-dire que l’on part voir un ami pour lui demander de l’argent avec un intérêt de x% pour cinq jours. On lui remboursera à la fin des cinq jours la somme empruntée plus les intérêts.\n\nLes obligations couponnées: Ce sont des obligations qui versent des intérêts à intervalles réguliers. C’est-à-dire que chaque mois nous verserons une partie de la somme empruntée(la somme empruntée multipliée par le taux d’intérêt) et à la fin de la période, nous rembourserons la somme empruntée plus les intérêts."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "href": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les actions.",
    "text": "Les actions.\nEn anglais les actions sont appelées les equities. Ce sont des titres de propriété d’une entreprise."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "href": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les produits dérivés.",
    "text": "Les produits dérivés.\nCe sont les intruments financiers qui pose actuellement beaucoup de difficultés.\nCe sont des intruments financiers dont la valeur dépend de la valeur d’un autre actif qui est appelé sous-jacent. On compte parmi les produits dérivés les options, les futures, les swaps, etc.\n\nLes options.\nUne option est un contrat qui donne à son détenteur le droit mais pas l’obligation d’acheter ou de vendre un actif à un prix fixé à l’avance et à une date donnée.\nIl y’a deux choses à retenir dans cette définition: le prix fixé à l’avance et la date donnée.\nOn signe ce type de contrat lorsque pense que le prix de l’actif va augmenter.\nPar exemple je pense que le prix de l’Iphone va augmenter dans les prochains jours. Je vois un ami qui vend des Iphones. Je lui donne un montant afin qu’il me reserve cet Iphone à un prix k fixé aujourd’hui. Si le prix de l’Iphone augmente, je pourrais l’acheter à un prix inférieur à celui du marché. Si le prix de l’Iphone diminue, je ne l’achèterai pas et je perdrai le montant que j’ai donné à mon ami.\n\n\nLes futures."
  },
  {
    "objectID": "posts/entretienCreditAgricole/index.html",
    "href": "posts/entretienCreditAgricole/index.html",
    "title": "Compte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.",
    "section": "",
    "text": "Sommet mondial IA\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\nListe des personnes présentes :\n\n\n\nIntervenants\n\n\nPrénom\n\n\nNom\n\n\n\n\nRecruteur\n\n\nprenom\n\n\nnom\n\n\n\n\nRecruteur\n\n\nMarie\n\n\nnom\n\n\n\n\nCandidat\n\n\nJunior\n\n\nJUMBONG\n\n\n\nDéroulement de l’Entretien :\nL’entretien du jeudi, 30 novembre pour le poste de data scientist - Auditeur a duré 1 heure et a eu pour objectif d’évaluer les compétences du candidat et la compréhension du poste de stage.\n\nParcours de l’Étudiant :\n\nLa discussion a débuté par un tour de table permettant à chacun de se présenter. Le candidat a notamment présenté son parcours, et mis en avant quelques compétences acquises durant ses différents projets universitaires.\n\nPrésentation de Crédit Agricole CACIB :\n\nCrédit Agricole CACIB est la banque de financement et d’investissement du groupe Crédit Agricole. Elle offre une gamme étendue de produits et services en banque d’investissement, financements structurés, banque commerciale, et marchés financiers.\n\nMissions du Stage :\n\nÉquipe Méthode Support - Data Scientist :\nUne première mission dans l’équipe méthode support où le candidat, s’il est retenu, travaillera sur la modélisation des risques de la banque. Il devra utiliser les compétences théoriques et pratiques acquises lors des formations précédentes, notamment en clustering et modélisation statistique.\nMission d’Inspection - Encadrement :\nLa deuxième mission en tant qu’inspecteur consistera à utiliser des techniques basées sur la donnée, l’analyse quantitative, et à proposer de nouvelles techniques afin de contribuer à la réussite de la mission. Les méthodologies utilisées, telles que la collecte de la donnée ,son exploration, sa préparation, son analyse ainsi que la diffusion des résultats, ont été précisément présentées.\n\n\nConclusion :\nLe processus de recrutement est actuellement en cours. Le candidat recevra une réponse finale d’ici quelques semaines, une fois que tous les entretiens seront terminés avec les autres candidats.\nIl lui a été demandé d’informer s’il est engagé dans d’autres processus de recrutement parallèle. Sa transparence est importante afin de permettre aux recruteurs d’adapter leurs réponses dans le cas échéant."
  },
  {
    "objectID": "posts/ClimateScenario/index.html",
    "href": "posts/ClimateScenario/index.html",
    "title": "Analyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.",
    "section": "",
    "text": "Risques physiques et climatiques\n\n\n\nObjectif :\nL’objectif est d’intégrer les données climatiques dans le cadre d’allocation d’actifs.\nIl s’agit plus simplement de comprendre comment intégrer les scénarions climatiques et les risques climatiques dans les projections financières futures.\nComprendre comment ceux-ci impactent les performances financières d’investissement.\nCet article de Luca Bongiorno et al. met à lumière les impacts potentiels du changement climatique sur les marchés financiers, en se concentrant sur leurs effets à long terme.\nElle utilise un outil de modélisation top-down dévéloppé par Ortec finance qui intégre la science climatique aux effets macroéconomiques et financiers pour examiner les impacts possibles de trois scénarios plausible et non extrèmes.\nL’article analyse l’impact sur le PIB, il en ressort une réduction de PIB dans les trois scénarios, avec un impact très sévère dans le cadre d’une transition ratée où les objectifs climatiques de l’accord de Paris ne sont pas atteints.\nL’impact sur les marchés financiers est également analysé. Il en ressort une baisse des rendements cumulés des actions mondiales\nGIEC : groupe d’experts intergouvernemental sur l’évolution du climat.\nCes risques climatiques commencent à se manifester à travers les changements de conditions climatiques(variations de température, régimes pluviométriques, etc.), l’élévation du niveau de la mer, les conditions météorologiques extrêmes, les incendies de forêt, les sécheresses, les inondations, les ouragans, les cyclones, les tempêtes de neige, les vagues de chaleur, les vagues de froid, etc. Ces événements ont des impacts négatifs significatifs et pertubent divers secteurs notamment l’agriculture, la sylviculture, l’énergie, les infrastructures.\nLes actions menant à limiter l’ampleur du changement climatique en reduisant les émissions de gaz à effet de serre, principalement en diminuant l’utilisation de l’énergie fossile, en utilisant l’énergie renouvelable.\nLe changement climatique est un risque systémique car il affecte tous les secteurs de l’économie de la même manière et en même temps.\nLes changements climatiques posent des risques pour l’économie et le système financier.\nLes risques climatiques sont classés en deux catégories : les risques physiques et les risques de transition.\nles risques physiques découlent des changements des conditions climatiques et des événements météorologiques extrêmes. Ils peuvent être aigus ou chroniques.\nles risques de transition résultent du passage à une économie à faible émission de carbone, à l’utilisation d’une technologie propre et à l’adoption de pratiques commerciales durables. Ils peuvent être liés à la politique, à la technologie, à la marché et aux risques légaux.\nBonjour, merci de me donner l’opportunité de m’exprimer aujourd’hui. Je m’appelle Junior Jumbong. Mon intérêt",
    "crumbs": [
      "About",
      "climat",
      "Scénarios climatiques"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Menu",
    "section": "",
    "text": "Courbe de taux et valorisation d’obligations\n\n\n\n\n\nLors de la valorisation d’une obligation, il est essentiel de considérer des éléments clés tels que le prix sale (dirty price) et le prix net (clean price), lesquels dépendent de facteurs variés incluant le taux d’intérêt, la maturité et le taux de coupon. Dans cette analyse, nous avons généré la courbe des taux spot en utilisant des obligations zéro coupon et évalué le prix d’une obligation sous l’hypothèse d’une courbe de taux déterministe. Par la suite, nous avons exploré un modèle simplifié de Hull et White, examiné les spécificités d’une obligation munie d’une clause de rappel et discuté des stratégies de couverture contre le risque de taux d’intérêt. \n\n\n\n\n\nJunior Jumbong\n\n\n\n\n\n\n\n\n\n\n\n\nModélisation de la value at risk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nNotions clés pour comprendre le réchauffement climatique\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nDifférence entre climat et météo\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMaitrise des données financières avec Python\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning QCM\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nLes intruments financiers à un bébé\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nTBATS : N’a pas de contraintes de saisonnalité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nUn entretien raté\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nLeadership : Pouvoir, autorité et légitimité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMarket risk generalities\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nIA : Eléments clés du premier sommet mondial\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ClimatMeteo/index.html",
    "href": "posts/ClimatMeteo/index.html",
    "title": "Différence entre climat et météo",
    "section": "",
    "text": "Il fera plus chaud demain, le temps était très pluvieurs hier, il fait froid aujourd’hui, les températures seront supérieures de trois degrés par rapport à la normale, etc. Ces phrases sont des exemples qui montrent que la météo est présente dans notre quotidien. Dès qu’on allume la télévision, la radio, nous voulons savoir comment nous habiller, si nous devons prendre un parapluie, etc.\nRéchauffement climatique, fonte des calottes polaires, hausse du niveau des mers, records de températures, diminution des nos gaz à effet de serre… Le climat aussi est un sujet d’actualité, il mobilise les scientifiques, les politiques, les citoyens.\nAu fond sait-on vraiment ce que c’est le climat et qu’elle est la différence entre le climat et la météo? Reprenons ces notions à la base.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#introduction",
    "href": "posts/ClimatMeteo/index.html#introduction",
    "title": "Différence entre climat et météo",
    "section": "",
    "text": "Il fera plus chaud demain, le temps était très pluvieurs hier, il fait froid aujourd’hui, les températures seront supérieures de trois degrés par rapport à la normale, etc. Ces phrases sont des exemples qui montrent que la météo est présente dans notre quotidien. Dès qu’on allume la télévision, la radio, nous voulons savoir comment nous habiller, si nous devons prendre un parapluie, etc.\nRéchauffement climatique, fonte des calottes polaires, hausse du niveau des mers, records de températures, diminution des nos gaz à effet de serre… Le climat aussi est un sujet d’actualité, il mobilise les scientifiques, les politiques, les citoyens.\nAu fond sait-on vraiment ce que c’est le climat et qu’elle est la différence entre le climat et la météo? Reprenons ces notions à la base.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#définition-de-la-météo",
    "href": "posts/ClimatMeteo/index.html#définition-de-la-météo",
    "title": "Différence entre climat et météo",
    "section": "Définition de la météo",
    "text": "Définition de la météo\nLa météo c’est le temps présent qui est caractérisé par l’ensemble des paramètres de l’atmosphère tels que la température, la pression atmosphérique, l’humidité, la vitesse et la direction du vent, la quantité de précipitations, etc. Ces paramètres peuvent être évalués à la perception; on ouvre la fenêtre, on observe le temps qu’il fait, on ressent la température, on voit s’il pleut, etc. On peut aussi les mesurer à l’aide de nombreux instruments et ainsi obtenir une description scientifique du temps qu’il fait. Par exemple, ce matin, il fait 15°C, le ciel est couvert, il pleut, le vent souffle à 20 km/h, etc. Maintenant, passons à la définition du climat.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#définition-du-climat",
    "href": "posts/ClimatMeteo/index.html#définition-du-climat",
    "title": "Différence entre climat et météo",
    "section": "Définition du climat",
    "text": "Définition du climat\nLorsqu’on parle de climat, il est toujours question de l’atmosphère et de ces interactions avec le sol mais la façon et les échelles de temps pour l’étudier sont complétement différentes. Le climat est la succession de temps qu’il fait en un lieu donné sur une très longue période de temps. Le climat est décrit selon des éléments statistiques. Par exemple, à Bruz, il pleut en moyenne 1000 mm par an, la température moyenne est de 12°C, etc.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#récapitulatif",
    "href": "posts/ClimatMeteo/index.html#récapitulatif",
    "title": "Différence entre climat et météo",
    "section": "Récapitulatif",
    "text": "Récapitulatif\nLa météo est une notion instantanée, son évolution est perceptible directement par l’humain. Le climat est une notion statistique avec une évolution incomplètement perceptible par l’humain. C’est donc grâce à un enregistement méthodique et régulier de la météo dans un même lieu et sur une longue période que l’on parvient à décrire la météo.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#illustration-par-un-exemple",
    "href": "posts/ClimatMeteo/index.html#illustration-par-un-exemple",
    "title": "Différence entre climat et météo",
    "section": "Illustration par un exemple",
    "text": "Illustration par un exemple\nConsidérons les données ci-dessous qui représentent la précipitation maximale tombée en 24 heures pour chaque mois de Janvier 1950 à Décembre 2020. Ces données sont issues de la station météorologique de Rennes.\nChaque point correspond à une donnée météorologique.\n\nimport matplotlib.pyplot as plt\n# Data Visualization Libraries:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\n\n\n\ndata = pd.read_csv('data.csv',sep=';')\ncolumns = ['AAAAMM','RRAB']\ndata = data[columns]\n\n\ndata['AAAAMM'] = pd.to_datetime(data['AAAAMM'],format='%Y%m')\n# faire un tri par date\ndata = data.sort_values(by='AAAAMM')\n# Tracer le graphe en fonction de la date\nfig,ax = plt.subplots(figsize=(10,5))\nsns.lineplot(data=data, x='AAAAMM', y='RRAB')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Précipitation maximale en 24h (mm)\")\nplt.title(\"Précipitation maximale en 24h pour chaque mois de Janvier 1950 à Juin 2021\")\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessous représente le diagramme en bâton de la précipitation maximale pour chaque mois depuis 1950 à 2022. On peut déduire que la précipitation maximale moyenne en Janvier est de 14 mm. Nous avons donc une donnée climatique.\n\nfig,ax = plt.subplots(figsize=(10,5))\n\ndata = data.assign(month=lambda x: x['AAAAMM'].dt.month)\n\n# Calculer la moyenne pour chaque mois\nmonthly_means = data.groupby('month')['RRAB'].mean()\n\n\nsns.barplot(\n    data= data,\n    \n    x='month',\n    y='RRAB',\n)\nplt.xlabel(\"Month\")\nplt.xticks(ticks = range(12), labels = [\"Jan\", \"Feb\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"]);",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/ClimatMeteo/index.html#conclusion",
    "href": "posts/ClimatMeteo/index.html#conclusion",
    "title": "Différence entre climat et météo",
    "section": "Conclusion",
    "text": "Conclusion\nLa météo et le climat sont deux notions différentes. Pour terminer, il faut retenir que un orage violent, de fortes précipitations, demain il fera 25°C, le 1er juillet 2022 est 1.5°C plus chaud que la normale, etc. sont des notions météorologiques. En revanche, la temperature moyenne en Janvier est de 5°C, il pleut en moyenne 1000 mm par an, etc. sont des notions climatiques.",
    "crumbs": [
      "About",
      "climat",
      "Différence entre climat et météo"
    ]
  },
  {
    "objectID": "posts/forumEnsai/index.html",
    "href": "posts/forumEnsai/index.html",
    "title": "Un entretien raté",
    "section": "",
    "text": "entretien"
  },
  {
    "objectID": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "href": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "title": "Un entretien raté",
    "section": "Comment raté son entretien de stage ?",
    "text": "Comment raté son entretien de stage ?\nAujourd’hui, a eu lieu le forum des ingénieurs de l’ENSAI où tout étudiant rêve de décrocher un stage. C’est l’occasion pour les étudiants de discuter avec les entreprises de leur parler d’eux.\nIl y’a une première étape, où les étudiants doivent s’inscrire sur une platform pour facililer la prise de rendez-vous. Ensuite, les étudiants doivent se présenter à l’heure à l’entretien. Ce n’est pas grave si un étudiant n’arrive pas à décrocher un entretien, il peut toujours se présenter à l’entreprise pour discuter avec les recruteurs.\nJ’ai pu obtenir quelques entretiens\nJe me suis lévé aujourd’hui à 9 heures, j’ai pris mon petit déjeuné, je me suis habillé, direction l’ENSAI. Enfin d’être présentable, j’ai porté une chémise.\nQuand je suis arrivé, j’ai localisé les différent"
  },
  {
    "objectID": "posts/Leadership/index.html",
    "href": "posts/Leadership/index.html",
    "title": "Leadership : Pouvoir, autorité et légitimité",
    "section": "",
    "text": "Leadership : Pouvoir, autorité et légitimité\n\n\n\nLe Pouvoir se reçoit, l’autorité se construit.\nComme l’éminent penseur Christian Monjou, débutons avec une citation marquante. Niccolò Machiavel, une figure clé dans l’étude du leadership et de la stratégie politique, affirmait : ‘Les qualités nécessaires pour conserver le pouvoir ne sont pas les mêmes que celles pour l’acquérir.’ Cette citation soulève un point crucial dans le domaine de la gestion du leadership et de l’autorité en entreprise. Lorsqu’on crée une entreprise, devenir un patron ou un chef d’entreprise confère du pouvoir, mais pas nécessairement de l’autorité. L’autorité, dans le contexte de la leadership efficace, est une reconnaissance octroyée par autrui ; elle repose sur la légitimité perçue par les collaborateurs. Un leader en cours de construction de son autorité n’est pas automatiquement reconnu comme légitime. En effet, la légitimité en leadership et en gestion d’équipe se manifeste à travers le respect et la confiance que l’on inspire à son entourage. Souvent, ceux qui doivent affirmer leur propre légitimité en sont en réalité dépourvus. La construction de l’autorité est donc un élément essentiel à l’efficacité du leadership dans le monde des affaires et la gestion d’équipe.\n\n\nLa construction de l’autorité\nL’essence du leadership transformationnel réside dans la capacité à favoriser la croissance et l’épanouissement de la communauté à laquelle on appartient. Cela implique d’encourager et de soutenir le développement personnel et professionnel des membres de l’équipe. Le concept de développement personnel est crucial dans le cadre du leadership efficace. En effet, un leader qui cesse de s’investir dans son propre développement personnel risque de perdre la légitimité aux yeux de ses collaborateurs. Dans la construction de l’autorité, la notion de style personnel joue également un rôle clé. Un style de leadership rigide ou répétitif peut s’avérer contre-productif. Il est important de rester adaptable et innovant, quelle que soit la situation. Cela soulève une question fondamentale : le leadership est-il inné ou acquis ? La réponse réside souvent dans la capacité d’un individu à évoluer et à s’adapter continuellement, renforçant ainsi son autorité et son influence au sein de son organisation.\n\n\nL’origine du leadership\nLa question de savoir si le leadership est inné ou acquis est un débat central dans le domaine du développement du leadership. Bien qu’il puisse sembler que certaines personnes naissent leaders, cette idée comporte des risques. En effet, se reposer uniquement sur des compétences innées en leadership peut conduire à une stagnation, limitant l’innovation et l’adaptabilité. Comme le soulignait le théoricien du leadership, Berson, ‘Plaquer du mécanique sur du vivant, c’est risquer le ridicule.’ Un leader efficace ne se contente pas de compter sur des compétences innées, mais s’interroge continuellement sur l’adéquation de son style de leadership avec les besoins actuels de sa communauté ou de son organisation. Un exemple éloquent est celui du leadership féminin, où l’adaptabilité et l’empathie jouent souvent un rôle clé dans la réussite et l’impact du leadership.\n\n\nElisabeth I er\nLa figure que vous voyez est celle d’Elisabeth Ière elisabeth., reine d’Angleterre ayant régné durant 45 ans (de 1558 à 1603). Fille de Henri VIII et Anne Boleyn, elle fut la dernière souveraine de la dynastie des Tudors et la première femme à régner sur l’Angleterre et l’Irlande. Son ascension au trône, perçue initialement comme un coup du hasard, est un exemple remarquable de la manière dont un leader peut transformer une opportunité imprévue en un règne influent et significatif. Sa présence imposante, souvent décrite comme une ‘hypertrophie de signe’, reflétait une stratégie de communication visant à établir sa légitimité et son autorité. En effet, Elisabeth Ière comprenait l’importance de l’image et du symbolisme dans la construction du leadership. Elle incarnait la dualité du ‘signe et du sens’ – un aspect essentiel dans la représentation du leadership. Son règne illustre parfaitement la nécessité pour un leader de maintenir un équilibre entre son identité privée et sa persona publique. Finalement, l’efficacité de son leadership résidait dans sa capacité à maîtriser l’art de la communication – un aspect crucial pour tout leader aspirant à un impact durable.\n\n\nLa qualité de la parole\nDans le contexte actuel de l’éducation et du leadership, une transformation notable s’est opérée par rapport aux méthodes traditionnelles. Autrefois, l’entrée d’un enseignant dans une salle de classe était synonyme de respect absolu et d’une reconnaissance incontestée de son savoir. Cependant, dans le monde moderne, marqué par l’accès instantané à l’information via la technologie numérique, cette dynamique a évolué. Imaginez un professeur mentionnant un pourcentage spécifique, comme 2.23 %, devant des étudiants équipés de smartphones. Il est probable qu’ils vérifient immédiatement cette information, remettant en question l’exactitude des données présentées. Cette situation illustre un changement fondamental : la légitimité dans le domaine de l’éducation et du leadership ne repose plus exclusivement sur la transmission du savoir, mais de plus en plus sur la qualité de la communication et de l’engagement. L’ère numérique, en économisant du temps, nous incite non pas à intensifier notre présence numérique, mais plutôt à valoriser davantage le temps consacré à la communication directe et significative. Selon Marcel Mauss, ce principe d’échange symbolique, où la qualité de la communication exige en retour un engagement – ici sous forme de loyauté – est essentiel. Enfin, la vraie mesure d’un leader réside dans sa capacité à anticiper les idées potentiellement nuisibles à sa communauté et à persuader celle-ci d’adopter de nouvelles perspectives pour éviter les pièges futurs. Mener une communauté du connu vers l’inconnu est une des responsabilités les plus ardues d’un leader.\n\n\nUn monde de changement\nNous vivons dans une ère de transformation rapide, caractérisée par un marché mondial en constante évolution. À l’ère numérique et face aux défis de la transition écologique, les communautés et organisations qui ne reconnaissent pas ce changement de paradigme risquent l’obsolescence. Cette transformation s’illustre particulièrement avec l’avènement de l’intelligence artificielle, comme en témoigne le développement de technologies telles que ChatGPT. De plus, les appels urgents du GIEC et des communautés scientifiques, notamment des astrophysiciens, mettent en lumière l’imminence d’une crise écologique mondiale. Il est impératif pour les entreprises, et plus spécifiquement pour les banques, d’évaluer et de gérer ces risques. L’enjeu n’est pas simplement financier, mais il s’agit de la préservation de la vie sur Terre dans toutes ses formes. La responsabilité sociale des entreprises et la durabilité environnementale sont devenues des facteurs clés dans la stratégie d’entreprise moderne, dictant une nouvelle approche vers un avenir plus durable et conscient."
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html",
    "href": "posts/machineLearning3A/QCM.html",
    "title": "Examen de machine learning QCM",
    "section": "",
    "text": "qcm"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?",
    "text": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?\n\n\nMinimize training error\n\n\nMinimize testing error\n\n\nMinimize a combination of training and testing error\n\n\nMaximize model complexity\n\n\nRéponse: C) Minimize training error"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "href": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "title": "Examen de machine learning QCM",
    "section": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?",
    "text": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?\n\n\nFeature selection\n\n\nOverfitting\n\n\nUnderfitting\n\n\nData preprocessing errors\n\n\nRéponse: B) Overfitting"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "href": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "title": "Examen de machine learning QCM",
    "section": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?",
    "text": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?\n\n\nLinear regression\n\n\nRidge regression\n\n\nLasso regression\n\n\nSupport vector regression\n\n\nRéponse: C) Lasso regression"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "href": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "title": "Examen de machine learning QCM",
    "section": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?",
    "text": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?\n\n\nGradient Descent\n\n\nNewton’s Method\n\n\nGenetic Algorithms\n\n\nDecision Trees\n\n\nRéponse: A) Gradient Descent"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "href": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "title": "Examen de machine learning QCM",
    "section": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?",
    "text": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?\n\n\nMaximize the margin between data points from different classes.\n\n\nMinimize the margin between data points from the same class.\n\n\nMaximize the number of support vectors.\n\n\nMinimize the number of support vectors.\n\n\nRéponse: A) Maximize the margin between data points from different classes."
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "href": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "title": "Examen de machine learning QCM",
    "section": "Q6: In Ridge regression, what does the regularization term penalize?",
    "text": "Q6: In Ridge regression, what does the regularization term penalize?\n\n\nThe magnitude of the coefficients\n\n\nThe number of features\n\n\nThe mean squared error\n\n\nThe bias of the model\n\n\nRéponse: A) The magnitude of the coefficients"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "href": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "title": "Examen de machine learning QCM",
    "section": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?",
    "text": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?\n\n\nPrincipal Component Analysis (PCA)\n\n\nK-Means Clustering\n\n\nSupport Vector Machine (SVM)\n\n\nK-Nearest Neighbors (KNN)\n\n\nRéponse: C) Support Vector Machine (SVM)"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?",
    "text": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?\n\n\nIncreasing model complexity reduces bias and increases variance\n\n\nIncreasing model complexity increases both bias and variance\n\n\nDecreasing model complexity reduces bias and increases variance\n\n\nDecreasing model complexity reduces both bias and variance\n\n\nRéponse: A) Increasing model complexity reduces bias and increases variance"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "href": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "title": "Examen de machine learning QCM",
    "section": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?",
    "text": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?\n\n\nIt reduces overfitting\n\n\nIt simplifies the optimization problem\n\n\nIt allows SVM to handle non-linear data\n\n\nIt speeds up the training process\n\n\nRéponse: C) It allows SVM to handle non-linear data"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q10: What is the primary purpose of cross-validation in machine learning?",
    "text": "Q10: What is the primary purpose of cross-validation in machine learning?\n\n\nTo train a model on multiple datasets\n\n\nTo select the best hyperparameters for a model\n\n\nTo overfit the model to the training data\n\n\nTo evaluate a model’s performance on the training data\n\n\nRéponse: B) To select the best hyperparameters for a model"
  },
  {
    "objectID": "posts/MasteringFinancialData/index.html",
    "href": "posts/MasteringFinancialData/index.html",
    "title": "Maitrise des données financières avec Python",
    "section": "",
    "text": "Introduction\nLes données financières sont très utilisées dans la filière gestion des risques de l’ENSAI. Avoir rapidement accès est un atout pour les étudiants et les fera gagner du temps. Ces données sont utilisées dans plusieurs cours notamment le cours de séries temporelles, le cours de la théorie de gestion des risques multiples,le cours d’asset pricing, etc. C’est pourquoi j’ai décidé de partager avec vous quelques astuces pour maitriser les données financières avec Python. Nous utiliserons la librairie yfinance pour récupérer les données financières et pandas pour les manipuler et matplotlib pour les visualiser.\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nJ’ai importé la library datetime pour manipuler les dataes. Maintenant, nous pouvons importer les données.\n\n\nImporter les données\n\nimport yfinance as yf\n\naapl = yf.download('AAPL', \n                      start='2012-01-01', \n                      end='2024-01-01',)\naapl.head()                    \n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\nLe code ci-dessus utilise la fonction downloadde la librairir yfinance pour télécharger les données de la société Apple (AAPL) de 2012 à 2024. De la même manière, vous pouvez télécharger les données d’autres sociétés. Par exemple, pour accèder aux données du CAC40, vous pouvez utiliser le code suivant:\n\ncac40 = yf.download('^FCHI', \n                      start='2012-01-01', \n                      end='2024-01-01',)\ncac40.head()\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n3231.429932\n3246.739990\n3193.629883\n3245.399902\n3245.399902\n123415200\n\n\n2012-01-04\n3227.459961\n3242.840088\n3186.479980\n3193.649902\n3193.649902\n114040800\n\n\n2012-01-05\n3197.159912\n3200.149902\n3136.750000\n3144.909912\n3144.909912\n121161600\n\n\n2012-01-06\n3156.419922\n3184.379883\n3122.629883\n3137.360107\n3137.360107\n104492800\n\n\n2012-01-09\n3143.949951\n3157.310059\n3114.449951\n3127.689941\n3127.689941\n96976800\n\n\n\n\n\n\n\nNous savons que pour les données de marché, nous avons les colonnes suivantes: Open, High, Low, Close, Adj Close, Volume. Nous allons maintenant travailler avec la colonne Close qui représente le prix de clôture de l’action. Nous pouvons maintenant visualiser les données.\n\nplt.figure(figsize=(10, 6))\nplt.plot(aapl['Close'], label='AAPL')\nplt.title('Prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Prix de clôture')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCes données ne sont pas stationnaires. Généralement, pour les données financières, nous travaillons avec les rendements car ils sont stationnaires. Si \\(P_t\\) est le prix de l’action à la date \\(t\\), le rendement à la date \\(t\\) est donné par: \\[\nr_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\]\nest équivalent à\n\\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right) = \\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\nDonc nous travaillons avec le logarithme des rendements. Nous pouvons maintenant calculer les rendements. Il existe une fonction pct_change dans la librairie pandas qui permet de calculer les rendements. Nous allons utiliser cette fonction combinée avec la fonction log de la librairie numpy pour calculer les logarithmes des rendements du prix de clôture de l’action AAPL.\n\ndaily_close = aapl[['Close']]\ndaily_close_returns = daily_close.pct_change().apply(lambda x: np.log(1+x))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nDans le code ci-dessus nous avons utilisé l’expression ci-dessous pour calculer les rendements: \\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right)\n\\]\nNous pouvons plutôt utiliser la fonction log de la librairie numpy pour calculer les rendements à partir de l’expression ci-dessous: \\[\n\\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\n\ndaily_close_returns = np.log(daily_close / daily_close.shift(1))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nNous avons les mêmes résultats avec la première valeur qui est NaN. Nous pouvons supprimer cette valeur.\n\ndaily_close_returns = daily_close_returns.dropna()\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n2012-01-10\n0.003574\n\n\n\n\n\n\n\nNous pouvons maintenant visualiser les rendements.\n\ndaily_close_returns.plot(figsize=(10, 6))\nplt.title('Logarithme des rendements du prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Logarithme des rendements')\nplt.show()\n\n\n\n\n\n\n\n\nSi vous voulez, vous pouvez étudier les statistiques descriptives des rendements. Une fonction que j’adore qui donne les statistiques sommaire est la fonction describe de la librairie pandas.\n\ndaily_close_returns.describe()\n\n\n\n\n\n\n\n\nClose\n\n\n\n\ncount\n3017.000000\n\n\nmean\n0.000853\n\n\nstd\n0.017967\n\n\nmin\n-0.137708\n\n\n25%\n-0.007562\n\n\n50%\n0.000765\n\n\n75%\n0.010275\n\n\nmax\n0.113157\n\n\n\n\n\n\n\nNous pouvons aussi rééchantillonner les données pour avoir les log rendements minimums, maximums, moyens, etc. par semaine, par mois, par trimestre,etc. Par exemple pour avoir les log rendements moyens par semaine, nous pouvons utliser la fonction resample de la librairie pandas combinée avec la fonction mean pour avoir les moyennes.\n\nweekly = daily_close_returns.resample('W').mean()\nweekly.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-08\n0.008933\n\n\n2012-01-15\n-0.001230\n\n\n2012-01-22\n0.000292\n\n\n2012-01-29\n0.012443\n\n\n2012-02-05\n0.005469\n\n\n\n\n\n\n\nJe vais terminer ce poste par vous montrer comment télécharger les données de plusieurs sociétés. Par exemple, pour télécharger les données de Apple, Microsoft, Google et Amazon, vous pouvez utiliser le code suivant:\n\nimport yfinance as yf\ntickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n\ndef get_data(tickers, startdate, enddate):\n    def data(ticker):\n        return (yf.download(ticker, start=startdate, end=enddate))\n    datas = map(data, tickers)\n    return(pd.concat(datas,keys= tickers, names=['Ticker', 'Date']))\nall_data = get_data(tickers, '2012-01-01', '2024-01-01')\nall_data.head()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nTicker\nDate\n\n\n\n\n\n\n\n\n\n\nAAPL\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\n\n\nConclusion\nJ’espère que ce poste vous sera utile. Si vous avez des questions, n’hésitez pas à me contacter si chatgpt ne peut pas vous aider.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Mastering Financial Data"
    ]
  },
  {
    "objectID": "posts/obligation/index.html",
    "href": "posts/obligation/index.html",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "",
    "text": "Une obligation zéro coupon est une obligation dont les intérêts sont versés en totalité à l’échéance de l’emprunt après avoir capitalisé sur toute la période.Elle est mise à un prix inférieur à sa valeur nominale et remboursée à sa valeur nominale complète à l’échéance.Par exemple, une obligation de valeur nominale 1000€ et de maturité pourrait être mise à 680 €. Ce prix inférieur reflète le fait que l’investisseur ne recevra pas d’intérêt annuel. À l’échéance, l’investisseur recevra 1000 €. Il est donc important de savoir calculer le prix d’une obligation zéro. Nous noterons B(t,T) le prix d’une obligation zéro coupon de maturité T à l’instant t. Il a plusieurs applications : - Il va permettre de construire la courbe de taux zéro coupon. - Il joue le rôle de facteur d’actualisation. - Il est aussi utilisé dans la modélisation du modèle de Hull et White.\nLa première section de ce document se focalisera sur l’élaboration de la courbe relative aux obligations zéro coupon, à partir de laquelle nous déduirons aisément la courbe des taux zéro coupon. Dans la 2nde partie, nous allons valoriser une obligation. Dans la seconde partie, nous procéderons à la valorisation d’une obligation, en mettant l’accent sur le concept de dirty price et de clean price. Ensuite nous étudier la sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit. Enfin, la troisième partie sera consacrée à l’étude d’un modèle simple de Hull et White.\n\n\nL’objectif de cette partie est de construire la courbe spot des taux zéro coupon \\(r_T\\) à partir des données de marché. Il est donné par la formule suivante : \\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\nLa calibration de la courbe de taux se fait par bootstrapping à partir des cotations du marché : - CT = cash rates (les cash flow) : Ici c’est la courbe de taux qui est constatée sur le marché interbancaire - FRA = Forward Rate Agreement (les taux forward) - SWAP = Interest Rate Swap (les taux swap)\n\n\nCode\n#!pip install openpyxl\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom scipy import interpolate\n#importer un fichier xlsx : la première feuille\n\n#courses = pd.read_csv('courses.csv')\ntaux_BBG = pd.read_excel('Taux_BBG.xlsx', sheet_name='Feuil1') \n\n\n\n\n\n\nCode\n# calcul du Mid qui est la moyenne entre le bid et l'ask\n\nnouvelle_ligne = pd.DataFrame({\n    'Term': [5],  # Supposons que vous voulez ajouter une période de 5 mois au début\n    'Unit': ['MO'],\n    'Ticker': ['EUR005M'],  # Supposons un ticker pour la nouvelle ligne\n    'Bid': [3.9],  # Exemple de valeur Bid\n    'Ask': [3.9],  # Exemple de valeur Ask\n    'Spread': [None],\n    'Bid Spr Val': [0],\n    'Ask Spr Val': [0],\n    'Final Bid Rate': [None],\n    'Final Ask Rate': [None],\n    'Rate Type': [None],\n    'Daycount': [None],\n    'Freq': [None]\n}, index=[0])\ndf = pd.concat([nouvelle_ligne, taux_BBG], ignore_index=True)\n\n\n\n\n\nPour calculer la courbe de taux à court terme, nous avons besoin du Mid qui est la moyenne entre le bid et l’ask. Nous nous le considérons comme proxy des taux interbancaires par exemple le LIBOR. Il était important de considérer la liquidité de l’instrument. Les écarts plus larges entre le bid et l’ask indiquent une liquidité plus faible.Ce qui pourrait influencer l’approximation du Libor.Pour des raisons de simplicité, nous avons, nous n’avons pas pris en compte la liquidité des instruments.\n\n\nCode\ndf['Mid'] = (df['Bid'] + df['Ask']) / 2\n\n\n\n\nCode\n# Initialisation des colonnes\ndf['T'] = 0\ndf['T1'] = 0\ndf['T2'] = 0\ndf['B(0,T)'] = 0\n\n\n\n\n\n\nCode\n# calcule du premier élement de B(0,T)\ndf['B(0,T)'].astype(int)[0] = 1\n\n\n\n\n\n\nSa formule est donnée par :\n\\[B(0,T) = \\frac{1}{(1+\\delta*L )}\\]\nOù \\(L\\) est le taux de prêt interbancaire(Libor par exemple) et \\(\\delta\\) est la fraction de l’année.\n\n\nCode\n# Conversion des termes en années\n\ndf['T2'] = df['Term'].astype(int)/12\n\n\n\n\n\n\n\nCode\n\ndf['T1'] = df['T2'] - 0.5\n\n\n\n\n\n\n\nCode\n# df['T'] = df['Term'].astype(int)/12 if df['Unit'] == 'MO' else df['Term'].astype(int)\n# With apply function\ndf['T'] = df.apply(lambda x: x['Term']/12 if x['Unit'] == 'MO' else x['Term'], axis=1)\ndf[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n5\nMO\n3.900\n0.416667\n-0.083333\n0.416667\n0\n\n\n1\n6\nMO\n3.832\n0.500000\n0.000000\n0.500000\n0\n\n\n\n\n\n\n\n\n\nCode\ndf['B(0,T1)'] =0\ndf['T'].astype(int)[0] = 0\n\ndf.loc[0, 'B(0,T)'] = 1\n\n\ndf.loc[1,'B(0,T)'] = 1/(1+ df.loc[1,'Mid']/100*0.5)\n\ndf[[\"Term\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\",\"B(0,T1)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nMid\nT\nT1\nT2\nB(0,T)\nB(0,T1)\n\n\n\n\n0\n5\n3.900\n0.416667\n-0.083333\n0.416667\n1.0000\n0\n\n\n1\n6\n3.832\n0.500000\n0.000000\n0.500000\n0.9812\n0\n\n\n\n\n\n\n\nDans la partie ci-dessus, nous avons calculé la courbe spot à partir des cash rates. Nous allons maintenant calculer la courbe spot à partir des taux forward et les taux swap. Nous pourrons ainsi prolonger la courbe spot jusqu’à 50 ans par des interpolations linéaires.\n\n\n\n\n\nCode\n# creation d'un dataframe qui va  prolonger le dataframe data_month jusqu'à 50 ans\n\nimport pandas as pd\nNombre_year = 51\nterms = [0,6, 7, 8, 9, 10, 11, 12, 15, 18] + list(range(2,Nombre_year))\nunits = ['MO' if i &lt; 10 else 'YR' for i in range(len(terms))]\n\ndata_50 = pd.DataFrame({\n    'Term': terms,\n    'Unit': units\n})\n\ndata_final = pd.merge(data_50, df, how='left', on=['Term', 'Unit'])\n\ndata_final \n\n#data_final['B(0,T)'][0] =1\ndata_final.loc[0, 'B(0,T)'] = 1\n#data_final['T'][0] = 0\ndata_final.loc[0, 'T'] = 0\ndata_final[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n0\nMO\nNaN\n0.0\nNaN\nNaN\n1.0000\n\n\n1\n6\nMO\n3.832\n0.5\n0.0\n0.5\n0.9812\n\n\n\n\n\n\n\n\n\n\nNous avons utilisé le Mid comme proxy des taux interbancaires. Il représente la moyenne arithmétique entre le “ask” et le “bid”.Nous avons utilisé la méthode de l’interpolation linéaire pour remplir les valeurs manquantes.\n\n\nCode\nfrom scipy.interpolate import interp1d\n\nnan_indices = data_final['Mid'][data_final['Mid'].isna()].index\nvalid_data = data_final.dropna(subset=['Mid'])\nfunction = interp1d(valid_data.index, valid_data['Mid'], fill_value='extrapolate')\nfor nan_index in nan_indices:\n    data_final.at[nan_index, 'Mid'] = function(nan_index)\n   \n\n\nLe code ci-dessous va permettre de prolonger la courbe des obligations spot jusqu’à 50 ans. Nous avons ainsi propagé des valeurs de la colonne T jusqu’à 50 ans.\n\n\nCode\n\n\nlast_t = data_final['T'].iloc[0]\nfor i in range(len(data_final)):\n    if pd.isna(data_final.loc[i, 'T']):\n        last_t += 1\n        data_final.loc[i, 'T'] = last_t\n    else:\n        last_t = data_final.loc[i, 'T']\n\n\n\n\n\nEn moyen terme, la courbe spot est donnée à partir des taux forward par la formule suivante:\n\\[B(0,T+\\delta) = \\frac{B(0,T)}{(1+\\delta*f )}\\]\noù \\(f\\) est le taux forward et \\(\\delta\\) est la fraction de l’année. Un prêt forward est un engagement entre 2 parties de vendre ou d’acheter un instrument financier à une date ultérieure et à un prix déterminé à l’avance.\nEn long terme, la courbe spot est donnée à partir des taux swap.Un contrat swap est un contrat d’échange de flux financiers entre deux parties. Un swap de taux est utilisé pour échanger des taux d’intérêt entre deux parties. La courbe spot est donnée par la formule suivante:\n\\[B(0,T_n) = \\frac{1 -S_{T_n} \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1+\\delta S_{T_n}}\\]\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\nimport pandas as pd\n\nimport pandas as pd\n\ndef calculate_B0T(df):\n    \"\"\"\n    Calculate B(0,T) by interpolating B(0,T1) \n    based on given B(0,T) values in a DataFrame.\n    \n    Parameters:\n    - df: DataFrame containing columns 'T', 'B(0,T)',\n    'T1', and 'Mid'.\n    \n    Returns:\n    - Updated DataFrame with interpolated 'B(0,T1)' \n    and recalculated 'B(0,T)'.\n    \"\"\"\n    \n    df['PVBP'] = 0\n    for i, row in df.iterrows():\n        if i in [0,1]:\n            continue\n        t1 = row['T1']\n        \n        if row['Unit']==\"MO\":\n            \n            ## On vérifie que l'interpolation est possible\n            \n            try:\n                # On cherche les valeurs les plus proches de t1 dans la colonne T\n                \n                valid_points = df.sort_values('T')\n                interp = interpolate.interp1d(valid_points['T'], \n                                              valid_points['B(0,T)'],\n                                              kind='linear', fill_value='extrapolate')\n                b1 = interp(t1)\n                \n                # Update the DataFrame with the interpolated value\n                df.at[i, 'B(0,T1)'] = b1  # Use 'at' for scalar value updates\n                \n                # Calcule du forward rate\n                df.at[i, 'B(0,T)'] = b1 / (1 + 0.5 * row['Mid'] / 100)\n                # Initialisation de la PVBP\n                df.at[i, 'PVBP'] = 0\n                \n                            \n            except ValueError:\n                print(f\"Impossible d'interpoler la valeur à l'index {i} avec t1 = {t1}\")\n                continue\n\n        else:\n            # Calcul de PVBP et calcul des swaps\n            df.at[i, 'PVBP'] = df.at[i-1, 'B(0,T1)'] + df.at[i-1, 'PVBP']\n            df.at[i, 'B(0,T)'] = (1- row['Mid']/100* df.at[i, 'PVBP'] )/(1+row['Mid']/100)\n            df.at[i, 'B(0,T1)'] = df.at[i, 'B(0,T)']\n    return df\n\nessai = calculate_B0T(data_final)\n\n\n\n\nLa fonction ci-dessus nous a permis de terminer la construction de notre couple. Maintenant, nous pouvons passer à une représentation graphique de la courbe spot.\n\n\nCode\n#pip install matplotlib\nimport matplotlib.pyplot as plt\nplt.plot(essai['T'], essai['B(0,T)'])\nplt.xlabel('T')\nplt.ylabel('B(0,T)')\nplt.title(\"Courbe d'obligation zéro coupon spot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLa courbe de taux zéro coupon spot est donnée par la formule suivante:\n\\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\n\n\nCode\nessai['RT'] = -np.log(essai['B(0,T)'])/essai['T']\n\n#plot the cure\n\nplt.plot(essai['T'], essai['RT'])\nplt.xlabel('Maturité en années')\nplt.ylabel('RT')\nplt.title('Courbe de taux spot sur 50 ans')\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#préparation-des-données-et-calcul-de-la-courbe-des-taux-de-zéro-coupon.",
    "href": "posts/obligation/index.html#préparation-des-données-et-calcul-de-la-courbe-des-taux-de-zéro-coupon.",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "",
    "text": "L’objectif de cette partie est de construire la courbe spot des taux zéro coupon \\(r_T\\) à partir des données de marché. Il est donné par la formule suivante : \\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\nLa calibration de la courbe de taux se fait par bootstrapping à partir des cotations du marché : - CT = cash rates (les cash flow) : Ici c’est la courbe de taux qui est constatée sur le marché interbancaire - FRA = Forward Rate Agreement (les taux forward) - SWAP = Interest Rate Swap (les taux swap)\n\n\nCode\n#!pip install openpyxl\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom scipy import interpolate\n#importer un fichier xlsx : la première feuille\n\n#courses = pd.read_csv('courses.csv')\ntaux_BBG = pd.read_excel('Taux_BBG.xlsx', sheet_name='Feuil1') \n\n\n\n\n\n\nCode\n# calcul du Mid qui est la moyenne entre le bid et l'ask\n\nnouvelle_ligne = pd.DataFrame({\n    'Term': [5],  # Supposons que vous voulez ajouter une période de 5 mois au début\n    'Unit': ['MO'],\n    'Ticker': ['EUR005M'],  # Supposons un ticker pour la nouvelle ligne\n    'Bid': [3.9],  # Exemple de valeur Bid\n    'Ask': [3.9],  # Exemple de valeur Ask\n    'Spread': [None],\n    'Bid Spr Val': [0],\n    'Ask Spr Val': [0],\n    'Final Bid Rate': [None],\n    'Final Ask Rate': [None],\n    'Rate Type': [None],\n    'Daycount': [None],\n    'Freq': [None]\n}, index=[0])\ndf = pd.concat([nouvelle_ligne, taux_BBG], ignore_index=True)\n\n\n\n\n\nPour calculer la courbe de taux à court terme, nous avons besoin du Mid qui est la moyenne entre le bid et l’ask. Nous nous le considérons comme proxy des taux interbancaires par exemple le LIBOR. Il était important de considérer la liquidité de l’instrument. Les écarts plus larges entre le bid et l’ask indiquent une liquidité plus faible.Ce qui pourrait influencer l’approximation du Libor.Pour des raisons de simplicité, nous avons, nous n’avons pas pris en compte la liquidité des instruments.\n\n\nCode\ndf['Mid'] = (df['Bid'] + df['Ask']) / 2\n\n\n\n\nCode\n# Initialisation des colonnes\ndf['T'] = 0\ndf['T1'] = 0\ndf['T2'] = 0\ndf['B(0,T)'] = 0\n\n\n\n\n\n\nCode\n# calcule du premier élement de B(0,T)\ndf['B(0,T)'].astype(int)[0] = 1\n\n\n\n\n\n\nSa formule est donnée par :\n\\[B(0,T) = \\frac{1}{(1+\\delta*L )}\\]\nOù \\(L\\) est le taux de prêt interbancaire(Libor par exemple) et \\(\\delta\\) est la fraction de l’année.\n\n\nCode\n# Conversion des termes en années\n\ndf['T2'] = df['Term'].astype(int)/12\n\n\n\n\n\n\n\nCode\n\ndf['T1'] = df['T2'] - 0.5\n\n\n\n\n\n\n\nCode\n# df['T'] = df['Term'].astype(int)/12 if df['Unit'] == 'MO' else df['Term'].astype(int)\n# With apply function\ndf['T'] = df.apply(lambda x: x['Term']/12 if x['Unit'] == 'MO' else x['Term'], axis=1)\ndf[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n5\nMO\n3.900\n0.416667\n-0.083333\n0.416667\n0\n\n\n1\n6\nMO\n3.832\n0.500000\n0.000000\n0.500000\n0\n\n\n\n\n\n\n\n\n\nCode\ndf['B(0,T1)'] =0\ndf['T'].astype(int)[0] = 0\n\ndf.loc[0, 'B(0,T)'] = 1\n\n\ndf.loc[1,'B(0,T)'] = 1/(1+ df.loc[1,'Mid']/100*0.5)\n\ndf[[\"Term\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\",\"B(0,T1)\"]].head(2)\n\n\n\n\n\n\n\n\n\nTerm\nMid\nT\nT1\nT2\nB(0,T)\nB(0,T1)\n\n\n\n\n0\n5\n3.900\n0.416667\n-0.083333\n0.416667\n1.0000\n0\n\n\n1\n6\n3.832\n0.500000\n0.000000\n0.500000\n0.9812\n0\n\n\n\n\n\n\n\nDans la partie ci-dessus, nous avons calculé la courbe spot à partir des cash rates. Nous allons maintenant calculer la courbe spot à partir des taux forward et les taux swap. Nous pourrons ainsi prolonger la courbe spot jusqu’à 50 ans par des interpolations linéaires.\n\n\n\n\n\nCode\n# creation d'un dataframe qui va  prolonger le dataframe data_month jusqu'à 50 ans\n\nimport pandas as pd\nNombre_year = 51\nterms = [0,6, 7, 8, 9, 10, 11, 12, 15, 18] + list(range(2,Nombre_year))\nunits = ['MO' if i &lt; 10 else 'YR' for i in range(len(terms))]\n\ndata_50 = pd.DataFrame({\n    'Term': terms,\n    'Unit': units\n})\n\ndata_final = pd.merge(data_50, df, how='left', on=['Term', 'Unit'])\n\ndata_final \n\n#data_final['B(0,T)'][0] =1\ndata_final.loc[0, 'B(0,T)'] = 1\n#data_final['T'][0] = 0\ndata_final.loc[0, 'T'] = 0\ndata_final[[\"Term\",\"Unit\",\"Mid\",\"T\",\"T1\",\"T2\",\"B(0,T)\"]].head(2)\n\n\n\n\n\n\n\n\n\n\n\nTerm\nUnit\nMid\nT\nT1\nT2\nB(0,T)\n\n\n\n\n0\n0\nMO\nNaN\n0.0\nNaN\nNaN\n1.0000\n\n\n1\n6\nMO\n3.832\n0.5\n0.0\n0.5\n0.9812\n\n\n\n\n\n\n\n\n\n\nNous avons utilisé le Mid comme proxy des taux interbancaires. Il représente la moyenne arithmétique entre le “ask” et le “bid”.Nous avons utilisé la méthode de l’interpolation linéaire pour remplir les valeurs manquantes.\n\n\nCode\nfrom scipy.interpolate import interp1d\n\nnan_indices = data_final['Mid'][data_final['Mid'].isna()].index\nvalid_data = data_final.dropna(subset=['Mid'])\nfunction = interp1d(valid_data.index, valid_data['Mid'], fill_value='extrapolate')\nfor nan_index in nan_indices:\n    data_final.at[nan_index, 'Mid'] = function(nan_index)\n   \n\n\nLe code ci-dessous va permettre de prolonger la courbe des obligations spot jusqu’à 50 ans. Nous avons ainsi propagé des valeurs de la colonne T jusqu’à 50 ans.\n\n\nCode\n\n\nlast_t = data_final['T'].iloc[0]\nfor i in range(len(data_final)):\n    if pd.isna(data_final.loc[i, 'T']):\n        last_t += 1\n        data_final.loc[i, 'T'] = last_t\n    else:\n        last_t = data_final.loc[i, 'T']\n\n\n\n\n\nEn moyen terme, la courbe spot est donnée à partir des taux forward par la formule suivante:\n\\[B(0,T+\\delta) = \\frac{B(0,T)}{(1+\\delta*f )}\\]\noù \\(f\\) est le taux forward et \\(\\delta\\) est la fraction de l’année. Un prêt forward est un engagement entre 2 parties de vendre ou d’acheter un instrument financier à une date ultérieure et à un prix déterminé à l’avance.\nEn long terme, la courbe spot est donnée à partir des taux swap.Un contrat swap est un contrat d’échange de flux financiers entre deux parties. Un swap de taux est utilisé pour échanger des taux d’intérêt entre deux parties. La courbe spot est donnée par la formule suivante:\n\\[B(0,T_n) = \\frac{1 -S_{T_n} \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1+\\delta S_{T_n}}\\]\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\nimport pandas as pd\n\nimport pandas as pd\n\ndef calculate_B0T(df):\n    \"\"\"\n    Calculate B(0,T) by interpolating B(0,T1) \n    based on given B(0,T) values in a DataFrame.\n    \n    Parameters:\n    - df: DataFrame containing columns 'T', 'B(0,T)',\n    'T1', and 'Mid'.\n    \n    Returns:\n    - Updated DataFrame with interpolated 'B(0,T1)' \n    and recalculated 'B(0,T)'.\n    \"\"\"\n    \n    df['PVBP'] = 0\n    for i, row in df.iterrows():\n        if i in [0,1]:\n            continue\n        t1 = row['T1']\n        \n        if row['Unit']==\"MO\":\n            \n            ## On vérifie que l'interpolation est possible\n            \n            try:\n                # On cherche les valeurs les plus proches de t1 dans la colonne T\n                \n                valid_points = df.sort_values('T')\n                interp = interpolate.interp1d(valid_points['T'], \n                                              valid_points['B(0,T)'],\n                                              kind='linear', fill_value='extrapolate')\n                b1 = interp(t1)\n                \n                # Update the DataFrame with the interpolated value\n                df.at[i, 'B(0,T1)'] = b1  # Use 'at' for scalar value updates\n                \n                # Calcule du forward rate\n                df.at[i, 'B(0,T)'] = b1 / (1 + 0.5 * row['Mid'] / 100)\n                # Initialisation de la PVBP\n                df.at[i, 'PVBP'] = 0\n                \n                            \n            except ValueError:\n                print(f\"Impossible d'interpoler la valeur à l'index {i} avec t1 = {t1}\")\n                continue\n\n        else:\n            # Calcul de PVBP et calcul des swaps\n            df.at[i, 'PVBP'] = df.at[i-1, 'B(0,T1)'] + df.at[i-1, 'PVBP']\n            df.at[i, 'B(0,T)'] = (1- row['Mid']/100* df.at[i, 'PVBP'] )/(1+row['Mid']/100)\n            df.at[i, 'B(0,T1)'] = df.at[i, 'B(0,T)']\n    return df\n\nessai = calculate_B0T(data_final)\n\n\n\n\nLa fonction ci-dessus nous a permis de terminer la construction de notre couple. Maintenant, nous pouvons passer à une représentation graphique de la courbe spot.\n\n\nCode\n#pip install matplotlib\nimport matplotlib.pyplot as plt\nplt.plot(essai['T'], essai['B(0,T)'])\nplt.xlabel('T')\nplt.ylabel('B(0,T)')\nplt.title(\"Courbe d'obligation zéro coupon spot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLa courbe de taux zéro coupon spot est donnée par la formule suivante:\n\\[r_T = -\\frac{1}{T} \\ln B(0,T)\\]\n\n\nCode\nessai['RT'] = -np.log(essai['B(0,T)'])/essai['T']\n\n#plot the cure\n\nplt.plot(essai['T'], essai['RT'])\nplt.xlabel('Maturité en années')\nplt.ylabel('RT')\nplt.title('Courbe de taux spot sur 50 ans')\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#sensibilité-du-prix-dune-obligation-en-fonction-du-taux-dintérêt-et-du-spread-de-crédit",
    "href": "posts/obligation/index.html#sensibilité-du-prix-dune-obligation-en-fonction-du-taux-dintérêt-et-du-spread-de-crédit",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "2.1 Sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit",
    "text": "2.1 Sensibilité du prix d’une obligation en fonction du taux d’intérêt et du spread de crédit\nCette fois-ci, nous allons étudier la sensibilité du prix d’obligations en fonction du taux d’intérêt et le spread de crédit.\n\n\nCode\n# Tracer pt en fonction de t pour c=0.03, s=0.01 et T=10 avec un pas de 0.1\ntemps_initial = 0\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Supposons que vos fonctions et variables sont déjà définies correctement\n\nfig, ax = plt.subplots(1,2, figsize=(8,4))  # Crée une figure et deux subplots (axes)\nplage_h = np.arange(0, 1, 0.001)\n\n\n# Supposons que prix_obligation_spread soit votre fonction personnalisée\nplage_pt_h = [prix_obligation_spread(temps_initial,coupon, spread, maturite, h, essai) for h in plage_h]\nplage_pt__h = [prix_obligation_spread(temps_initial,coupon, spread, maturite, -h, essai) for h in plage_h]\n\n# Utiliser les méthodes de l'objet Axes pour tracer\nax[0].plot(plage_h, plage_pt_h,color='green')\nax[1].plot(plage_h, plage_pt__h,color='green')\n\nax[0].set_xlabel('h')\nax[0].set_ylabel('p0')\nax[1].set_xlabel('h')\nax[1].set_ylabel('p0')\n\nax[0].set_title('Sensibilité au taux pour h positifs')\nax[1].set_title('Sensibilité  au taux pour h négatifs')\n\nplt.tight_layout()  # Ajuste automatiquement le layout pour éviter le chevauchement\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Tracer pt en fonction de t pour c=0.03, s=0.01 et T=10 avec un pas de 0.1\ntemp_initial = 0\nfig,ax = plt.subplots(1,2, figsize=(8,4))\nplage_h = np.arange(0, 1, 0.001)\nplage_pt_h = [prix_obligation_spread(temp_initial, coupon, spread, maturite, h, essai) for h in plage_h]\nplage__pt_h = [prix_obligation_spread(temp_initial, coupon, spread, maturite, -h, essai) for h in plage_h]\nax[0].plot(plage_h, plage_pt_h,color='r')\nax[1].plot(plage_h, plage__pt_h,color='r')\n\n\nax[0].set_xlabel('h')\nax[0].set_ylabel('p0')\nax[1].set_xlabel('h')\nax[1].set_ylabel('p0')\n\nax[0].set_title('Sensibilité au spread pour h positifs')\nax[1].set_title('Sensibilité  au spread pour h négatifs')\n\nplt.tight_layout()  # Ajuste automatiquement le layout pour éviter le chevauchement\nplt.show()\n\n\n\n\n\n\n\n\n\nIl en ressort des graphiques ci-dessus qu’une augmentation(diminution) du taux d’intérêt ou du spread de crédit conduit à une augmentation(diminution) du prix de l’obligation zéro coupon.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#valorisation-dun-produit-exotique-avec-hull-white",
    "href": "posts/obligation/index.html#valorisation-dun-produit-exotique-avec-hull-white",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "3.1 Valorisation d’un produit Exotique avec Hull White",
    "text": "3.1 Valorisation d’un produit Exotique avec Hull White\nDans cette partie, nous allons dans un premier temps définir le prix de n’importe quel zéro-coupon c’est-à-dire trouver une forme analytique de B(t,T)-qui est le prix d’une obligation sans coupon définie par la valeur en t d’une unité monétaire payée à l’échéance. Ensuite, nous allons nous hedger contre le risque de taux d’intérêt en utilisant les caplets. Un caplet est une option d’achat sur un taux d’intérêt. Il est utilisé pour se protéger contre une hausse des taux d’intérêt. Nous allons calibrer sa volatilité. Enfin, nous utiliserons cette volatilité et des simulations de Monte Carlo pour calibrer les niveaux de strike d’un caplet pour se protéger de la hausse d’un Libor. Nous savons par exemple que si le Libor est supérieur au strike, l’option est exercée.\n\n3.1.1 Calcul de B(t,T) avec Hull White\nLe modèle de Hull et White est une extension du modèle de Vasicek. Sous le modèle de Hull et White, le taux court est donné par la formule suivante: \\[dr_t=\\lambda (\\theta -  r_t)dt+\\sigma dW_t\\]\nNous allons étudier la partie simplifiée avec \\(\\lambda=0\\)\nMontrer tout d’abord que : La formule pour calculer (B(t,T)) est donnée par :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\noù l’équation différentielle stochastique pour (X_t) est :\n\\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\nNous avons déjà montrer dans le cours que :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -\\beta(t,T) X_t - \\frac{1}{2} \\beta(t,T)^2 \\phi(t) \\right)\\]\nAvec : \\[\\phi(t) =  \\exp(-2\\lambda t)*\\int_0^t \\sigma^2 \\exp(2\\lambda s) ds = \\frac{\\sigma^2}{2\\lambda} (1-\\exp(-2\\lambda t))\\] et \\[\\beta(t,T) = \\frac{1}{\\lambda} (1-\\exp(-\\lambda(T-t)))\\]\n\\[dX_t = (\\phi(t)- \\lambda X_t)dt + \\sigma dW_t\\]\nLorsque \\(\\lambda=0\\), nous avons : \\(\\phi(t) = \\sigma^2 t\\) et \\(\\beta(t,T) = T-t\\)\nDonc la formule pour calculer (B(t,T)) est donnée par :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left( -(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\] et \\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\n\n\n3.1.2 Calibration de la volatilité pour un strike K=4%, une maturité T=1 et \\(\\delta\\)(la fraction de l’année) = 1/2\nLe prix du caplet dans le modèle de Hull et White est donné par la formule suivante:\n\\[\\text{Caplet}^{HW1F} = B(0, T + \\delta) \\cdot \\text{Black}\\left(\\text{Fwd} = \\frac{B(0, T)}{B(0, T + \\delta)}, \\text{maturité} = T, \\text{vol} = \\sigma\\delta, \\text{Strike} = 1 + \\delta K\\right)\\]\nEt le prix du caplet du marché est obtenu par la formule suivante:\n\\[\\text{Caplet}^{\\text{Marché}} = \\delta \\cdot B(0, T + \\delta) \\cdot \\text{Black}\\left(\\text{Fwd} = \\frac{1}{\\delta} \\left( \\frac{B(0, T)}{B(0, T + \\delta)} - 1 \\right), \\text{maturité} = T, \\text{vol} = \\text{vol\\_implicite}, \\text{Strike} = K\\right)\\]\nBlack est la formule de Black Scholes qui est utilisée pour calculer le prix d’une option européenne et la vol_implicite est la volatilité implicite qui sera considérée égale à 25%.\nCalibrer \\(\\sigma\\), revient à déterminer la volatilité qui fait correspondre le prix du caplet du marché au prix du caplet dans le modèle de Hull et White. Nous utiliserons pour ce fait la méthode de Newton.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import newton\n\n# Ecrire une fonction qui calcule le prix d'un call européen\n\ndef blackScholesCall(S,K,T,r,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\n\ndef prix_caplet(K,T,delta,sigma,data= essai):\n    f_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T_delta = f_interpolation((T + delta))\n    B_0_T = f_interpolation(T)\n    fwd = B_0_T/B_0_T_delta\n    vol = sigma* delta\n    strike = 1+delta*K\n    black = blackScholesCall(S=fwd, K=strike, T=T, r=0, sigma=vol)\n    prix_caplet = B_0_T_delta  * black\n    return prix_caplet\n    \ndef prix_caplet_maket(K,T,delta,sigma,data= essai):\n    f_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T_delta = f_interpolation((T + delta))\n    B_0_T = f_interpolation(T)\n    fwd = (B_0_T/B_0_T_delta-1)*(1/delta)\n    black = blackScholesCall(S=fwd, K=K, T=T, r=0, sigma=sigma)\n    prix_caplet_maket = B_0_T_delta  * black*delta\n    return prix_caplet_maket\n\ndef implied_volatility(K,T,delta,sigma,data,prix_caplet_maket):\n    difference = lambda sigma:np.abs( prix_caplet_maket - prix_caplet(K,T,delta,sigma,data))\n    return newton(difference, sigma)\n\n    \n\n\n\n\nCode\nstrike = 0.04\ndelta = 0.5\nsigma = 0.25\nprix_caplet_maket = prix_caplet_maket(K=strike,T=maturite,delta =delta,sigma=sigma,data = essai)\n\n\n\n\nCode\nprint(f\"Le prix du caplet est {prix_caplet_maket:.6f}\")\n\n\nLe prix du caplet est 0.002126\n\n\n\n\nCode\n\nsigma_calibre = implied_volatility(K=strike,T=maturite,delta =delta,sigma=sigma,data = essai,prix_caplet_maket = prix_caplet_maket)\n\nprint(f\"La volatilité implicite est {sigma_calibre*100:.2f}%\")\n\n\nLa volatilité implicite est 0.81%\n\n\n\n\n3.1.3 Simulation de Monte Carlo pour calibrer les niveaux de seuil\nNous savons que le Libor entre deux instants i et i+1 est donné par la formule suivante:\n\\[L(i+1) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0, {i+1})} - 1 \\right)\\]\nSi nous utilisons la formule de générale de B(t,T) calculé précédemment avec t=i et T= i+1, nous obtenons la formule suivante:\n\\[L(i,i+1) = \\frac{1}{\\delta} \\left( \\frac{B(0,i)}{B(0, {i+1})} \\exp(X_i + \\frac{1}{2} \\sigma^2 i) - 1 \\right)\\]\n\\(\\sigma\\) est la volatilité de Hull et White que nous avons calibré précédemment. Nous remarquons que, calibrer L revient à calibrer X qui est donnée par la formule suivante:\n\\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\] Nous aurons pu calibrer X en prenant des pas très petits, comme dans le graphique ci-dessous, mais nous avons décidé de calibrer X avec des pas de 1 :\n\\[X_{i+1} = X_i + \\sigma^2 (i+0.5) + \\sigma Z\\]\nOù Z est une variable aléatoire suivant une loi normale centrée réduite.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Paramètres de simulation\nT = 10  # Temps final\nN = 1000  # Nombre de pas\ndt = T/N  # Pas de temps\nsigma = sigma_calibre  # Volatilité\n\n# Initialisation\nt = np.linspace(0, T, N+1)\nX = np.zeros(N+1)\nZ = np.random.normal(0, 1, N)  # Génère N réalisations de N(0,1)\n\n# Simulation par différences finies\nfor i in range(N):\n    X[i+1] = X[i] + sigma**2 * t[i] * dt + sigma * np.sqrt(dt) * Z[i]\n\n# Affichage\nplt.plot(t, X)\nplt.xlabel('Temps')\nplt.ylabel('$X_t$')\nplt.title('Simulation de $X_t$ par différences finies')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ndef simulation_monte_carlo(delta,sigma,essai,M,alpha):\n    \"\"\"La fonction simule le prix d'un caplet par la méthode de Monte Carlo\n\n    Args:\n        delta (float): Fréquence de paiement\n        sigma (float): Volatilité\n        essai (dataframe): ce dataframe contient les données des zéro coupon\n        M (float): Nombre de simulations\n        alpha (float): Taux de référence\n\n    Returns:\n        float: prix du caplet\n    \"\"\"\n    mean_price = []\n    for i in range(M):\n        Z = np.random.normal(0, 1, 10)  # Génère N réalisations de N(0,1)\n        X = np.zeros(11)\n        f_interpolation = interpolate.interp1d(essai['T'], essai['B(0,T)'])\n        L_i_iplus1 = np.zeros(10)\n        for j in range(0,10):\n            X[j+1] = X[j] + sigma**2*(j+0.5) + sigma*Z[j]\n            \n        j_values = np.arange(1, 11)  \n        B_0_i = f_interpolation(j_values)\n        B_0_i_plus_1 = f_interpolation(j_values + 1)\n        \n        exp_X_i_plus_sigma = np.exp(X[1:] + sigma**2*0.5*j_values)\n        L_i_iplus1 = ((B_0_i / B_0_i_plus_1) * exp_X_i_plus_sigma - 1) * (1 / delta)\n        meanl = np.mean(L_i_iplus1&gt;alpha)\n        mean_price.append(meanl)\n    B_0_T = f_interpolation(10)\n    return np.mean(mean_price)*B_0_T\n        \n\n\n\n\nCode\n\ndelta = 0.5\nsigma = sigma_calibre\ndata = essai\nM = 10000\nalpha = 0.03\n\nsimulation_monte_carlo(delta,sigma,data,M,alpha)\n\nprint(f\"Pour un seuil de {alpha}, le prix du flux résultant de la simulation Monte Carlo\\\n est : {simulation_monte_carlo(delta, sigma, data, M, alpha):.2f}\")\n\n\nPour un seuil de 0.03, le prix du flux résultant de la simulation Monte Carlo est : 0.58\n\n\n\n3.1.3.1 variation du prix du flux en fonction du seuil alpha\nNous avons représenté ci-dessous le prix du flux en fonction de différents niveaux de seuil alpha. Il est ressort que le prix est décroissant en fonction du seuil alpha. Le produit est donc plus cher lorsque le seuil alpha est plus bas -inférieur à 4% et plus cher lorsque le strile est plus élevé -supérieur à 4%.\nCette analyse peut permettre de développer des stratégies d’investissement pour les investisseurs et les émetteurs.\n\n\nCode\nplage_alpha = np.arange(0, 0.11, 0.01)\nplage_prix = [simulation_monte_carlo(delta,sigma,data,M,alpha) for alpha in plage_alpha]\n\nplt.plot(plage_alpha, plage_prix)\nplt.xlabel('strike')\nplt.ylabel('prix du flux')\nplt.title('Prix du flux en fonction du strike')\nplt.axhline(y=0.5, color='r', linestyle='--')\nplt.axvline(x=0.04, color='r', linestyle='--')\nplt.xticks([0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1])\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Courbe de taux"
    ]
  },
  {
    "objectID": "posts/obligation/index.html#valorisation-dune-obligation-avec-clause-de-rappel-avec-hull-white.",
    "href": "posts/obligation/index.html#valorisation-dune-obligation-avec-clause-de-rappel-avec-hull-white.",
    "title": "Courbe de taux et valorisation d’obligations",
    "section": "3.2 Valorisation d’une obligation avec clause de rappel avec Hull White.",
    "text": "3.2 Valorisation d’une obligation avec clause de rappel avec Hull White.\nLa valorisation d’une obligation avec une clause de rappel implique l’évaluation d’une obligation qui donne à l’émetteur le droit, mais pas l’obligation de rembourser le principal de l’obligation avant l’échéance à un prix prédéterminé, appelé prix de rappel, cette caractéristique offre à l’émetteur de l’obligation, la flexibilité de refinancer sa dette à un coût inférieur si les taux d’intérêt du marché baissent.\nDans cette partie, avec un exemple simple, nous allons valoriser le prix d’une option sur une obligation avec clause de rappel. Nous emploierons pour cela des simulations de Monte-Carlo afin de déterminer le prix de l’obligation avec clause de rappel. Par la suite, nous comparerons cette obligation à une obligation classique sans clause de rappel, afin de déterminer le prix d’une option de rappel elle-même. Enfin, nous procéderons à diverses analyses de sensibilité pour évaluer l’impact de différents facteurs sur la valorisation.\n\n3.2.1 Modèle de Hull et White\nNous avons déjà montrer dans le cours que :\n\\[B(t,T) = \\frac{B(0,T)}{B(0,t)} \\exp \\left(-(T-t) X_t - \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\nAvec : \\[dX_t = \\sigma^2 t \\, dt + \\sigma dW_t\\]\nsi on pose \\(X_0 = 0\\), nous avons : En intégrant l’équation différentielle stochastique pour \\(X_t\\), nous obtenons la formule suivante: \\[X_t = \\sigma^2 \\frac{t^2}{2} + \\sigma W_t\\]\nComme \\(W_t\\) est un mouvement brownien, nous avons que \\(W_t\\) est une variable aléatoire suivant une loi normale centrée de variance t. donc si on pose \\(U =\\sqrt{t}W_t\\), nous avons que U est une variable aléatoire suivant une loi normale centrée de variance 1. Ainsi on trouve que notre modèle de Hull et White s’écrira sous la forme suivante:\n\\[B(t,T,U,\\sigma) = \\frac{B(0,T)}{B(0,t)} \\exp \\left(-(T-t)(\\frac{1}{2} \\sigma^2 t^2+\\sigma \\sqrt(t)U)- \\frac{1}{2} (T-t)^2 \\sigma^2 t \\right)\\]\nAinsi nous pouvons calibrer par monte-Carlo le prix de l’obligation avec clause de rappel. Dans un prix temps la fonction de valorisation de l’obligation est donnée par la formule suivante: \\[P(t, T, U, \\sigma) = \\sum_{i=1}^{T} \\left[ c \\cdot B(t, i, U, \\sigma) \\cdot PS(i) \\cdot \\mathbf{1}_{(t &lt; i)} \\right] + B(t, T, U, \\sigma) \\cdot PS(T) \\cdot \\mathbf{1}_{(t &lt; T)}\\]\navec \\(PS(t)\\) la fonction de survie qui est donnée par la formule suivante: \\[PS(i)=\\exp(-s \\cdot(i-t))\\]\nLe prix de l’obligation avec clause de rappel est donné par la formule suivante:\ncomment on écrire le minimum de deux valeurs dans une fonction mathématique en latex?\n\\[P(t,T,\\sigma)^c = \\frac{1}{N} \\sum_{i=1}^{N}  \\min \\left( P(t, T, U, \\sigma) , 1 \\right)\\]\nAvec N le nombre de simulations.\nNous avons implémenté sur prix le code du ci-dessous.\n\n\nCode\nimport numpy as np\n\ndef B(t, T, U, sigma,data,h_taux,h_sigma):\n    \"\"\"\n    Calcule la valeur de B(t, T, U, sigma) selon la formule spécifiée.\n\n    Paramètres:\n    - t : Temps actuel.\n    - T : Temps final.\n    - U : Variable aléatoire ou paramètre spécifique au modèle.\n    - sigma : Volatilité.\n    \n    Retourne:\n    - La valeur calculée de B(t, T, U, sigma).\n    \"\"\"\n    f_inte = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_T = f_inte(T)*np.exp(-h_taux*T)\n    B_0_t = f_inte(t)*np.exp(-h_taux*t)\n    sigma+=h_sigma\n    exp_part = np.exp(-1 * (T - t) * (0.5 * sigma**2 * t**2 + sigma * np.sqrt(t) * U) - 0.5 * (T - t)**2 * sigma**2 * t)\n    return (B_0_T / B_0_t) * exp_part\n\n\n  \n\ndef prix_obligation_Hull_White(c,s,t,T,U,sigma,data,h_taux,h_sigma):\n    \"\"\"\n    Calcule le prix d'une obligation.\n    paramètres:\n    - t: Taux d'intérêt sans risque\n    - c: Coupon de l'obligation\n    - s:spread\n    - T: Maturité de l'obligation\n    - U : Variable aléatoire ou paramètre spécifique au modèle.\n    - sigma : Volatilité.\n    - data : dataframe contenant les données\n    Returns:\n    - Prix de l'obligation\n\n    \"\"\"\n    prix = 0\n    for i in  range(1,T+1):\n        if t&lt;i:\n            ps_i = np.exp(-s*(i-t))\n            b_t_i = B(t,i,U,sigma,data,h_taux,h_sigma)\n            prix += c*b_t_i*ps_i\n    if t&lt;T:\n        ps_T = np.exp(-s*(T-t))\n        b_t_T = B(t,T,U,sigma,data,h_taux,h_sigma)\n        prix += b_t_T*ps_T\n    return prix   \n    \n# simulation des prix\n\ndef simulation_prix(c,s,t,T,sigma,data,N,h_taux,h_sigma):\n    liste_prix = []\n    for i in range(N):\n        U = np.random.normal(0,1)\n        prix = prix_obligation_Hull_White(c,s,t,T,U,sigma,data,h_taux,h_sigma)\n        liste_prix.append(prix)\n    prix_ajuste = np.minimum(liste_prix,1)\n    \n    \n    return np.mean(prix_ajuste)\n\n\n\n\nCode\nNombre_simulations = 10000\ntemps_exercice = 5\nsigma =0.01\n\nprix_c =simulation_prix(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0.01,h_sigma=0.01)\nprint(f\"Le prix de l'obligation de rappel est de  {prix_c:.2f}\")\n\n\nLe prix de l'obligation de rappel est de  0.87\n\n\n\n\n3.2.2 Calcul du prix de l’option de rappel\nNous avons initialement estimé la valeur d’une obligation zéro-coupon, sa clause de rappel Par la suite, la valorisation d’une obligation incorporant une clause de rappel a été réalisée.Afin de quantifier le coût de l’option de rappel, nous procéderons à la soustraction des 2 prix précédemment obtenus, puis à l’actualisation du résultat à la date d’exercices. Le prix de l’option de rappel est donné par la formule suivante:\n\\[\\text{option} = B(0,t) \\left[ P(t,T) - P(t,T,\\sigma)^c \\right]\\]\nRappelons que P(t,T) est donné par la formule suivante:\n\\[P_t = \\sum_{i=1}^{T} \\left[ c \\cdot B(t,i) \\cdot PS(i) \\cdot \\mathbf{1}_{(t &lt; i)} \\right] + B(t,T) \\cdot PS(T) \\cdot \\mathbf{1}_{(t &lt; T)}\n\\]\nNous avons implémenté le prix de cette option dans le code ci-dessous.\n\n\nCode\n\n\n\n\n# Créeons une fonction qui calcule le prix d'une option\n\ndef calcule_option(c,s,t,T,sigma,data,N,h_taux=0,h_sigma=0):\n    fonction_interpolation = interpolate.interp1d(data['T'], data['B(0,T)'])\n    B_0_t = fonction_interpolation(t)\n    prix_c =simulation_prix(c,s,t,T,sigma,data,N,h_taux,h_sigma)\n    prix = prix_obligation_shift(t, c, s,T, h_taux,data) \n    option = B_0_t * (prix - prix_c)\n    return option\n\noption = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0.01,h_sigma=0.01)\nprint(f\"Le prix de l'option est {option}\")\n\n\nLe prix de l'option est 0.05313592167256735\n\n\n\n\nCode\nplage_t=np.arange(1,10)\nplage_option = [calcule_option(c=coupon,s=spread,t=t,T=maturite,sigma=sigma_calibre,data=essai,N=Nombre_simulations) for t in plage_t]\n    \nplt.plot(plage_t, plage_option)\nplt.xlabel('t')\nplt.ylabel('Option')\n\nplt.title('Prix de l\\'option en fonction de t')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nLe graphique ci-dessus présente le prix de l’option du rappel en fonction du temps. Nous remarquons que si l’émetteur exerce son option prématurément, il devra payer un prix plus élevé. Cependant, lorsqu’on se rapproche de la maturité de l’obligation, le prix de l’option de rappel diminue.\n\n\n3.2.3 Sensibilité du prix de l’option de rappel en fonction de la date d’exercie, de la volatilité et du taux d’intérêt.\n\n3.2.3.1 Sensibilité du prix de l’option de rappel en fonction de la volatilité.\nLe code ci-dessous représente la sensibilité du prix de l’option en fonction de la volatilité. Comme nous pouvons le constater, une augmentation de la volatilité conduit à une augmentation du prix de l’option de rappel.\n\n\nCode\n# Je veux une plage de volatilité {0%,0.25%,0.5%,...,5%}\npas = 0.25 / 100\nplage_sigma =  np.arange(0, 10 * pas, pas)\nplage_option_sigma = [calcule_option(c=0.03,s=0.01,t=5,T=10,sigma=sigma,data=essai,N=10000) for sigma in plage_sigma]\nplt.plot(plage_sigma, plage_option_sigma)\nplt.xlabel('Volatilité')\nplt.ylabel('Option')\nplt.title('Prix de l\\'option en fonction de la volatilité')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2.3.2 Sensibilité du prix de l’option de rappel en fonction du taux d’intérêt.\nLe code ci-dessous représente la sensibilité du prix de l’option en fonction du taux d’intérêt. Comme nous pouvons le constater, une augmentation infinitésimale du taux d’intérêt conduit à une diminution du prix de l’option de rappel.\n\n\nCode\nplage_h_taux = np.arange(0, 0.11, 0.01)\nplage_option_h_taux = [calcule_option(c=0.03,s=0.01,t=5,T=10,sigma=0.01,data=essai,N=10000,h_taux=h_taux) for h_taux in plage_h_taux]\nplt.plot(plage_h_taux, plage_option_h_taux)\n\nplt.ylabel('Option')\nplt.title(\"Prix de l'option en fonction d'une variation du taux d'intérêt\")\nplt.xticks([0,0.05,0.1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2.3.3 Sensibilité du prix de l’option : Delta et Vega.\n\nLe delta est la sensibilité du prix de l’option par rapport au sous-jacent qui est ici le taux d’intérêt. Il est donné par la formule suivante:\n\n\\[\\Delta =\\frac{option_{h}-option}{h}\\]\nOù \\(option_{h}\\) est le prix de l’option avec un taux d’intérêt augmenté de h.\nIl peut aussi être utilisé pour se couvrir contre le risque de taux d’intérêt.\n\nLe Vega est la sensibilité du prix de l’option par rapport à la volatilité. Il est donné par la formule suivante:\n\n\\[\\text{Vega} =\\frac{option_{\\sigma+h}-option_{\\sigma}}{h}\\]\nOù \\(option_{\\sigma+h}\\) est le prix de l’option avec une volatilité augmentée de h.\nUne obligation avec un Vega null est moins sensible à la volatilité du taux d’intérêt.\nLe code ci-dessous permet d’implémenter le calcul du delta et du vega.\nNous observons qu’une augmentation infinitésimale du taux d’intérêt et de la volatilité conduit à une diminution du prix de l’option de rappel.\n\n\nCode\nsigma = 0.01\nh =0.001\noption_h = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=h,h_sigma=0.01)\noption_0 = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=0.01)\ndelta = (option_h - option_0) /h\nprint(f\"Le delta de l'option est {delta:.2f}\")\n\n\nLe delta de l'option est -3.16\n\n\n\n\nCode\n# Calcul des Vega\nh = 0.0001\noption_haut = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=h)\noption_bas = calcule_option(c=coupon,s=spread,t=temps_exercice,T=maturite,\n                        sigma=sigma,data=essai,N=Nombre_simulations,\n                        h_taux=0,h_sigma=0)\n                            \n                            \nvega = (option_haut - option_bas) / h\nprint(f\"Le vega de l'option est {vega:.2f}\")\n\n\nLe vega de l'option est -16.03",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Courbe de taux"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "title": "Modélisation de la value at risk",
    "section": "4.1 VaR historique :",
    "text": "4.1 VaR historique :\nIci on estime la distribution des rendements R par la fonction de répartition empirique du vecteur d’observations. La VaR est alors donnée par le quantile empirique d’ordre \\(1-\\alpha\\) :\n\\(\\hat{VaR}_h(\\alpha) = \\hat{F_n^{-1}}(1-\\alpha)\\) avec \\(\\hat{F_n}(1-\\alpha)= \\frac{1}{n} \\sum_{i=1}^{n} 1_{R_i \\leq (1-\\alpha)}\\)\nNous utiliserons ainsi la fonction numpy.percentile pour calculer la VaR historique.\n\ndef hist_var(returns, index, fenetre, seuil):\n    \"\"\"Cette fonction calcule la Value at Risk (VaR) historique d'une série temporelle de log rendements\n\n    Args:\n        returns (numpy_array ): serie de log rendements\n        index (int): indice maximal de la série à considérer pour le calcul\n        fenetre (int): nombre de jours sur lesquels on calcule la VaR\n        seuil (float): niveau de confiance de la VaR\n\n    Return:\n        float: VaR historique\n    \"\"\"\n    return np.percentile(returns[index-fenetre:index], 100*(1-seuil))\n\n\nvar_hist= hist_var(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_hist)\n\n-0.04320825141346711",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "title": "Modélisation de la value at risk",
    "section": "4.2 Backtesting",
    "text": "4.2 Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist for i in range(test_size)], label=\"Non parametric VaR\", color = 'red')\nlist_exceptions_np = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist]\nplt.scatter(test_close.index[list_exceptions_np], test_close['log_return'][list_exceptions_np], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)/test_size}\")\n\nLe nombre d'exceptions pour la VaR non paramétrique est: 10\nLe pourcentage d'exceptions pour la VaR non paramétrique est: 0.004640371229698376\n\n\nNous allons maintenant vérifier si la probabilité d’exception est statiquement égale à \\(\\alpha\\).\n\nfrom scipy import stats\n\ntest_except_np = stats.binomtest(len(list_exceptions_np), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np.pvalue}')\n\nla p-value du test binomial est: 0.008950138322753154\n\n\nLa pvalue du test est inférieure au seuil de 5%. On rejette donc l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR n’est pas satisfaisante.\n\nES_np = np.mean([r for r in train_close['log_return'] if r&lt;var_hist])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.054387464763168906",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "title": "Modélisation de la value at risk",
    "section": "4.3 VaR historique Bootstrap",
    "text": "4.3 VaR historique Bootstrap\nPour la VaR historique boostrap, nous allons construire B réplications bootstrap de la série des log rendements. Pour chaque réplication(b), nous allons calculer la VaR historique. La VaR historique bootstrap est alors donnée par la moyenne des VaR historiques des B réplications bootstrap.\n\\(\\hat{VaR}_{h,bootstrap}(\\alpha) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{VaR}_{h,b}(\\alpha)\\)\nNous avons aussi calculé l’intervalle de confiance à 95% de la VaR historique bootstrap. Cet intervalle est donné par les quantiles empiriques d’ordre 2.5% et 97.5% des VaR historiques des B réplications bootstrap.\n\n\ndef VaR_Hist_Bootstrap(returns, seuil, num_simulations, alpha_IC, n_B):\n\n\n    VaRs_boot = np.zeros(num_simulations)\n\n    for i in range(num_simulations):\n        sample = np.random.choice(returns, n_B, replace=True)\n        VaRs_boot[i] = hist_var(sample, len(sample), len(sample), seuil)\n\n    VaR = np.mean(VaRs_boot)\n\n    lower_bound = np.percentile(VaRs_boot, 100 * (1-alpha_IC) / 2)\n    upper_bound = np.percentile(VaRs_boot, 100 * (1 - (1-alpha_IC) / 2))\n    IC = (lower_bound, upper_bound)\n\n    return VaR, IC\n\n\nseuil = 0.99\nseuil_IC = 0.9\nnum_simulations = 5000\nn_B = 251*10 #on utilise 10 ans de données comme taille d'échantillon bootstrap\nvar_hist_boot, IC_hist_boot = VaR_Hist_Bootstrap(train_close[\"log_return\"], seuil, num_simulations,\n                                                 seuil_IC, n_B) \nprint(f\"La VaR historique bootstrap: {var_hist_boot}\")\nprint(f\"L'intervalle de confiance associé est: {IC_hist_boot}\")\n\nLa VaR historique bootstrap: -0.039930947829674754\nL'intervalle de confiance associé est: (-0.043415059657731805, -0.03610734170316266)\n\n\nNotre estimation bootstrap de la VaR se trouve bien dans l’intervalle de confiance à 90%. Nous pouvons donc conclure que notre estimation de la VaR est satisfaisante.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "title": "Modélisation de la value at risk",
    "section": "4.4 Backtesting",
    "text": "4.4 Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist_boot for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist_boot]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['log_return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Bootstrap non paramétrique est: 16\nLe pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: 0.007424593967517401\n\n\n\ntest_except_np_boot = stats.binomtest(len(list_exceptions_np_boot), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np_boot.pvalue}')\n\nla p-value du test binomial est: 0.2775641662941861\n\n\nLe test binomial vient confirmer les conclusions faites à partir de l’intervalle de confiance estimé.\n\nES_np_boot = np.mean([r for r in train_close['log_return'] if r&lt;var_hist_boot])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np_boot}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.05028777316523479",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "title": "Modélisation de la value at risk",
    "section": "5.1 VaR gaussienne",
    "text": "5.1 VaR gaussienne\nLa VaR gaussienne suppose que les rendements suivent une loi normale. Dans ce cas, on a \\(P(R&lt;VaR_h(\\alpha)) = 1-\\alpha\\) qui est équivalent à \\(P(\\frac{R-\\mu}{\\sigma} &lt; \\frac{VaR_h(\\alpha)-\\mu}{\\sigma}) = 1-\\alpha\\) en supposant que R suit une loi normale de moyenne \\(\\mu\\) et d’écart type \\(\\sigma\\). On a alors \\(VaR_h(\\alpha) = \\mu + \\sigma \\Phi^{-1}(1-\\alpha)\\). On peut estimer les paramètres à partir de l’échantillon. On a alors \\(\\hat{VaR}_h(\\alpha) = \\hat{\\mu} + \\hat{\\sigma} \\Phi^{-1}(1-\\alpha)\\).\n\nfrom scipy import stats\ndef var_gaussienne(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR = mean_returns + sd_returns * stats.norm.ppf(1-seuil)\n    return VaR\n\n\n## VaR gaussienne sur base d'apprentissage\n\nvar_gaus = var_gaussienne(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_gaus)\n\n-0.034733067424936016\n\n\n\n5.1.1 Validation\n\n# analyse graphique avec les densités des distributions\nplt.figure(figsize = (12,8))\nplt.hist(train_close[\"log_return\"], bins=30, density=True, color='blue', label = 'log rendements')\nplt.axvline(var_gaus, color='red', linestyle='dashed', linewidth=2, label=f'VaR Gaussienne ({var_gaus})')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"]))\nplt.plot(x, p, label = 'Loi normale', linewidth=2)\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOn remarque que la queue de la loi normale n’est pas assez lourde pour nos données. La loi normale aura donc tendance à mal estimer les queues de distribution.\n\n## Analyse graphique avec le QQ-plot\n\nplt.figure(figsize=(12, 8))\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\nNous constatons que lorsqu’on utilise la loi normale pour la modélisation de la VaR, au niveau des queues de distribution,les quantiles théoriques sont moins élévés que les quantiles empiriques à gauche et plus élevés à droite. Cela signifie que la loi normale sous-estime la probabilité d’exception. La loi normale semble donc ne pas être adaptée pour modéliser la VaR.\n\n## Test d'adéquation\n\n# Test de Kolmogorov-Smirnov\nks_statistic, ks_p_value = stats.kstest(train_close[\"log_return\"], 'norm', args = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])))\nprint(f\"Test de Kolmogorov-Smirnov - Statistique : {ks_statistic},\\nP-value : {ks_p_value}\")\n\nTest de Kolmogorov-Smirnov - Statistique : 0.05681055751622879,\nP-value : 1.4266789934088429e-18\n\n\nOn rejette l’hypothèse nulle selon laquelle les log rendements suivent une distribution normale.\n\n\n5.1.2 Représentation graphique\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_gaus for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_gaus]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['log_return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.1.3 Analyse des exceptions\n\nprint(f\"Le nombre d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)/test_size}\")\n\nLe nombre d'exceptions pour la VaR gaussienne est: 25\nLe pourcentage d'exceptions pour la VaR gaussienne est: 0.01160092807424594\n\n\n\ntest_except_gaus = stats.binomtest(len(list_exceptions_gaus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus.pvalue}')\n\nla p-value du test binomial est: 0.44697408691382107\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%.\n\n\n5.1.4 VaR gaussienne à 10 jours\n\n## Var gaussienne à 10 jours par la méthode de scaling\n\nperiode = 10\nvar_gaus_scaling = np.sqrt(periode)*var_gaus\nprint(f\"La VaR gaussienne à 10 jours par la méthode de scaling est: {var_gaus_scaling}\")\n\nLa VaR gaussienne à 10 jours par la méthode de scaling est: -0.10983560318699723\n\n\n\n## VaR gaussienne à 10 jours par la méthode de diffusion\nfrom numpy import random\n\nperiode = 10\nn_simul = 10000\nS0 = train_close['Close'].iloc[-1]\nmean_returns = np.mean(train_close[\"log_return\"])\nsd_returns = np.std(train_close[\"log_return\"])\nsimulations = []\nfor k in range(n_simul):\n    simul_k=[S0]\n    for _ in range(periode):\n        Z = random.standard_normal()\n        dS = simul_k[-1]*mean_returns + simul_k[-1]*sd_returns*Z\n        simul_k.append(simul_k[-1]+dS)\n    simulations.append(simul_k)\n\nrend10 = [np.log(simul[10] / S0) for simul in simulations]\n\nalpha = 0.99\nvar_gaus_diff = np.percentile(rend10, 100*(1-alpha))\nprint(f\"La VaR gaussienne à 10 jours par la méthode de diffusion est: {var_gaus_diff}\")\n\nLa VaR gaussienne à 10 jours par la méthode de diffusion est: -0.10286418206833377\n\n\n\n\n5.1.5 VaR gaussienne pondérée :\nUne façon de corriger la VaR, est de pondérer la moyenne et l’écart type des rendements. On peut utiliser une moyenne mobile pondérée.\n\nfrom scipy import stats\ndef var_gaussienne_ewma(returns, index, fenetre, seuil, lambd):\n    rendements = returns[index-fenetre:index]\n    n = len(rendements)\n    poids = [(lambd**i)*(1-lambd) for i in range(n)]\n    denom = sum(poids)\n    poids_pond = [poid/denom for poid in poids]\n    moy_pond = np.sum([poids_pond[i]*rendements[n-i-1] for i in range(n)])\n    variance_pond = np.sum([poids_pond[i]*(rendements[n-i-1]-moy_pond)**2 for i in range(n)])\n    VaR = moy_pond + np.sqrt(variance_pond) * stats.norm.ppf(1-seuil)\n    return moy_pond, variance_pond, VaR\n\n\n## Calcul \n\nlambd1= 0.9\nlambd2 = 0.95\nlambd3 = 0.99\nmoy_pond1, variance_pond1, VaR1 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd1)\nmoy_pond2, variance_pond2, VaR2 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd2)\nmoy_pond3, variance_pond3, VaR3 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd3)\n\nlist_exceptions_gaus1 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR1]\nprint(100*\"_\"+ f\"\\nlambda = {lambd1}\\nMoyenne pondérée: {moy_pond1}\\nVariance pondérée: {variance_pond1}\\nVaR gaussienne EWMA: {VaR1}\\nNombre d'exceptions: {len(list_exceptions_gaus1)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus1)/test_size}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Premier graphique\naxes[0].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[0].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[0].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[0].plot(ts_close.index[train_size:], [VaR1 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd1})\", color='red')\naxes[0].scatter(test_close.index[list_exceptions_gaus1], [test_close['log_return'][i] for i in list_exceptions_gaus1], color='red', label='Exceptions')\naxes[0].set_title('Lambda = 0.9')\naxes[0].legend()\n\nlist_exceptions_gaus2 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR2]\nprint(100*\"_\"+ f\"\\nlambda = {lambd2}\\nMoyenne pondérée: {moy_pond2}\\nVariance pondérée: {variance_pond2}\\nVaR gaussienne EWMA: {VaR2}\\nNombre d'exceptions: {len(list_exceptions_gaus2)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus2)/test_size}\")\n\n# Deuxième graphique\naxes[1].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[1].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[1].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[1].plot(ts_close.index[train_size:], [VaR2 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd2})\", color='red')\naxes[1].scatter(test_close.index[list_exceptions_gaus2], [test_close['log_return'][i] for i in list_exceptions_gaus2], color='red', label='Exceptions')\naxes[1].set_title('Lambda = 0.95')\naxes[1].legend()\n\n\nlist_exceptions_gaus3 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR3]\nprint(100*\"_\"+ f\"\\nlambda = {lambd3}\\nMoyenne pondérée: {moy_pond3}\\nVariance pondérée: {variance_pond3}\\nVaR gaussienne EWMA: {VaR3}\\nNombre d'exceptions: {len(list_exceptions_gaus3)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus3)/test_size}\")\n\n# Troisième graphique\naxes[2].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[2].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[2].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[2].plot(ts_close.index[train_size:], [VaR3 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd3})\", color='red')\naxes[2].scatter(test_close.index[list_exceptions_gaus3], [test_close['log_return'][i] for i in list_exceptions_gaus3], color='red', label='Exceptions')\naxes[2].set_title('Lambda = 0.99')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n____________________________________________________________________________________________________\nlambda = 0.9\nMoyenne pondérée: -0.002655293241077962\nVariance pondérée: 0.0003655826690051906\nVaR gaussienne EWMA: -0.047135567638603576\nNombre d'exceptions: 8\nPourcentage d'exceptions: 0.0037122969837587007\n____________________________________________________________________________________________________\nlambda = 0.95\nMoyenne pondérée: -0.0024114946992036604\nVariance pondérée: 0.0003654872172300481\nVaR gaussienne EWMA: -0.04688596193096311\nNombre d'exceptions: 8\nPourcentage d'exceptions: 0.0037122969837587007\n____________________________________________________________________________________________________\nlambda = 0.99\nMoyenne pondérée: -0.0005003090834522245\nVariance pondérée: 0.00022779933004877785\nVaR gaussienne EWMA: -0.03561193003277455\nNombre d'exceptions: 23\nPourcentage d'exceptions: 0.010672853828306265\n\n\n\n\n\n\n\n\n\n\ntest_except_gaus1 = stats.binomtest(len(list_exceptions_gaus1), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus1.pvalue}')\n\nla p-value du test binomial est: 0.0015372581472268324\n\n\n\ntest_except_gaus2 = stats.binomtest(len(list_exceptions_gaus2), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus2.pvalue}')\n\nla p-value du test binomial est: 0.0015372581472268324\n\n\n\ntest_except_gaus3 = stats.binomtest(len(list_exceptions_gaus3), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus3.pvalue}')\n\nla p-value du test binomial est: 0.7444680028961591\n\n\nSeule la VaR estimée avec \\(\\lambda\\) = 0.99 a une p value supérieure au seuil de 5%.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "title": "Modélisation de la value at risk",
    "section": "7.1 Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements",
    "text": "7.1 Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements\n\n\nx_values = np.linspace(min(test_close['log_return']), max(test_close['log_return']), 1000)\n\nmu, sigma, skew, df =est_params\n\ntheoretical_density = SkStudentPdf(x_values, mu, sigma, skew, df)\nplt.figure(figsize = (10,8))\nplt.hist(test_close['log_return'], bins=30, density=True, alpha=0.5, label='Données empiriques')\n\nplt.plot(x_values, theoretical_density, label='Densité Skew Student', color='red')\n\n# Personnalisation du graphique\nplt.xlabel('Rendements')\nplt.ylabel('Densité')\nplt.title('Comparaison entre les données et la densité théorique')\nplt.legend()\n\n# Affichage du graphique\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densité théorique de la skew-student est très proche de la densité empirique des log rendements. Nous pouvons donc conclure que la skew-student est une bonne modélisation des log rendements.\n\n7.1.1 Fonction de repartition de la skew-student et fonction quantile\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\n\ndef integrale_SkewStudent(x):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: SkStudentPdf(x, mu_est, sigma_est, gamma_est, nu_est), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha):\n    value = integrale_SkewStudent(x)-alpha\n    return abs(value)\n\ndef theoretical_quantile(alpha):\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha))\n        return resultat_minimisation.x\n\n\nLe code ci-dessus nous a permis de construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l’inverse de cette fonction de repartition. Cette fonction quantile est le coeur de la modélisation de la VaR.\n\n\n7.1.2 QQ plot :\nLe graphique de QQ plot nous permettra de discuter de la qualité d’ajustement de la loi skew-student à la série des log rendements.\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\n\nquantiles_empiriques = np.quantile(train_close['log_return'], niveaux_quantiles)\nquantiles_theoriques = [theoretical_quantile(alpha) for alpha in niveaux_quantiles]\n\n\n\n# Créer le QQ plot\nplt.figure(figsize=(8, 8))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles théoriques')\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nNous constatons que la loi skew-student est une bonne approximation de la distribution des log rendements. Même si on constate des écarts aux queues de distribution, la loi skew-student semble bien modéliser la distribution des log rendements.\n\n\n7.1.3 comparaison entre loi gaussienne et loi de Skew Student\n\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.xlabel('Quantiles théoriques (distribution loi normale)')\nplt.ylabel('Quantiles empiriques')\nplt.title(\"QQ-plot d'une modélisation par loi normale\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi Skew Student\")\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nComme on peut le voir sur le graphique ci-dessus, la loi skew-student est une meilleure approximation de la distribution des log rendements que la loi normale. En effet, les écarts aux queues de distribution sont moins importants pour la loi skew-student que pour la loi normale.\n\n\n7.1.4 Calcul de la VaR Skew Student\n\ndef var_skewstudent(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR =  theoretical_quantile(1-seuil)\n    return VaR\n\n\n## VaR Skew Student sur base d'apprentissage\n\nvar_skew = var_skewstudent(train_close[\"log_return\"], train_size, train_size, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_skew)\n\n-0.039652480749928415\n\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_skew for i in range(test_size)], label=\"VaR Skew Student\", color = 'red')\nlist_exceptions_skew = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_skew]\nplt.scatter(test_close.index[list_exceptions_skew], test_close['log_return'][list_exceptions_skew], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Skew student est: {len(list_exceptions_skew)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Skew Student est: {len(list_exceptions_skew)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Skew student est: 16\nLe pourcentage d'exceptions pour la VaR Skew Student est: 0.007424593967517401\n\n\n\ntest_except_skew = stats.binomtest(len(list_exceptions_skew), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_skew.pvalue}')\n\nla p-value du test binomial est: 0.2775641662941861\n\n\nLa p-value du test binomial est au dessus du seuil de 5%. On ne peut donc pas rejeter l’hypothèse selon laquelle la probabilité d’exception est de 1%. Le modèle VaR Skew Student estimé semble donc satisfaisant.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "title": "Modélisation de la value at risk",
    "section": "10.1 Commentaires",
    "text": "10.1 Commentaires\nNous avons estimé jusque là 10 valeurs différentes de la VaR. Ces différentes valeurs estimées correspondent à des approches différentes. Nous avons à chaque fois estimer une VaR 99% à horizon 1 jour. Le taux d’exceptions attendu afin de juger de la bonne qualité de la VaR est de 1%. - La VaR dynamique est particulière car, comme son nom l’indique, elle est dynamique donc pas constante au cours du temps. Elle a plus d’exceptions car elle est testée sur plus de données que les autres VaRs. Toutefois son taux d’exceptions se rapproche de celui des autres VaRs. - Le test binomial fournit une p-value inférieure au seuil de 5% pour les VaRs non paramétrique, Gaussienne EWMA (\\(\\lambda\\) = 0.9 et \\(\\lambda\\) = 0.95), dynamique et TVE par maxima par blocs. Ce qui sous entend que le taux d’exception est statistiquement différent du taux attendu de 1%. On en déduit donc que les modèles utilisés sont peu adéquats pour nos données. - La VaR non paramétrique et les VaR Gaussiennes EWMA pour \\(\\lambda\\) faible ont tendance à sous estimer la VaR tandis que la VaR dynamique et la VaR TVE par maxima par blocs ont tendance à sur estimer la vraie valeur de la VaR - De plus, la validation ex-ante de la VaR TVE POT nous indique une inadéquation entre les quantiles théoriques et les quantiles empiriques des excès. - Nous avons également montré que la VaR Skew Student semblait nettement plus valide que la VaR gaussienne. Toutefois, nous avons également estimé une amélioration de la VaR gaussienne qui est la VaR gaussienne pondérée par la méthode EWMA. - Les VARs qui nous semblent donc les plus pertinentes sont la VaR Bootstrap non paramétrique, la VaR gaussienne EWMA (\\(\\lambda\\)=0.99) et la VaR Skew Student.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "title": "Modélisation de la value at risk",
    "section": "11.1 Modélisation du GARCH sur les résidus.",
    "text": "11.1 Modélisation du GARCH sur les résidus.\nNous allons appliquer un modèle GARCH(1,1) sur les résidus.\n\ngarch_final_model = arch_model(residus, vol='Garch', p=1, q=1).fit(disp='off')\nprint(garch_final_model.summary())\n\n                     Constant Mean - GARCH Model Results                      \n==============================================================================\nDep. Variable:                   None   R-squared:                       0.000\nMean Model:             Constant Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                19233.9\nDistribution:                  Normal   AIC:                          -38459.7\nMethod:            Maximum Likelihood   BIC:                          -38432.6\n                                        No. Observations:                 6462\nDate:                Sat, Feb 10 2024   Df Residuals:                     6461\nTime:                        21:16:42   Df Model:                            1\n                                 Mean Model                                 \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nmu         3.3553e-04  7.461e-06     44.969      0.000 [3.209e-04,3.502e-04]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9447e-06  9.286e-12  4.248e+05      0.000 [3.945e-06,3.945e-06]\nalpha[1]       0.1000  1.048e-02      9.539  1.442e-21   [7.945e-02,  0.121]\nbeta[1]        0.8800  8.851e-03     99.420      0.000     [  0.863,  0.897]\n============================================================================\n\nCovariance estimator: robust\n\n\nLes résultats du modèle GARCH(1,1) se trouvent dans le tableau ci-dessus. Nous pouvons constater que les paramètres sont tous significatifs. Nous allons maintenant vérifier la qualité d’ajustement du modèle GARCH(1,1) aux résidus.\n\n11.1.0.1 Analyse des résidus du modèle GARCH\nLa figure ci-dessous représente les différents graphiques : - Le graphique des résidus du modèle GARCH - La volatilité conditionnelle estimée - Le graphique des résidus standardisés\nLe résidu dont l’analyse de la blancheur nous intéresse est le résidu standardisé. Il est le résultat du rapport entre le résidu du modèle GARCH et la volatilité conditionnelle estimée. Nous allons donc vérifier si ce résidu est un bruit blanc. C’est ce résidu que nous allons utiliser pour modéliser la VaR.\n\nstd_residus = garch_final_model.resid / garch_final_model.conditional_volatility\nfig, axes = plt.subplots(3, 1, figsize=(15, 6))\n\n\naxes[0].plot(garch_final_model.resid)\naxes[0].set_title(\"Résidus\")\naxes[1].plot(garch_final_model.conditional_volatility)\naxes[1].set_title(\"Volatilité conditionnelle\")\naxes[2].plot(std_residus)\naxes[2].set_title(\"Résidus standardisés\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.1.1 Vérifions que les résidus de ce modèle GARCH sont des bruits blancs.\nNous allons faire le test de Ljung-Box pour vérifier l’autocorrélation des résidus. Et nous allons également faire ce test sur les carrés des résidus pour vérifier l’absence d’autocorrélation des résidus au carré d’où l’absence d’hetéroscedasticité.\n\n# test de Ljung-Box sur les résidus et les résidus au carré\n\ntest_lyungbow = acorr_ljungbox(std_residus, lags=[10], return_df=False)\nprint(test_lyungbow)\ntest_lyungbow_squared = acorr_ljungbox(std_residus**2, lags=[10], return_df=False)\nprint(test_lyungbow_squared)\n\n      lb_stat  lb_pvalue\n10  15.733691   0.107514\n     lb_stat  lb_pvalue\n10  9.253333   0.508242\n\n\nDans les deux cas, la p-value est supèrieur à 5%. Nous ne pouvons donc pas rejeter l’hypothèse nulle selon laquelle les résidus sont des bruits blancs. Nous pouvons donc conclure que le modèle GARCH(1,1) est satisfaisant. Nous pouvons faire d’autres tests pour vérifier la qualité de notre modèle tel que le test de Jarque-Bera et le QQ plot.\nLes estimations des paramètres du modèle AR(1)-GARCH(1,1) sont significatifs. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements. Les paramètres estimés sont : - alpha = 0.0858 - beta = 0.8978 - omega = 0.0322 - phi = -0.0092",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "title": "Modélisation de la value at risk",
    "section": "11.2 Estimation des paramètres du modèle AR(1)-GARCH(1,1).",
    "text": "11.2 Estimation des paramètres du modèle AR(1)-GARCH(1,1).\nNous pouvons directement estimer les paramètres du modèle AR(1)-GARCH(1,1) en utilisant la fonction arch_model de la bibliothèque arch. C’est ce que nous allons faire dans la cellule suivante.\n\n## estimation des paramètres du modèle AR(1) -GARCH(1,1)\n\nar_garch_model = arch_model(train_close['log_return'], vol='Garch', p=1, q=1, mean='AR', lags=1).fit(disp='off')\nprint(ar_garch_model.summary())\n\n                           AR - GARCH Model Results                           \n==============================================================================\nDep. Variable:             log_return   R-squared:                      -0.000\nMean Model:                        AR   Adj. R-squared:                 -0.001\nVol Model:                      GARCH   Log-Likelihood:                19233.9\nDistribution:                  Normal   AIC:                          -38457.7\nMethod:            Maximum Likelihood   BIC:                          -38423.9\n                                        No. Observations:                 6462\nDate:                Sat, Feb 10 2024   Df Residuals:                     6460\nTime:                        21:20:13   Df Model:                            2\n                                    Mean Model                                   \n=================================================================================\n                     coef    std err          t      P&gt;|t|       95.0% Conf. Int.\n---------------------------------------------------------------------------------\nConst          4.7602e-04  2.667e-05     17.851  2.864e-71  [4.238e-04,5.283e-04]\nlog_return[1] -9.2258e-03  1.310e-02     -0.704      0.481 [-3.490e-02,1.644e-02]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9447e-06  9.121e-12  4.325e+05      0.000 [3.945e-06,3.945e-06]\nalpha[1]       0.1000  1.048e-02      9.544  1.375e-21   [7.946e-02,  0.121]\nbeta[1]        0.8800  8.849e-03     99.450      0.000     [  0.863,  0.897]\n============================================================================\n\nCovariance estimator: robust\n\n\n\n# les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants\n\nprint(f\"Les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\\n{ar_garch_model.params}\")\nconstance,phi, omega, alpha, beta = ar_garch_model.params\n\nLes paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\nConst            0.000476\nlog_return[1]   -0.009226\nomega            0.000004\nalpha[1]         0.100000\nbeta[1]          0.880000\nName: params, dtype: float64\n\n\nLes résultats ci-dessus donnent les mêmes résultats que ceux obtenus précédemment. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "title": "Modélisation de la value at risk",
    "section": "16.1 Backtesting",
    "text": "16.1 Backtesting\n\nlist_exceptions_TVE_residus = [i for i in range(train_size,len(ts_close['log_return'])) if ts_close['log_return'][i]&lt;var_dyn_TVE_residus[i]]\nlen(list_exceptions_TVE_residus)\n\n25\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)/test_size*100.:0.2f}%\")\n\nLe nombre d'exceptions pour la VaR TVE est: 25\nLe pourcentage d'exceptions pour la VaR TVE est: 1.16%\n\n\n\ntest_except_TVE_residus = stats.binomtest(len(list_exceptions_TVE_residus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_TVE_residus.pvalue:.2f}')\n\nla p-value du test binomial est: 0.45\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR dynamique est satisfaisante.\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index, ts_close['log_return'], label=\"historical log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], var_dyn_TVE_residus[train_size:], label='VaR dynamique', color = 'red')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.scatter(ts_close.index[list_exceptions_TVE_residus], ts_close['log_return'][list_exceptions_TVE_residus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/Simulation/index.html",
    "href": "posts/Simulation/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Dans le cadre d’un projet sur la théorie de copule, je me suis rendu compte que je ne maîtrisais pas bien la simulation des variables aléatoires. J’ai donc décidé de me pencher sur le sujet. Nous utiliserons les atouts du langage de python pour aborder de façon empirique(avec des données observées), de loi d’une variable aléatoire, la loi des grands nombres et le théorème central limite. Je m’inspirerai du livre “Le Logiciel R: Maîtriser le langage - Effectuer des analyses statistiques” de Pierre Lafaye de Micheaux, Rémy Drouilhet et Benoit Liquet. Je pense que ce livre est très intéressant pour tout novice en statistique et en programmation. Il est très bien écrit et très pédagogique. Je vous le recommande vivement.\n\nGénération de nombres aléatoires\nJe me suis souvent demandé comment, les ordinateurs généraient des nombres aléatoires, et une autre question qui me pertubait était de savoir comment générait-on des nombres aléatoires qui suivent une loi donnée. Une approche de professionnel commence toujours par mettre en place un contexte, une expérience, un problème. On peut imaginer la génération de nombres aléatoires comme une expérience qui consiste à lancer un dé à 6 faces. L’ordre d’apparition des faces de ce dé peut suivre une loi uniforme.\nLorsque que l’on fait de la simulation, la génération des nombres aléatoires est très importante. La production de cet algorithme est basée sur un algorithme mathématique. Voici un exemple de génération de nombres aléatoires. Un algorithme fondé sur la méthode de congruence linéaire. Cet algorithme est basé sur la relation de récurrence suivante: \\(X_{n+1} = (aX_n + c) \\mod m\\). Rappelons que x mod m est le reste de la division euclidienne de x par m. Implémentons cet algorithme en python. Avec \\(x_1\\) un nombre aléatoire dans l’intervalle \\([0, 1]\\), \\(a =48271\\) et \\(c=0\\) et m = \\(2^{31} - 1\\).\n\ndef congruence_lineaire(x, a, c, m):\n    return (a*x + c) % m\n\nx = 0.5\na = 48271\nc = 0\nm = 2**31 - 1\ncongruence_lineaire(x, a, c, m)\n\n24135.5"
  }
]