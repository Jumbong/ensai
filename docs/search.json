[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "",
    "text": "somment mondial IA"
  },
  {
    "objectID": "posts/welcome/index.html#déroulement-du-sommet",
    "href": "posts/welcome/index.html#déroulement-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Déroulement du sommet",
    "text": "Déroulement du sommet\nCe sommet, qui s’est tenu sur deux jours (1er et 2 novembre), a réuni des sommités en IA, des entrepreneurs, des chercheurs et des représentants politiques de haut niveau. Parmi eux se trouvaient Antonio Guterres, Secrétaire général de l’ONU; Kamala Harris, vice-présidente des États-Unis; Giorgia Meloni, Première ministre italienne et seule dirigeante du G7 présente; et Elon Musk, CEO de X (ex Twitter)."
  },
  {
    "objectID": "posts/welcome/index.html#résultats-du-sommet",
    "href": "posts/welcome/index.html#résultats-du-sommet",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Résultats du sommet",
    "text": "Résultats du sommet\n\nUn accord sur la sécurité des systèmes d’IA a été signé par les grandes puissances mondiales telles que la Chine, les États-Unis et l’Union européenne, soulignant une responsabilité partagée dans la régulation de l’IA.\nUn premier rapport sur l’IA a été confié à Yoshua Bengio, scientifique canadien récompensé par le prix Turing, chargé d’évaluer les risques et avantages de l’IA.\nAntonio Guterres a plaidé pour une approche universelle : il a exhorté à une gestion de l’IA collective et éthique, alignée avec les principes fondamentaux et les droits de l’homme énoncés dans la charte des Nations Unies.\nElon Musk, lors d’un échange avec Rishi Sunak, a mis en lumière les risques de l’IA, même s’il envisage une “ère d’abondance”, alertant sur le fait que les robots humanoïdes pourraient un jour dépasser les capacités humaines.\n\nL’IA peut certes faciliter le travail et accélérer la résolution des problèmes, mais elle soulève également des inquiétudes. Si un robot peut penser et agir à la place d’une équipe de quatre personnes, que deviendront ces travailleurs ? C’est l’une des questions cruciales que soulève l’essor de l’IA."
  },
  {
    "objectID": "posts/welcome/index.html#prochaines-étapes",
    "href": "posts/welcome/index.html#prochaines-étapes",
    "title": "IA : Eléments clés du premier sommet mondial",
    "section": "Prochaines étapes",
    "text": "Prochaines étapes\nLe prochain sommet se déroulera à Paris, se concentrant sur des problèmes immédiats tels que la désinformation et l’impact sur l’emploi, avec une date encore à déterminer."
  },
  {
    "objectID": "posts/TBATS/index.html",
    "href": "posts/TBATS/index.html",
    "title": "TBATS : N’a pas de contraintes de saisonnalité",
    "section": "",
    "text": "Dans notre quête pour décrypter les tendances de consommation énergétique en France, tout au long de notre projet de série temporelle, nous avons été confrontés à un défi de taille : prédire efficacement la consommation future d’énergie dans un contexte de double saisonnalité. En explorant au-delà des méthodes classiques telles que les modèles SARIMA et ARIMA, nous avons rencontré des obstacles liés à l’estimation des variations saisonnières hebdomadaires et annuelles. Ces complexités nous ont conduits vers une solution innovante : le modèle TBATS.\nLe modèle TBATS, une avancée significative dans l’analyse des séries temporelles, embrasse la multifacette de la saisonnalité grâce à une élégante synthèse de fonctions trigonométriques. Contrairement aux modèles ARIMA et SARIMA qui peinaient à capturer les subtilités de nos données, TBATS a brillé par sa capacité à intégrer des périodicités multiples.\nDans cet article, nous plongeons au cœur de la méthodologie mathématique du modèle TBATS avant de le mettre en application sur nos données de consommation énergétique. Rejoignez-nous pour une exploration de la modélisation prédictive avec Python et la librairie tbats, et découvrez comment nous avons illuminé le chemin vers des prévisions énergétiques plus précises.\n\nPrésentation du modèle TBATS\nLe modèle TBATS ou encore (Trigonometric, Box-Cox transform, ARMA errors, Trend and Seasonal components)(De Livera, Hyndman, and Snyder 2011) a pour paramètres TBATS(\\(\\omega\\), {p,q}, \\(\\phi\\), \\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\)) où :\n\n\\(\\omega\\) correspond à la transformation de Box-Cox.\n{p,q} correspond aux paramètres de l’ARMA.\n\\(\\phi\\) correspond à la tendance.\n\\({&lt;m_1,k_1&gt;,...,&lt;m_n,k_n&gt;}\\) correspond aux paramètres de saisonnalité.\n\\(k_1,...,k_n\\) correspond aux nombres de Fourier de séries pairs.\n\nLe model s’écrit de la manière suivante :\n\\[\ny_t(\\omega) = l_{t-1} + \\phi b_{t-1} + \\sum_{i=1}^{T} s_{t-1}(i) + \\alpha d_t\n\\] \\[\nb_t = b_{t-1} + \\beta d_t\n\\] \\[\ns_t(i) = \\sum_{j=1}^{k_i} s_{j,t}(i)\n\\] \\[\ns_{j,t}(i) = s_{j,t-1}(i) \\cos \\lambda_j(i) + s^{*}_{j,t-1}(i) \\sin \\lambda_j(i) + \\gamma^{(i)}_1 d_t\n\\] \\[\ns^{*}_{j,t}(i) = -s_{j,t-1}(i) \\sin \\lambda_j(i) + s^{*}_{j,t-1}(i) \\cos \\lambda_j(i) + \\gamma^{(i)}_2 d_t\n\\] \\[\n\\lambda_j(i) = \\frac{2\\pi j}{m}\n\\]\noù :\n\n\\(i = 1, \\ldots, T\\)\n\\(d_t\\) est un processus ARMA ( p, q ),\n\\(\\alpha\\), \\(\\beta\\), \\(\\gamma_1\\) et \\(\\gamma_2\\) sont des paramètres de lissage,\n\\(l_0\\) est le niveau initial,\n\\(b_0\\) est la valeur de la pente.\n\nLes erreurs de prévisions seront modélisées par les indicateurs de qualité suivants :\n\nMean Squared Error (MSE) : \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\\)\nRoot Mean Squared Error (RMSE) : \\(RMSE = \\sqrt{MSE}\\)\nMean Absolute Error (MAE) : \\(MAE = \\frac{1}{n} \\sum_{i=1}^{n} |e_i|\\)\nMean Absolute Percentage Error (MAPE) : \\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{e_i}{y_i} \\right| \\cdot 100\\%\\)\nMean Error (ME) : \\(ME = \\frac{1}{n} \\sum_{i=1}^{n} e_i\\)\nMean Percentage Error (MPE) : $MPE = _{i=1}^{n} % $\nMean Absolute Scaled Error (MASE): \\(MASE = \\frac{n}{n-m} \\frac{\\sum_{i=1}^{n} |e_i|}{\\sum_{i=m+1}^{n} |y_i - y_{i-m}|}\\)\nAutocorrelation function of errors at lag 1 (ACF1) : \\(ACF1 = \\frac{\\sum_{i=1}^{n-1} (e_i - ME) \\cdot (e_{i+1} - ME)}{\\sum_{i=1}^{n} (e_i - ME)^2}\\)\n\noù : \\(e_t\\) is the error \\(e_t = y_t -{ y_t}^*\\)\n\\(y_t\\) est la valeur actuelle, \\(y_t^*\\) est la valeur prédicte , m est la période de saisonalité.\nDans l’analyse des prédictions de modèles statistiques, la compréhension des erreurs est cruciale. En comparant le Mean Error (ME) et le Mean Absolute Error (MAE), nous pouvons déterminer si les valeurs prédites sont systématiquement plus élevées ou plus faibles que les valeurs réelles, indiquant un biais directionnel. De même, la comparaison entre le Mean Percentage Error (MPE) et le Mean Absolute Percentage Error (MAPE) révèle l’ampleur de ce biais en termes de pourcentage.\nMais ce n’est pas tout. L’analyse du Mean Squared Error (MSE) peut révéler la présence de valeurs aberrantes ou d’erreurs exceptionnellement élevées dans les prédictions. Ces erreurs extrêmes se manifestent souvent par un écart significatif entre le MAE et le Root Mean Squared Error (RMSE), où le RMSE, en donnant plus de poids aux grandes erreurs, met en lumière les défauts plus subtils de notre modèle prédictif.\n\n\nPrésentation des données\nDans notre étude appronfondie sur la demande énergétique en france, nous avons plongé au coeur des données de consommation, mésurées en mégawattts, révelatrices des tendances de consommation du pays. Afin d’affiner notre analyse nous avons présenter les données sous forme de séries temporelles journalières. Cette approche nous permettra d’aborder plus aisement l’analyse.\n\n\nLe dendogramme de la série temporelle\nNous avons mis en évidence des motifs saisonniers clés à travers la représentation d’un dendogramme. Cet outil permettra de vérifier que 365 et 7 sont des saisonnalités.\n\nfrom scipy import signal\n\nfrequencies, power_spectral_density = signal.periodogram(df_con_daily['consommation'].values)\n\n\n# Tracé du périodogramme\nplt.figure(figsize=(5, 3))\nplt.plot(frequencies, power_spectral_density)\nplt.title('Périodogramme')\nplt.xlabel('Fréquence')\nplt.ylabel('Densité spectrale de puissance')\n\n# Ajout de la ligne verticale à 1/12\nplt.axvline(x=1/365, color='red', linestyle='--')\nplt.axvline(x=1/7, color='yellow', linestyle='--')\n\nplt.show()\n\n\n\n\ndendrogramme\n\n\nNous observons que les cylces de 365 et 7 se détachent nettement, mettant en évidence une saisonnalité annuelle est hebdommadaire.\nEn plus de l’analyse dendrogramme, une autre méthode efficace pour mettre en lumière les saisonnalités dans nos données énergétiques consiste à décomposer la série temporelle. Cette approche permet d’isoler et d’examiner les composantes saisonnières annuelles et hebdomadaires de manière distincte. L’analyse des fonctions d’autocorrélation et d’autocorrélation partielle offre également des insights précieux. Cependant, pour rester concentrés sur les aspects les plus pertinents de notre étude, nous choisirons de ne pas approfondir cette méthode dans ce cadre. Les paramètres du model TBATS(False,{0,0},0.85,{&lt;7,365&gt;}) correspondent au mieux aux données.\nDans notre démarche analytique, une étape cruciale a été la segmentation de nos données en ensembles d’entraînement et de test. Cette division stratégique est essentielle pour affiner et évaluer la précision de nos modèles prédictifs. La variable date_cutoff joue un rôle clé dans ce processus, définissant le point de séparation entre les périodes d’entraînement et de test.\n\nimport pandas as pd\nfrom sktime.datatypes import check_raise\nfrom datetime import datetime\n\ny = df_con_daily['consommation']\ny.index = pd.to_datetime(y.index)\n\n\n\ndate_cutoff = pd.Timestamp('2019-12-31')\n\n# Ensuite, effectuez la comparaison\ny_train = df_con_daily[df_con_daily.index &lt; date_cutoff]['consommation']\ny_test = df_con_daily[df_con_daily.index &gt;= date_cutoff]['consommation']\ny_train.index = pd.to_datetime(y_train.index)\n\nEnsuite nous avons instancier le modèle TBATS avec les différentes saisonnalités 7 et 365. n_jobs facilite le temps de computation. Pour plus de détails voir [TBATS](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.tbats.TBATS.html). Assurez vous que vos index ou la colonne date soit de type pandas.core.indexes.datetimes.DatetimeIndex. Pour cela vous pouvez vous servir de chatgpt.\n\nforecaster = TBATS(sp = [7, 365], n_jobs = 1)\nmodel = forecaster.fit(y_train)\n\n# Prediction\nfh = len(y_test)\ny_pred = model.forecast(fh)\n\nfig, ax = plt.subplots(figsize = (15,5))  \ny.plot(title = 'TBATS dayly energie consumption', xlabel = '', ax = ax)\ny_pred.plot(ax = ax)\nax.legend(['Actual Values', 'Forecast'])\nplt.show()\n\nUne fonction pour les différentes métriques pour évaluer la qualité du modèle est donnée par :\n\n\n\nprevision\n\n\n\ndef print_metrics(y_true, y_pred, model_name):\n    mae_ = mean_absolute_error(y_true, y_pred)\n    rmse_ = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape_ = mean_absolute_percentage_error(y_true, y_pred)\n    smape_ = mean_absolute_percentage_error(y_true, y_pred, symmetric = True)\n    \n    dict_ = {'MAE': mae_, 'RMSE': rmse_,\n             'MAPE': mape_, 'SMAPE': smape_ }\n    \n    df = pd.DataFrame(dict_, index = [model_name])\n    return(df.round(decimals = 2)) \n\nPour avoir les résultats des performances du modèle, il faut exécuter cette fonction.\n\nprint_metrics(y_test, y_pred, 'TBATS Forecaster')\n\n\n\nRésultats de Prévision avec TBATS\nNous avons essayer de mettre les résultats dans un tableau :\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate \n\ntable = [[\"TBATS Forecaster\",\"290.94\",\"412.76\",\"0.07\",\"0.06\"]]\n\nMarkdown((tabulate(\n    table,\n    headers = [\"\",\"MAE\",\"RMSE\",\"MAPE\",\"SMAPE\"]\n)\n))\n\n\nIndicateurs de performance\n\n\n\nMAE\nRMSE\nMAPE\nSMAPE\n\n\n\n\nTBATS Forecaster\n290.94\n412.76\n0.07\n0.06\n\n\n\n\n\n\n\nConclusion\n\nLe Mean Absolute Error (MAE) : Il montre qu’en moyenne, les prévisions s’écartent de 290.94 unités des valeurs réelles, nous donnant une idée de l’erreur moyenne absolue.\nLe Root Mean Squared Error (RMSE) : Sa valeur de 493.17, n’étant pas très élévée comparé au MAE, nous pouvons conclure que les erreurs de prévision sont relativement faibles.\nLe Mean Absolute Percentage Error (MAPE) et le Symmetric Mean Absolute Percentage Error (SMAPE) : Ils indiquent une erreur moyenne de prédiction de 0.07%, ce qui est considéré comme relativement précis dans notre contexte.\n\nAinsi le modèle TBATS est un modèle qui permet de faire des prévisions acceptables sur les données possèdant des saisonnalités multiples.\n\nReferences\n\n\nBROŻYNA, Jacek, Grzegorz Mentel, Beata Szetela, and Wadim Strielkowski. 2018. “MULTI-SEASONALITY IN THE TBATS MODEL USING DEMAND FOR ELECTRIC ENERGY AS a CASE STUDY.” Economic Computation & Economic Cybernetics Studies & Research 52 (1). https://www.researchgate.net/profile/Grzegorz-Mentel/publication/323868510_Multi-Seasonality_in_the_TBATS_Model_Using_Demand_for_Electric_Energy_as_a_Case_Study/links/5ab0afafaca2721710fe20b8/Multi-Seasonality-in-the-TBATS-Model-Using-Demand-for-Electric-Energy-as-a-Case-Study.pdf.\n\n\nDe Livera, Alysha M., Rob J. Hyndman, and Ralph D. Snyder. 2011. “Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing.” Journal of the American Statistical Association 106 (496): 1513–27. https://doi.org/10.1198/jasa.2011.tm09771."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html",
    "href": "posts/MarketRiskGen/index.html",
    "title": "Market risk generalities",
    "section": "",
    "text": "A market risk is a risk of loss an investment value due to a variation of the market factors. The market factors are the parameters that influence the price of a financial instrument. The most common market factors are:\n\nInterest rates\nEquity prices\nForeign exchange rates\nCommodity prices\n\nThe interest rate risk is the risk that the value of a financial instrument(ex:bond) will decline due to an increase of the interest rates.\nC’est donc une offre dont la valeur va baisser si les taux augmentent. C’est le cas des obligations.\nExemple : A zero coupon bond with a face value of 1000€ and a maturity of 10 years. The interest rate is 5%. The value of the bond is p = \\frac{1000}{(1+0.05)^{10}} = 613.\nIf the interest rate increases to 6%, the value of the bond is p = \\frac{1000}{(1+0.06)^{10}} = 558\nUne autre chose qu’il faut remarquer est que si les taux d’intérêts augmentent, la valeur des obligations diminue. Qu’elle est l’interprétation économique de cela ?\n\n\nSi les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue.\n\n\n\nUne banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "href": "posts/MarketRiskGen/index.html#augmentation-des-taux-dintérêts-entraine-une-diminution-de-la-valeur-des-obligations",
    "title": "Market risk generalities",
    "section": "",
    "text": "Si les taux d’intérêts augmentent, les investisseurs peuvent obtenir plus de rendements sur les nouvelles obligations, la demande pour les anciennes obligations diminue et donc leur prix diminue."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#curve-risk",
    "href": "posts/MarketRiskGen/index.html#curve-risk",
    "title": "Market risk generalities",
    "section": "",
    "text": "Une banque se finance sur le coup terme et prête sur long terme. Si les taux d’intérêts augmentent, la banque va devoir payer plus cher pour se financer et donc son bénéfice va diminuer.\nComment le régulateur peut diminuer l’inflation ? En augmentant les taux d’intérêts, cela rend le credit plus cher, ce qui peut réduire les dépenses des ménages et des entreprises et les investissements. Ceci réduit la demande de crédit et dont l’offre de monnaie. Cela réduit l’inflation.\nDonc pour relancer l’économie, on peut baisser les taux d’intérêts.\nFinalement, pour que la banque puisse se financer à coup terme et prêter à long terme, il faut une absence d’une corrélation entre les taux d’intérêts à court terme et à long terme.\nEquity price risk(le risque du prix des actions, ETF (Exchange-Traded Funds) : ) is the risk associated with the volatility of the stock prices. It can be divided into two categories: general market risk and specific risk.\nLet R_i the return of a portfolio of N equities. The global return R_p = \\frac{\\sum_{i=1}^{N}R_i}{N}. If we denode by R_m the return of the market, the global return can be written as R_i = \\beta R_m + \\alpha_i where \\beta is the sensitivity of the portfolio to the market and \\alpha is the specific return of the portfolio.\nForeign exchange risk is the risk that the value of a financial instrument will decline due to a change in the exchange rate.\nThe price of commodities is influenced by the supply and demand.\nThe market risk can be measured by several methods: sensitivity analysis, scenario analysis, value at risk, stress testing."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "href": "posts/MarketRiskGen/index.html#sensitivity-based-methods",
    "title": "Market risk generalities",
    "section": "Sensitivity-based methods",
    "text": "Sensitivity-based methods\n\nDuration\nLa duration exprime comment un porte-feuille est sensible aux variations des taux d’intérêts.\nExemple: Un porte-feuille avec une duration de 5 ans signifie que si les taux d’intérêts augmentent de 1%, la valeur du porte-feuille va diminuer de 5%.\nDonc une duration élevée signifie un risque élevé.\nMaintenant si on a deux obligations(bonds): Le premier de nominal 1000, maturité 10 ans, de prix 900 et de coupon 5%. Le deuxième de nominal 1000, maturité 10 ans, de prix 1100 et de coupon 7%.\nSur quels bond va t-on investir ?\nPour répondre à cette question, nous avons besoin d’un outil: the yield to maturity.\nThe yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre. Mathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\noù C_i = nominal* coupon i!=N C_N = nominal + coupon*nominal\nIl faut deux hypothèses pour calculer le yield to maturity: - Le coupon est réinvesti au taux d’intérêt du marché. - Le titre est détenu jusqu’à la maturité. On a défini le yield to maturity, on peut définir la duration.\nLa duration est la sensibilité de la valeur d’un titre par rapport au yield to maturity. Mathématiquement,\nD = -\\frac{1}{P}\\frac{dP}{dy}.\nConséquence de la duration:\n\nSi le taux d’intérêt augmente diminue, nous allons investir dans un porte-feuille avec une duration élevée.\nSoit une obligation dont le prix est de 90 qui paye des coupons annnuels de 5% de nominal 1000 et de maturité 5 ans.\n\nDeterminons son yield to maturity.\nOn sait que le yield to maturity est le taux d’intérêt qui rend la valeur actuelle des flux de trésorerie futurs égale au prix actuel du titre.\nMathématiquement, P = \\sum_{i=1}^{N} \\frac{C_i}{(1+y)^i}\nPour déterminer la duration, on a besoin de calculer la dérivée de P par rapport à y.\non sait que la dérivée peut être calculée comme une limite de la formule suivante:\n\\frac{dP}{dy} = \\lim_{\\Delta y \\to 0} \\frac{P(y+\\Delta y) - P(y- \\Delta y)}{2 \\Delta y}\nAinsi si on a déterminé le yield to maturity, on peut déterminer la duration. Il faut pour cela, calculer le prix de l’obligation pour deux valeurs de y: y+\\Delta et y-\\Delta y. Prenons \\Delta = 0.01 Pour y1 = y+\\Delta P(y+\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y+\\Delta)^i}\npour y2 = y-\\Delta P(y-\\Delta) = \\sum_{i=1}^{N} \\frac{C_i}{(1+y-\\Delta)^i}\nFinalement, D = -\\frac{1}{P}\\frac{P(y+\\Delta) - P(y- \\Delta y)}{2 \\Delta y}\nIl y’a une hypothèse forte derrière la duration: Une courbe de déplacement parallèle. C’est à dire que si les taux d’intérêts augmentent de 1%, la courbe des taux d’intérêts va se déplacer parallèlement vers le haut."
  },
  {
    "objectID": "posts/MarketRiskGen/index.html#the-greeks",
    "href": "posts/MarketRiskGen/index.html#the-greeks",
    "title": "Market risk generalities",
    "section": "The Greeks",
    "text": "The Greeks\nLa valeur d’un porte-feuille dépend de plusieurs facteurs, qui peuvent interagir entre eux. Pour mesurer l’impact de ces facteurs sur la valeur du porte-feuille, on utilise les lettres grecques : delta, gamma, vega, theta, rho.\n\nDelta\nLe delta exprime le changement entre un changement de la valeur du porte-feuille et un changement de la valeur du sous-jacent. Exemple : Soit C_0 le prix d’une option de maturité T et de strike k. Soit S_t le prix du sous-jacent à la date t.\npayoff de l’option : max(S_T - k, 0), le prix de l’option est le payoff actualisé : C_0 = \\frac{max(S_T - k, 0)}{(1+r)^T}\nLes facteurs de risques de cette option sont : le prix du sous-jacent, le taux d’intérêt et la volatilité.\nLe delta de cette option est : \\frac{\\partial C_0}{\\partial S_t}\nComment se couvrir de la variation du prix du sous-jacent ?\nIl faut construire un porte-feuille qui a un delta égal à 0. C’est à dire que si le prix du sous-jacent augmente, la valeur du porte-feuille ne change pas.\non peut par exemple considéré un porte-feuille de prix P^{'} = C_0 + \\alpha S_t. La nouvelle option est de sensibilité nulle par rapport au prix du sous-jacent si \\alpha = -\\frac{\\partial C_0}{\\partial S_t}. Ceci revient à dire que pour se couvrir du risque de la variation de prix du sous-jacent, il faut vendre à decouvert c’est-à-dire short selling \\frac{\\partial C_0}{\\partial S_t} unités du sous-jacent. Le delta d’une action est 1.\nUn portefeuille avec un delta de 0 est option risque neutre. Il est construit pour (hedging purposes) se couvrir du risque de la variation du prix du sous-jacent.\n\n\nGamma\nLe gamma est la dérivée seconde du prix de l’option par rapport au prix du sous-jacent. Il mesure la sensibilité du delta par rapport au prix du sous-jacent. Son implication dans le cas d’un portefeuille delta-neutre est qu’il a besoin d’être réajusté régulièrement si son gamma est élevé. Le gamma est donc de mesurer le degré d’exposition au risque qu’une position couverte(hedged position) dévellopera si la couverture(hedge) n’est pas réajustée.\n\n\nVega\nLe vega mesure le changement du prix de l’option par rapport à la volatilité du sous-jacent. Il mesure la sensibilité du prix de l’option par rapport à la volatilité du sous-jacent.\nUn portefeuille avec un vega de 0 est vega-neutre. Il est construit pour se couvrir du risque de la variation de la volatilité du sous-jacent. C’est-à-dire qu’il est insensitive à la variation de la volatilité du sous-jacent.\n\n\nTheta and Rho\nLe Theta et le rho déterminent respectivement le taux de changement de la valeur d’un portefeuille par rapport au temps to maturity et par rapport au taux d’intérêt.\nEn practique, les sensibilités d’un portefeuille sont calculées intensivement, intra-day(à l’intérieur de la journée) et quotidiennement pour chaque traded product. Ils sont utilisées par les traders afin de gérer leur risque, their position(hedging) et par les managers pour expliquer leur perte et leur profit(P&L). Cependant, elles souffrent de plusieurs limitations: elles ne peuvent pas être comparées entre plusieurs activités pour conclure qu’une activité est plus risquée qu’une autre."
  },
  {
    "objectID": "posts/machineLearning3A/index.html",
    "href": "posts/machineLearning3A/index.html",
    "title": "Examen de machine learning",
    "section": "",
    "text": "Il est possible que ce soit Da Veiga qui assure vos cours, mais je vais vous offrir un aperçu de ce à quoi pourrait ressembler votre examen. C’est important, car nous avons tendance à sous-estimer ce type d’activité, surtout lorsqu’il autorise l’utilisation de générateurs de texte tels que ChatGPT. Cette année, peu d’étudiants ont achevé le projet, la charge de données étant longue et fastidieuse. Je vous conseille de vous y prendre en avance. Préparez des fonctions exécutant certaines tâches spécifiques, que je vous expliquerai progressivement."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#box-plot",
    "href": "posts/machineLearning3A/index.html#box-plot",
    "title": "Examen de machine learning",
    "section": "Box plot",
    "text": "Box plot\nLe graphique ci-dessous montre la distribution de chaque variable en fonction de la variable cible.\n\n# box plot\n# Transformed Cover_Type to categorical\nY_sampled = Y_sampled.astype('category')\n\nfor col in X_sampled.columns:\n    sns.boxplot(x=Y_sampled, y=X_sampled[col])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterprétation: Nous allons nous concentrer sur la variable Elevation.\nOn peut voir que la variable Elevation est très discriminante.\nLes types de couverture 1, 2, et 7 montrent des médianes relativement élevées pour l’élévation, avec 7 ayant la médiane la plus élevée, suivie par 1 et 2."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#elastic-net-regression",
    "href": "posts/machineLearning3A/index.html#elastic-net-regression",
    "title": "Examen de machine learning",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nC’est une méthode de machine learning qui combine la régression Ridge et Lasso. Elle est utilisée pour résoudre le problème de surraprentissage, la multicollinéarité et la sélection de variables.\nPassons à sa modélisation :\nNous avons utilisé ici un modèle de régression logistique avec une pénalité elasticnet. Nous avons utilisé une validation croisée pour trouver le meilleur paramètre de régularisation. Nous avons utilisé une pénalité elasticnet avec un ratio de 0.5. Nous avons utilisé un solver saga qui est adapté aux problèmes multiclasse. Nous avons utilisé une tolérance de 0.01. Nous avons utilisé un random state de 12345.\n\nclf_l1l2_LR = LogisticRegressionCV(penalty='elasticnet', l1_ratios=[0.5], \n                                   cv=5, multi_class=\"multinomial\", \n                                 solver=\"saga\",tol=0.01, random_state=12345)\n\nmodel = Pipeline(steps=[('preprocessor', preprocessor), ('logistic', clf_l1l2_LR)])\n\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\naccuracy_LR = accuracy_score(Y_test, prediction)\n\nprint(\"Accuracy of Logistic Regression :\",\"%.3f\" % accuracy_LR)\n\nAccuracy of Logistic Regression : 0.714\n\n\nJ’ai un accuracy de 0.714. Ce n’est pas mal. Nous pouvons voir la matrice de confusion.\n\n# Confusion matrix\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(Y_test, prediction)\n\n# Display the confusion matrix using Seaborn's heatmap\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\n\n\n\n\n\n\n\n\nLes valeurs sur la diagonale principale (de haut à gauche à bas à droite) représentent le nombre de prédictions correctes pour chaque classe. Par exemple, il y a 29724 prédictions correctes pour la classe 0, 44616 pour la classe 1, et ainsi de suite.\nLes valeurs hors de la diagonale indiquent les erreurs de classification. Par exemple, 11947 instances de la classe 0 ont été incorrectement prédites comme appartenant à la classe 1.\nLa classe 0 a le plus grand nombre de faux positifs, c’est-à-dire que de nombreuses instances d’autres classes ont été incorrectement prédites comme appartenant à la classe 0.\nLes classes avec le moins de prédictions incorrectes (et donc les plus sombres dans la visualisation) sont la classe 3 et la classe 6, avec respectivement 187 et 1993 prédictions correctes. Les cases avec un fond plus clair, en dehors de la diagonale, indiquent des erreurs moins fréquentes entre les classes spécifiques."
  },
  {
    "objectID": "posts/machineLearning3A/index.html#random-forest",
    "href": "posts/machineLearning3A/index.html#random-forest",
    "title": "Examen de machine learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nOOB error (Out-of-bag error)\nOOB est une méthode de validation croisée pour les forêts aléatoires. Chaque arbre dans la forêt est construit à partir d’un échantillon bootstrap du jeu de données d’entraînement. Certaines observations sont laissées de côté et non utilisées dans la construction d’un arbre donné. Ces observations “hors sac” peuvent être utilisées pour évaluer les performances de cet arbre. Du coup on peut utiliser cette méthode pour évaluer la performance de la forêt aléatoire et ajuster les hyperparamètres.\nLe code ci-dessous montre comment calculer l’erreur OOB pour un modèle de forêt aléatoire. Il permet en particulier de sélectionner la profondeur de l’arbre dans la forêt aléatoire. Le modèle de forêt aléatoire est entraîné avec une profondeur d’arbre de 10, 20 et 30. L’erreur OOB est calculée pour chaque modèle. Le modèle avec la plus petite erreur OOB est sélectionné.\n\n## Training Random Forest\n\n\n\n\ndepths = [10, 20, 30]\noob_errors = []\nmodels = []\nbest_oob_error = float('inf')\nbest_model = None\ni = 0\nfor depth in depths:\n    print(i)\n    model = RandomForestClassifier(max_depth=depth, oob_score=True, random_state=42,\n                                   n_estimators=100,  \n                                   warm_start=True  # This allows us to add more estimators later if needed\n                                  )\n    model.fit(X_train, Y_train)\n    oob_error = 1 - model.oob_score_\n    oob_errors.append(oob_error)\n    models.append(model)\n    if oob_error &lt; best_oob_error:\n        best_oob_error = oob_error\n        best_model = model\n    i = i+1\n    print(\"Done\")\n\n# Print OOB errors for each model\nfor depth, error in zip(depths, oob_errors):\n    print(f\"Depth: {depth}, OOB Error: {error}\")\n\n0\nDone\n1\nDone\n2\nDone\nDepth: 10, OOB Error: 0.21680518234371537\nDepth: 20, OOB Error: 0.05706860237215716\nDepth: 30, OOB Error: 0.038628770096964526\n\n\n\n\nAccuracy et matrice de confusion\n\n# Compute the accuracy of the best random forest model\npredictions = best_model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy of the Best Random Forest: {:.3f}\".format(accuracy))\n\n# Display the confusion matrix\nconf_matrix = confusion_matrix(Y_test, predictions)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n\nAccuracy of the Best Random Forest: 0.962\n\n\n\n\n\n\n\n\n\nNous avons un accuracy de 0.962. C’est très bien si on compare avec le modèle de régression logistique qui a un accuracy de 0.714. Nous pouvons voir la matrice de confusion."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html",
    "href": "posts/InstrumentsFinanciers/index.html",
    "title": "Les intruments financiers à un bébé",
    "section": "",
    "text": "Les instruments financiers\nDans le cadre d’un cours d’asset pricing à l’ENSAI, je suis totalement perdu dans la compréhension des instruments financiers.\nCe décrochage m’empêche de suivre le cours et je suis donc totalement perdu sur les autres notions telles que pricing, hedging, etc.\nJ’ai donc décidé de retourner à la base c’est-à-dire de comprendre les instruments financiers.\nDans mes recherches sur chatgpt, j’ai trouvé les différents instruments financiers et leurs définitions."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "href": "posts/InstrumentsFinanciers/index.html#les-obligations.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les obligations.",
    "text": "Les obligations.\nJ’ai été vraiment déçu de la définition et de sa complexité. Une obligation est définie comme un titre de dette émis par une entreprise, un gouvernement, etc. Les détenteurs de ces obligations reçoivent des intérêts à intervalles réguliers et remboursent le montant total à l’échéance.\nQu’est-ce que cela signifie concrètement ?\nPrenons un exemple. J’ai faim, j’ai besoin d’argent pour manger à l’école mais je n’ai pas d’argent. Je vais donc voir un ami pour lui expliquer ma situation. Il me prête de l’argent de l’argent avec des intérêts sur une durée de cinq jours et il me demande de lui verser une partie de cette argent(qui correspond au produit entre le taux d’intérêt et la somme dont je lui est empruntée) chaque jour et de lui rembourser le montant total à la fin des cinq jours. Pour être sûr que je vais lui rembourser, il me demande un papier signé par moi et par lui. Ce papier est une obligation.\nIl existe deux types d’obligations: - Les obligations 0 coupon: Ce sont des prêts que l’on fait généralement au lycée. C’est-à-dire que l’on part voir un ami pour lui demander de l’argent avec un intérêt de x% pour cinq jours. On lui remboursera à la fin des cinq jours la somme empruntée plus les intérêts.\n\nLes obligations couponnées: Ce sont des obligations qui versent des intérêts à intervalles réguliers. C’est-à-dire que chaque mois nous verserons une partie de la somme empruntée(la somme empruntée multipliée par le taux d’intérêt) et à la fin de la période, nous rembourserons la somme empruntée plus les intérêts."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "href": "posts/InstrumentsFinanciers/index.html#les-actions.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les actions.",
    "text": "Les actions.\nEn anglais les actions sont appelées les equities. Ce sont des titres de propriété d’une entreprise."
  },
  {
    "objectID": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "href": "posts/InstrumentsFinanciers/index.html#les-produits-dérivés.",
    "title": "Les intruments financiers à un bébé",
    "section": "Les produits dérivés.",
    "text": "Les produits dérivés.\nCe sont les intruments financiers qui pose actuellement beaucoup de difficultés.\nCe sont des intruments financiers dont la valeur dépend de la valeur d’un autre actif qui est appelé sous-jacent. On compte parmi les produits dérivés les options, les futures, les swaps, etc.\n\nLes options.\nUne option est un contrat qui donne à son détenteur le droit mais pas l’obligation d’acheter ou de vendre un actif à un prix fixé à l’avance et à une date donnée.\nIl y’a deux choses à retenir dans cette définition: le prix fixé à l’avance et la date donnée.\nOn signe ce type de contrat lorsque pense que le prix de l’actif va augmenter.\nPar exemple je pense que le prix de l’Iphone va augmenter dans les prochains jours. Je vois un ami qui vend des Iphones. Je lui donne un montant afin qu’il me reserve cet Iphone à un prix k fixé aujourd’hui. Si le prix de l’Iphone augmente, je pourrais l’acheter à un prix inférieur à celui du marché. Si le prix de l’Iphone diminue, je ne l’achèterai pas et je perdrai le montant que j’ai donné à mon ami.\n\n\nLes futures."
  },
  {
    "objectID": "posts/entretienCreditAgricole/index.html",
    "href": "posts/entretienCreditAgricole/index.html",
    "title": "Compte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.",
    "section": "",
    "text": "Sommet mondial IA\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\nListe des personnes présentes :\n\n\n\nIntervenants\n\n\nPrénom\n\n\nNom\n\n\n\n\nRecruteur\n\n\nprenom\n\n\nnom\n\n\n\n\nRecruteur\n\n\nMarie\n\n\nnom\n\n\n\n\nCandidat\n\n\nJunior\n\n\nJUMBONG\n\n\n\nDéroulement de l’Entretien :\nL’entretien du jeudi, 30 novembre pour le poste de data scientist - Auditeur a duré 1 heure et a eu pour objectif d’évaluer les compétences du candidat et la compréhension du poste de stage.\n\nParcours de l’Étudiant :\n\nLa discussion a débuté par un tour de table permettant à chacun de se présenter. Le candidat a notamment présenté son parcours, et mis en avant quelques compétences acquises durant ses différents projets universitaires.\n\nPrésentation de Crédit Agricole CACIB :\n\nCrédit Agricole CACIB est la banque de financement et d’investissement du groupe Crédit Agricole. Elle offre une gamme étendue de produits et services en banque d’investissement, financements structurés, banque commerciale, et marchés financiers.\n\nMissions du Stage :\n\nÉquipe Méthode Support - Data Scientist :\nUne première mission dans l’équipe méthode support où le candidat, s’il est retenu, travaillera sur la modélisation des risques de la banque. Il devra utiliser les compétences théoriques et pratiques acquises lors des formations précédentes, notamment en clustering et modélisation statistique.\nMission d’Inspection - Encadrement :\nLa deuxième mission en tant qu’inspecteur consistera à utiliser des techniques basées sur la donnée, l’analyse quantitative, et à proposer de nouvelles techniques afin de contribuer à la réussite de la mission. Les méthodologies utilisées, telles que la collecte de la donnée ,son exploration, sa préparation, son analyse ainsi que la diffusion des résultats, ont été précisément présentées.\n\n\nConclusion :\nLe processus de recrutement est actuellement en cours. Le candidat recevra une réponse finale d’ici quelques semaines, une fois que tous les entretiens seront terminés avec les autres candidats.\nIl lui a été demandé d’informer s’il est engagé dans d’autres processus de recrutement parallèle. Sa transparence est importante afin de permettre aux recruteurs d’adapter leurs réponses dans le cas échéant."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Menu",
    "section": "",
    "text": "Modélisation de la value at risk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMaitrise des données financières avec Python\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning QCM\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nLes intruments financiers à un bébé\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nExamen de machine learning\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nCompte rendu de l’entretien pour le poste data scientist - Auditeur du jeudi, 30 novembre 2023.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nTBATS : N’a pas de contraintes de saisonnalité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nUn entretien raté\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nLeadership : Pouvoir, autorité et légitimité\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nMarket risk generalities\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\nIA : Eléments clés du premier sommet mondial\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJumbong Junior\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ClimateScenario/index.html",
    "href": "posts/ClimateScenario/index.html",
    "title": "Analyse de scénario climatique : une illustration des impacts économiques et sur les marchés financiers à long terme potentiels.",
    "section": "",
    "text": "Risques physiques et climatiques\n\n\n\nObjectif :\nL’objectif est d’intégrer les données climatiques dans le cadre d’allocation d’actifs.\nIl s’agit plus simplement de comprendre comment intégrer les scénarions climatiques et les risques climatiques dans les projections financières futures.\nComprendre comment ceux-ci impactent les performances financières d’investissement.\nCet article de Luca Bongiorno et al. met à lumière les impacts potentiels du changement climatique sur les marchés financiers, en se concentrant sur leurs effets à long terme.\nElle utilise un outil de modélisation top-down dévéloppé par Ortec finance qui intégre la science climatique aux effets macroéconomiques et financiers pour examiner les impacts possibles de trois scénarios plausible et non extrèmes.\nL’article analyse l’impact sur le PIB, il en ressort une réduction de PIB dans les trois scénarios, avec un impact très sévère dans le cadre d’une transition ratée où les objectifs climatiques de l’accord de Paris ne sont pas atteints.\nL’impact sur les marchés financiers est également analysé. Il en ressort une baisse des rendements cumulés des actions mondiales\nGIEC : groupe d’experts intergouvernemental sur l’évolution du climat.\nCes risques climatiques commencent à se manifester à travers les changements de conditions climatiques(variations de température, régimes pluviométriques, etc.), l’élévation du niveau de la mer, les conditions météorologiques extrêmes, les incendies de forêt, les sécheresses, les inondations, les ouragans, les cyclones, les tempêtes de neige, les vagues de chaleur, les vagues de froid, etc. Ces événements ont des impacts négatifs significatifs et pertubent divers secteurs notamment l’agriculture, la sylviculture, l’énergie, les infrastructures.\nLes actions menant à limiter l’ampleur du changement climatique en reduisant les émissions de gaz à effet de serre, principalement en diminuant l’utilisation de l’énergie fossile, en utilisant l’énergie renouvelable.\nLe changement climatique est un risque systémique car il affecte tous les secteurs de l’économie de la même manière et en même temps.\nLes changements climatiques posent des risques pour l’économie et le système financier.\nLes risques climatiques sont classés en deux catégories : les risques physiques et les risques de transition.\nles risques physiques découlent des changements des conditions climatiques et des événements météorologiques extrêmes. Ils peuvent être aigus ou chroniques.\nles risques de transition résultent du passage à une économie à faible émission de carbone, à l’utilisation d’une technologie propre et à l’adoption de pratiques commerciales durables. Ils peuvent être liés à la politique, à la technologie, à la marché et aux risques légaux.\nBonjour, merci de me donner l’opportunité de m’exprimer aujourd’hui. Je m’appelle Junior Jumbong. Mon intérêt"
  },
  {
    "objectID": "posts/forumEnsai/index.html",
    "href": "posts/forumEnsai/index.html",
    "title": "Un entretien raté",
    "section": "",
    "text": "entretien"
  },
  {
    "objectID": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "href": "posts/forumEnsai/index.html#comment-raté-son-entretien-de-stage",
    "title": "Un entretien raté",
    "section": "Comment raté son entretien de stage ?",
    "text": "Comment raté son entretien de stage ?\nAujourd’hui, a eu lieu le forum des ingénieurs de l’ENSAI où tout étudiant rêve de décrocher un stage. C’est l’occasion pour les étudiants de discuter avec les entreprises de leur parler d’eux.\nIl y’a une première étape, où les étudiants doivent s’inscrire sur une platform pour facililer la prise de rendez-vous. Ensuite, les étudiants doivent se présenter à l’heure à l’entretien. Ce n’est pas grave si un étudiant n’arrive pas à décrocher un entretien, il peut toujours se présenter à l’entreprise pour discuter avec les recruteurs.\nJ’ai pu obtenir quelques entretiens\nJe me suis lévé aujourd’hui à 9 heures, j’ai pris mon petit déjeuné, je me suis habillé, direction l’ENSAI. Enfin d’être présentable, j’ai porté une chémise.\nQuand je suis arrivé, j’ai localisé les différent"
  },
  {
    "objectID": "posts/Leadership/index.html",
    "href": "posts/Leadership/index.html",
    "title": "Leadership : Pouvoir, autorité et légitimité",
    "section": "",
    "text": "Leadership : Pouvoir, autorité et légitimité\n\n\n\nLe Pouvoir se reçoit, l’autorité se construit.\nComme l’éminent penseur Christian Monjou, débutons avec une citation marquante. Niccolò Machiavel, une figure clé dans l’étude du leadership et de la stratégie politique, affirmait : ‘Les qualités nécessaires pour conserver le pouvoir ne sont pas les mêmes que celles pour l’acquérir.’ Cette citation soulève un point crucial dans le domaine de la gestion du leadership et de l’autorité en entreprise. Lorsqu’on crée une entreprise, devenir un patron ou un chef d’entreprise confère du pouvoir, mais pas nécessairement de l’autorité. L’autorité, dans le contexte de la leadership efficace, est une reconnaissance octroyée par autrui ; elle repose sur la légitimité perçue par les collaborateurs. Un leader en cours de construction de son autorité n’est pas automatiquement reconnu comme légitime. En effet, la légitimité en leadership et en gestion d’équipe se manifeste à travers le respect et la confiance que l’on inspire à son entourage. Souvent, ceux qui doivent affirmer leur propre légitimité en sont en réalité dépourvus. La construction de l’autorité est donc un élément essentiel à l’efficacité du leadership dans le monde des affaires et la gestion d’équipe.\n\n\nLa construction de l’autorité\nL’essence du leadership transformationnel réside dans la capacité à favoriser la croissance et l’épanouissement de la communauté à laquelle on appartient. Cela implique d’encourager et de soutenir le développement personnel et professionnel des membres de l’équipe. Le concept de développement personnel est crucial dans le cadre du leadership efficace. En effet, un leader qui cesse de s’investir dans son propre développement personnel risque de perdre la légitimité aux yeux de ses collaborateurs. Dans la construction de l’autorité, la notion de style personnel joue également un rôle clé. Un style de leadership rigide ou répétitif peut s’avérer contre-productif. Il est important de rester adaptable et innovant, quelle que soit la situation. Cela soulève une question fondamentale : le leadership est-il inné ou acquis ? La réponse réside souvent dans la capacité d’un individu à évoluer et à s’adapter continuellement, renforçant ainsi son autorité et son influence au sein de son organisation.\n\n\nL’origine du leadership\nLa question de savoir si le leadership est inné ou acquis est un débat central dans le domaine du développement du leadership. Bien qu’il puisse sembler que certaines personnes naissent leaders, cette idée comporte des risques. En effet, se reposer uniquement sur des compétences innées en leadership peut conduire à une stagnation, limitant l’innovation et l’adaptabilité. Comme le soulignait le théoricien du leadership, Berson, ‘Plaquer du mécanique sur du vivant, c’est risquer le ridicule.’ Un leader efficace ne se contente pas de compter sur des compétences innées, mais s’interroge continuellement sur l’adéquation de son style de leadership avec les besoins actuels de sa communauté ou de son organisation. Un exemple éloquent est celui du leadership féminin, où l’adaptabilité et l’empathie jouent souvent un rôle clé dans la réussite et l’impact du leadership.\n\n\nElisabeth I er\nLa figure que vous voyez est celle d’Elisabeth Ière elisabeth., reine d’Angleterre ayant régné durant 45 ans (de 1558 à 1603). Fille de Henri VIII et Anne Boleyn, elle fut la dernière souveraine de la dynastie des Tudors et la première femme à régner sur l’Angleterre et l’Irlande. Son ascension au trône, perçue initialement comme un coup du hasard, est un exemple remarquable de la manière dont un leader peut transformer une opportunité imprévue en un règne influent et significatif. Sa présence imposante, souvent décrite comme une ‘hypertrophie de signe’, reflétait une stratégie de communication visant à établir sa légitimité et son autorité. En effet, Elisabeth Ière comprenait l’importance de l’image et du symbolisme dans la construction du leadership. Elle incarnait la dualité du ‘signe et du sens’ – un aspect essentiel dans la représentation du leadership. Son règne illustre parfaitement la nécessité pour un leader de maintenir un équilibre entre son identité privée et sa persona publique. Finalement, l’efficacité de son leadership résidait dans sa capacité à maîtriser l’art de la communication – un aspect crucial pour tout leader aspirant à un impact durable.\n\n\nLa qualité de la parole\nDans le contexte actuel de l’éducation et du leadership, une transformation notable s’est opérée par rapport aux méthodes traditionnelles. Autrefois, l’entrée d’un enseignant dans une salle de classe était synonyme de respect absolu et d’une reconnaissance incontestée de son savoir. Cependant, dans le monde moderne, marqué par l’accès instantané à l’information via la technologie numérique, cette dynamique a évolué. Imaginez un professeur mentionnant un pourcentage spécifique, comme 2.23 %, devant des étudiants équipés de smartphones. Il est probable qu’ils vérifient immédiatement cette information, remettant en question l’exactitude des données présentées. Cette situation illustre un changement fondamental : la légitimité dans le domaine de l’éducation et du leadership ne repose plus exclusivement sur la transmission du savoir, mais de plus en plus sur la qualité de la communication et de l’engagement. L’ère numérique, en économisant du temps, nous incite non pas à intensifier notre présence numérique, mais plutôt à valoriser davantage le temps consacré à la communication directe et significative. Selon Marcel Mauss, ce principe d’échange symbolique, où la qualité de la communication exige en retour un engagement – ici sous forme de loyauté – est essentiel. Enfin, la vraie mesure d’un leader réside dans sa capacité à anticiper les idées potentiellement nuisibles à sa communauté et à persuader celle-ci d’adopter de nouvelles perspectives pour éviter les pièges futurs. Mener une communauté du connu vers l’inconnu est une des responsabilités les plus ardues d’un leader.\n\n\nUn monde de changement\nNous vivons dans une ère de transformation rapide, caractérisée par un marché mondial en constante évolution. À l’ère numérique et face aux défis de la transition écologique, les communautés et organisations qui ne reconnaissent pas ce changement de paradigme risquent l’obsolescence. Cette transformation s’illustre particulièrement avec l’avènement de l’intelligence artificielle, comme en témoigne le développement de technologies telles que ChatGPT. De plus, les appels urgents du GIEC et des communautés scientifiques, notamment des astrophysiciens, mettent en lumière l’imminence d’une crise écologique mondiale. Il est impératif pour les entreprises, et plus spécifiquement pour les banques, d’évaluer et de gérer ces risques. L’enjeu n’est pas simplement financier, mais il s’agit de la préservation de la vie sur Terre dans toutes ses formes. La responsabilité sociale des entreprises et la durabilité environnementale sont devenues des facteurs clés dans la stratégie d’entreprise moderne, dictant une nouvelle approche vers un avenir plus durable et conscient."
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html",
    "href": "posts/machineLearning3A/QCM.html",
    "title": "Examen de machine learning QCM",
    "section": "",
    "text": "qcm"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q1-what-is-the-primary-objective-of-empirical-risk-minimization-erm-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?",
    "text": "Q1: What is the primary objective of Empirical Risk Minimization (ERM) in machine learning?\n\n\nMinimize training error\n\n\nMinimize testing error\n\n\nMinimize a combination of training and testing error\n\n\nMaximize model complexity\n\n\nRéponse: C) Minimize training error"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "href": "posts/machineLearning3A/QCM.html#q2-in-ridge-regression-what-does-the-regularization-term-primarily-aim-to-prevent",
    "title": "Examen de machine learning QCM",
    "section": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?",
    "text": "Q2: In Ridge regression, what does the regularization term primarily aim to prevent?\n\n\nFeature selection\n\n\nOverfitting\n\n\nUnderfitting\n\n\nData preprocessing errors\n\n\nRéponse: B) Overfitting"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "href": "posts/machineLearning3A/QCM.html#q3-which-of-the-following-regression-techniques-is-primarily-used-for-feature-selection-by-adding-a-penalty-term-to-the-loss-function",
    "title": "Examen de machine learning QCM",
    "section": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?",
    "text": "Q3: Which of the following regression techniques is primarily used for feature selection by adding a penalty term to the loss function?\n\n\nLinear regression\n\n\nRidge regression\n\n\nLasso regression\n\n\nSupport vector regression\n\n\nRéponse: C) Lasso regression"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "href": "posts/machineLearning3A/QCM.html#q4-which-optimization-technique-aims-to-find-the-minimum-of-a-convex-function-by-iteratively-updating-the-model-parameters-in-the-direction-of-the-gradient",
    "title": "Examen de machine learning QCM",
    "section": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?",
    "text": "Q4: Which optimization technique aims to find the minimum of a convex function by iteratively updating the model parameters in the direction of the gradient?\n\n\nGradient Descent\n\n\nNewton’s Method\n\n\nGenetic Algorithms\n\n\nDecision Trees\n\n\nRéponse: A) Gradient Descent"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "href": "posts/machineLearning3A/QCM.html#q5-in-support-vector-machines-what-is-the-primary-goal-when-selecting-the-optimal-hyperplane",
    "title": "Examen de machine learning QCM",
    "section": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?",
    "text": "Q5: In support vector machines, what is the primary goal when selecting the optimal hyperplane?\n\n\nMaximize the margin between data points from different classes.\n\n\nMinimize the margin between data points from the same class.\n\n\nMaximize the number of support vectors.\n\n\nMinimize the number of support vectors.\n\n\nRéponse: A) Maximize the margin between data points from different classes."
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "href": "posts/machineLearning3A/QCM.html#q6-in-ridge-regression-what-does-the-regularization-term-penalize",
    "title": "Examen de machine learning QCM",
    "section": "Q6: In Ridge regression, what does the regularization term penalize?",
    "text": "Q6: In Ridge regression, what does the regularization term penalize?\n\n\nThe magnitude of the coefficients\n\n\nThe number of features\n\n\nThe mean squared error\n\n\nThe bias of the model\n\n\nRéponse: A) The magnitude of the coefficients"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "href": "posts/machineLearning3A/QCM.html#q7-which-of-the-following-machine-learning-algorithms-is-specifically-designed-for-binary-classification-and-uses-a-hyperplane-to-separate-data-points",
    "title": "Examen de machine learning QCM",
    "section": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?",
    "text": "Q7: Which of the following machine learning algorithms is specifically designed for binary classification and uses a hyperplane to separate data points?\n\n\nPrincipal Component Analysis (PCA)\n\n\nK-Means Clustering\n\n\nSupport Vector Machine (SVM)\n\n\nK-Nearest Neighbors (KNN)\n\n\nRéponse: C) Support Vector Machine (SVM)"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q8-which-of-the-following-statements-is-true-about-the-bias-variance-trade-off-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?",
    "text": "Q8: Which of the following statements is true about the bias-variance trade-off in machine learning?\n\n\nIncreasing model complexity reduces bias and increases variance\n\n\nIncreasing model complexity increases both bias and variance\n\n\nDecreasing model complexity reduces bias and increases variance\n\n\nDecreasing model complexity reduces both bias and variance\n\n\nRéponse: A) Increasing model complexity reduces bias and increases variance"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "href": "posts/machineLearning3A/QCM.html#q9-what-is-the-main-advantage-of-using-a-kernel-trick-in-support-vector-machines",
    "title": "Examen de machine learning QCM",
    "section": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?",
    "text": "Q9: What is the main advantage of using a kernel trick in Support Vector Machines?\n\n\nIt reduces overfitting\n\n\nIt simplifies the optimization problem\n\n\nIt allows SVM to handle non-linear data\n\n\nIt speeds up the training process\n\n\nRéponse: C) It allows SVM to handle non-linear data"
  },
  {
    "objectID": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "href": "posts/machineLearning3A/QCM.html#q10-what-is-the-primary-purpose-of-cross-validation-in-machine-learning",
    "title": "Examen de machine learning QCM",
    "section": "Q10: What is the primary purpose of cross-validation in machine learning?",
    "text": "Q10: What is the primary purpose of cross-validation in machine learning?\n\n\nTo train a model on multiple datasets\n\n\nTo select the best hyperparameters for a model\n\n\nTo overfit the model to the training data\n\n\nTo evaluate a model’s performance on the training data\n\n\nRéponse: B) To select the best hyperparameters for a model"
  },
  {
    "objectID": "posts/MasteringFinancialData/index.html",
    "href": "posts/MasteringFinancialData/index.html",
    "title": "Maitrise des données financières avec Python",
    "section": "",
    "text": "Introduction\nLes données financières sont très utilisées dans la filière gestion des risques de l’ENSAI. Avoir rapidement accès est un atout pour les étudiants et les fera gagner du temps. Ces données sont utilisées dans plusieurs cours notamment le cours de séries temporelles, le cours de la théorie de gestion des risques multiples,le cours d’asset pricing, etc. C’est pourquoi j’ai décidé de partager avec vous quelques astuces pour maitriser les données financières avec Python. Nous utiliserons la librairie yfinance pour récupérer les données financières et pandas pour les manipuler et matplotlib pour les visualiser.\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nJ’ai importé la library datetime pour manipuler les dataes. Maintenant, nous pouvons importer les données.\n\n\nImporter les données\n\nimport yfinance as yf\n\naapl = yf.download('AAPL', \n                      start='2012-01-01', \n                      end='2024-01-01',)\naapl.head()                    \n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\nLe code ci-dessus utilise la fonction downloadde la librairir yfinance pour télécharger les données de la société Apple (AAPL) de 2012 à 2024. De la même manière, vous pouvez télécharger les données d’autres sociétés. Par exemple, pour accèder aux données du CAC40, vous pouvez utiliser le code suivant:\n\ncac40 = yf.download('^FCHI', \n                      start='2012-01-01', \n                      end='2024-01-01',)\ncac40.head()\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2012-01-03\n3231.429932\n3246.739990\n3193.629883\n3245.399902\n3245.399902\n123415200\n\n\n2012-01-04\n3227.459961\n3242.840088\n3186.479980\n3193.649902\n3193.649902\n114040800\n\n\n2012-01-05\n3197.159912\n3200.149902\n3136.750000\n3144.909912\n3144.909912\n121161600\n\n\n2012-01-06\n3156.419922\n3184.379883\n3122.629883\n3137.360107\n3137.360107\n104492800\n\n\n2012-01-09\n3143.949951\n3157.310059\n3114.449951\n3127.689941\n3127.689941\n96976800\n\n\n\n\n\n\n\nNous savons que pour les données de marché, nous avons les colonnes suivantes: Open, High, Low, Close, Adj Close, Volume. Nous allons maintenant travailler avec la colonne Close qui représente le prix de clôture de l’action. Nous pouvons maintenant visualiser les données.\n\nplt.figure(figsize=(10, 6))\nplt.plot(aapl['Close'], label='AAPL')\nplt.title('Prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Prix de clôture')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCes données ne sont pas stationnaires. Généralement, pour les données financières, nous travaillons avec les rendements car ils sont stationnaires. Si \\(P_t\\) est le prix de l’action à la date \\(t\\), le rendement à la date \\(t\\) est donné par: \\[\nr_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\]\nest équivalent à\n\\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right) = \\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\nDonc nous travaillons avec le logarithme des rendements. Nous pouvons maintenant calculer les rendements. Il existe une fonction pct_change dans la librairie pandas qui permet de calculer les rendements. Nous allons utiliser cette fonction combinée avec la fonction log de la librairie numpy pour calculer les logarithmes des rendements du prix de clôture de l’action AAPL.\n\ndaily_close = aapl[['Close']]\ndaily_close_returns = daily_close.pct_change().apply(lambda x: np.log(1+x))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nDans le code ci-dessus nous avons utilisé l’expression ci-dessous pour calculer les rendements: \\[\n\\log\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right)\n\\]\nNous pouvons plutôt utiliser la fonction log de la librairie numpy pour calculer les rendements à partir de l’expression ci-dessous: \\[\n\\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\n\ndaily_close_returns = np.log(daily_close / daily_close.shift(1))\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-03\nNaN\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n\n\n\n\n\nNous avons les mêmes résultats avec la première valeur qui est NaN. Nous pouvons supprimer cette valeur.\n\ndaily_close_returns = daily_close_returns.dropna()\ndaily_close_returns.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-04\n0.005360\n\n\n2012-01-05\n0.011041\n\n\n2012-01-06\n0.010400\n\n\n2012-01-09\n-0.001587\n\n\n2012-01-10\n0.003574\n\n\n\n\n\n\n\nNous pouvons maintenant visualiser les rendements.\n\ndaily_close_returns.plot(figsize=(10, 6))\nplt.title('Logarithme des rendements du prix de clôture de l\\'action AAPL')\nplt.xlabel('Date')\nplt.ylabel('Logarithme des rendements')\nplt.show()\n\n\n\n\n\n\n\n\nSi vous voulez, vous pouvez étudier les statistiques descriptives des rendements. Une fonction que j’adore qui donne les statistiques sommaire est la fonction describe de la librairie pandas.\n\ndaily_close_returns.describe()\n\n\n\n\n\n\n\n\nClose\n\n\n\n\ncount\n3017.000000\n\n\nmean\n0.000853\n\n\nstd\n0.017967\n\n\nmin\n-0.137708\n\n\n25%\n-0.007562\n\n\n50%\n0.000765\n\n\n75%\n0.010275\n\n\nmax\n0.113157\n\n\n\n\n\n\n\nNous pouvons aussi rééchantillonner les données pour avoir les log rendements minimums, maximums, moyens, etc. par semaine, par mois, par trimestre,etc. Par exemple pour avoir les log rendements moyens par semaine, nous pouvons utliser la fonction resample de la librairie pandas combinée avec la fonction mean pour avoir les moyennes.\n\nweekly = daily_close_returns.resample('W').mean()\nweekly.head()\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2012-01-08\n0.008933\n\n\n2012-01-15\n-0.001230\n\n\n2012-01-22\n0.000292\n\n\n2012-01-29\n0.012443\n\n\n2012-02-05\n0.005469\n\n\n\n\n\n\n\nJe vais terminer ce poste par vous montrer comment télécharger les données de plusieurs sociétés. Par exemple, pour télécharger les données de Apple, Microsoft, Google et Amazon, vous pouvez utiliser le code suivant:\n\nimport yfinance as yf\ntickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n\ndef get_data(tickers, startdate, enddate):\n    def data(ticker):\n        return (yf.download(ticker, start=startdate, end=enddate))\n    datas = map(data, tickers)\n    return(pd.concat(datas,keys= tickers, names=['Ticker', 'Date']))\nall_data = get_data(tickers, '2012-01-01', '2024-01-01')\nall_data.head()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nTicker\nDate\n\n\n\n\n\n\n\n\n\n\nAAPL\n2012-01-03\n14.621429\n14.732143\n14.607143\n14.686786\n12.433825\n302220800\n\n\n2012-01-04\n14.642857\n14.810000\n14.617143\n14.765714\n12.500644\n260022000\n\n\n2012-01-05\n14.819643\n14.948214\n14.738214\n14.929643\n12.639428\n271269600\n\n\n2012-01-06\n14.991786\n15.098214\n14.972143\n15.085714\n12.771556\n318292800\n\n\n2012-01-09\n15.196429\n15.276786\n15.048214\n15.061786\n12.751299\n394024400\n\n\n\n\n\n\n\n\n\nConclusion\nJ’espère que ce poste vous sera utile. Si vous avez des questions, n’hésitez pas à me contacter si chatgpt ne peut pas vous aider.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Mastering Financial Data"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique",
    "title": "Modélisation de la value at risk",
    "section": "4.1 VaR historique :",
    "text": "4.1 VaR historique :\nIci on estime la distribution des rendements R par la fonction de répartition empirique du vecteur d’observations. La VaR est alors donnée par le quantile empirique d’ordre \\(1-\\alpha\\) :\n\\(\\hat{VaR}_h(\\alpha) = \\hat{F_n^{-1}}(1-\\alpha)\\) avec \\(\\hat{F_n}(1-\\alpha)= \\frac{1}{n} \\sum_{i=1}^{n} 1_{R_i \\leq (1-\\alpha)}\\)\nNous utiliserons ainsi la fonction numpy.percentile pour calculer la VaR historique.\n\ndef hist_var(returns, index, fenetre, seuil):\n    \"\"\"Cette fonction calcule la Value at Risk (VaR) historique d'une série temporelle de log rendements\n\n    Args:\n        returns (numpy_array ): serie de log rendements\n        index (int): indice maximal de la série à considérer pour le calcul\n        fenetre (int): nombre de jours sur lesquels on calcule la VaR\n        seuil (float): niveau de confiance de la VaR\n\n    Return:\n        float: VaR historique\n    \"\"\"\n    return np.percentile(returns[index-fenetre:index], 100*(1-seuil))\n\n\nvar_hist= hist_var(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_hist)\n\n-0.04320825141346711",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting",
    "title": "Modélisation de la value at risk",
    "section": "4.2 Backtesting",
    "text": "4.2 Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist for i in range(test_size)], label=\"Non parametric VaR\", color = 'red')\nlist_exceptions_np = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist]\nplt.scatter(test_close.index[list_exceptions_np], test_close['log_return'][list_exceptions_np], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR non paramétrique est: {len(list_exceptions_np)/test_size}\")\n\nLe nombre d'exceptions pour la VaR non paramétrique est: 10\nLe pourcentage d'exceptions pour la VaR non paramétrique est: 0.004640371229698376\n\n\nNous allons maintenant vérifier si la probabilité d’exception est statiquement égale à \\(\\alpha\\).\n\nfrom scipy import stats\n\ntest_except_np = stats.binomtest(len(list_exceptions_np), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np.pvalue}')\n\nla p-value du test binomial est: 0.008950138322753154\n\n\nLa pvalue du test est inférieure au seuil de 5%. On rejette donc l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR n’est pas satisfaisante.\n\nES_np = np.mean([r for r in train_close['log_return'] if r&lt;var_hist])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.054387464763168906",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-historique-bootstrap",
    "title": "Modélisation de la value at risk",
    "section": "4.3 VaR historique Bootstrap",
    "text": "4.3 VaR historique Bootstrap\nPour la VaR historique boostrap, nous allons construire B réplications bootstrap de la série des log rendements. Pour chaque réplication(b), nous allons calculer la VaR historique. La VaR historique bootstrap est alors donnée par la moyenne des VaR historiques des B réplications bootstrap.\n\\(\\hat{VaR}_{h,bootstrap}(\\alpha) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{VaR}_{h,b}(\\alpha)\\)\nNous avons aussi calculé l’intervalle de confiance à 95% de la VaR historique bootstrap. Cet intervalle est donné par les quantiles empiriques d’ordre 2.5% et 97.5% des VaR historiques des B réplications bootstrap.\n\n\ndef VaR_Hist_Bootstrap(returns, seuil, num_simulations, alpha_IC, n_B):\n\n\n    VaRs_boot = np.zeros(num_simulations)\n\n    for i in range(num_simulations):\n        sample = np.random.choice(returns, n_B, replace=True)\n        VaRs_boot[i] = hist_var(sample, len(sample), len(sample), seuil)\n\n    VaR = np.mean(VaRs_boot)\n\n    lower_bound = np.percentile(VaRs_boot, 100 * (1-alpha_IC) / 2)\n    upper_bound = np.percentile(VaRs_boot, 100 * (1 - (1-alpha_IC) / 2))\n    IC = (lower_bound, upper_bound)\n\n    return VaR, IC\n\n\nseuil = 0.99\nseuil_IC = 0.9\nnum_simulations = 5000\nn_B = 251*10 #on utilise 10 ans de données comme taille d'échantillon bootstrap\nvar_hist_boot, IC_hist_boot = VaR_Hist_Bootstrap(train_close[\"log_return\"], seuil, num_simulations,\n                                                 seuil_IC, n_B) \nprint(f\"La VaR historique bootstrap: {var_hist_boot}\")\nprint(f\"L'intervalle de confiance associé est: {IC_hist_boot}\")\n\nLa VaR historique bootstrap: -0.039930947829674754\nL'intervalle de confiance associé est: (-0.043415059657731805, -0.03610734170316266)\n\n\nNotre estimation bootstrap de la VaR se trouve bien dans l’intervalle de confiance à 90%. Nous pouvons donc conclure que notre estimation de la VaR est satisfaisante.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-1",
    "title": "Modélisation de la value at risk",
    "section": "4.4 Backtesting",
    "text": "4.4 Backtesting\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_hist_boot for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_hist_boot]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['log_return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: {len(list_exceptions_np_boot)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Bootstrap non paramétrique est: 16\nLe pourcentage d'exceptions pour la VaR Bootstrap non paramétrique est: 0.007424593967517401\n\n\n\ntest_except_np_boot = stats.binomtest(len(list_exceptions_np_boot), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_np_boot.pvalue}')\n\nla p-value du test binomial est: 0.2775641662941861\n\n\nLe test binomial vient confirmer les conclusions faites à partir de l’intervalle de confiance estimé.\n\nES_np_boot = np.mean([r for r in train_close['log_return'] if r&lt;var_hist_boot])\nprint(f\"L'Expected Shortfall associée à la VaR calculée est: {ES_np_boot}\")\n\nL'Expected Shortfall associée à la VaR calculée est: -0.05028777316523479",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#var-gaussienne",
    "title": "Modélisation de la value at risk",
    "section": "5.1 VaR gaussienne",
    "text": "5.1 VaR gaussienne\nLa VaR gaussienne suppose que les rendements suivent une loi normale. Dans ce cas, on a \\(P(R&lt;VaR_h(\\alpha)) = 1-\\alpha\\) qui est équivalent à \\(P(\\frac{R-\\mu}{\\sigma} &lt; \\frac{VaR_h(\\alpha)-\\mu}{\\sigma}) = 1-\\alpha\\) en supposant que R suit une loi normale de moyenne \\(\\mu\\) et d’écart type \\(\\sigma\\). On a alors \\(VaR_h(\\alpha) = \\mu + \\sigma \\Phi^{-1}(1-\\alpha)\\). On peut estimer les paramètres à partir de l’échantillon. On a alors \\(\\hat{VaR}_h(\\alpha) = \\hat{\\mu} + \\hat{\\sigma} \\Phi^{-1}(1-\\alpha)\\).\n\nfrom scipy import stats\ndef var_gaussienne(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR = mean_returns + sd_returns * stats.norm.ppf(1-seuil)\n    return VaR\n\n\n## VaR gaussienne sur base d'apprentissage\n\nvar_gaus = var_gaussienne(train_close[\"log_return\"], train_size, 251*10, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_gaus)\n\n-0.034733067424936016\n\n\n\n5.1.1 Validation\n\n# analyse graphique avec les densités des distributions\nplt.figure(figsize = (12,8))\nplt.hist(train_close[\"log_return\"], bins=30, density=True, color='blue', label = 'log rendements')\nplt.axvline(var_gaus, color='red', linestyle='dashed', linewidth=2, label=f'VaR Gaussienne ({var_gaus})')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"]))\nplt.plot(x, p, label = 'Loi normale', linewidth=2)\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOn remarque que la queue de la loi normale n’est pas assez lourde pour nos données. La loi normale aura donc tendance à mal estimer les queues de distribution.\n\n## Analyse graphique avec le QQ-plot\n\nplt.figure(figsize=(12, 8))\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\nNous constatons que lorsqu’on utilise la loi normale pour la modélisation de la VaR, au niveau des queues de distribution,les quantiles théoriques sont moins élévés que les quantiles empiriques à gauche et plus élevés à droite. Cela signifie que la loi normale sous-estime la probabilité d’exception. La loi normale semble donc ne pas être adaptée pour modéliser la VaR.\n\n## Test d'adéquation\n\n# Test de Kolmogorov-Smirnov\nks_statistic, ks_p_value = stats.kstest(train_close[\"log_return\"], 'norm', args = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])))\nprint(f\"Test de Kolmogorov-Smirnov - Statistique : {ks_statistic},\\nP-value : {ks_p_value}\")\n\nTest de Kolmogorov-Smirnov - Statistique : 0.05681055751622879,\nP-value : 1.4266789934088429e-18\n\n\nOn rejette l’hypothèse nulle selon laquelle les log rendements suivent une distribution normale.\n\n\n5.1.2 Représentation graphique\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_gaus for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_gaus]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['log_return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.1.3 Analyse des exceptions\n\nprint(f\"Le nombre d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR gaussienne est: {len(list_exceptions_gaus)/test_size}\")\n\nLe nombre d'exceptions pour la VaR gaussienne est: 25\nLe pourcentage d'exceptions pour la VaR gaussienne est: 0.01160092807424594\n\n\n\ntest_except_gaus = stats.binomtest(len(list_exceptions_gaus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus.pvalue}')\n\nla p-value du test binomial est: 0.44697408691382107\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%.\n\n\n5.1.4 VaR gaussienne à 10 jours\n\n## Var gaussienne à 10 jours par la méthode de scaling\n\nperiode = 10\nvar_gaus_scaling = np.sqrt(periode)*var_gaus\nprint(f\"La VaR gaussienne à 10 jours par la méthode de scaling est: {var_gaus_scaling}\")\n\nLa VaR gaussienne à 10 jours par la méthode de scaling est: -0.10983560318699723\n\n\n\n## VaR gaussienne à 10 jours par la méthode de diffusion\nfrom numpy import random\n\nperiode = 10\nn_simul = 10000\nS0 = train_close['Close'].iloc[-1]\nmean_returns = np.mean(train_close[\"log_return\"])\nsd_returns = np.std(train_close[\"log_return\"])\nsimulations = []\nfor k in range(n_simul):\n    simul_k=[S0]\n    for _ in range(periode):\n        Z = random.standard_normal()\n        dS = simul_k[-1]*mean_returns + simul_k[-1]*sd_returns*Z\n        simul_k.append(simul_k[-1]+dS)\n    simulations.append(simul_k)\n\nrend10 = [np.log(simul[10] / S0) for simul in simulations]\n\nalpha = 0.99\nvar_gaus_diff = np.percentile(rend10, 100*(1-alpha))\nprint(f\"La VaR gaussienne à 10 jours par la méthode de diffusion est: {var_gaus_diff}\")\n\nLa VaR gaussienne à 10 jours par la méthode de diffusion est: -0.10286418206833377\n\n\n\n\n5.1.5 VaR gaussienne pondérée :\nUne façon de corriger la VaR, est de pondérer la moyenne et l’écart type des rendements. On peut utiliser une moyenne mobile pondérée.\n\nfrom scipy import stats\ndef var_gaussienne_ewma(returns, index, fenetre, seuil, lambd):\n    rendements = returns[index-fenetre:index]\n    n = len(rendements)\n    poids = [(lambd**i)*(1-lambd) for i in range(n)]\n    denom = sum(poids)\n    poids_pond = [poid/denom for poid in poids]\n    moy_pond = np.sum([poids_pond[i]*rendements[n-i-1] for i in range(n)])\n    variance_pond = np.sum([poids_pond[i]*(rendements[n-i-1]-moy_pond)**2 for i in range(n)])\n    VaR = moy_pond + np.sqrt(variance_pond) * stats.norm.ppf(1-seuil)\n    return moy_pond, variance_pond, VaR\n\n\n## Calcul \n\nlambd1= 0.9\nlambd2 = 0.95\nlambd3 = 0.99\nmoy_pond1, variance_pond1, VaR1 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd1)\nmoy_pond2, variance_pond2, VaR2 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd2)\nmoy_pond3, variance_pond3, VaR3 = var_gaussienne_ewma(train_close[\"log_return\"], train_size, 251*10, 0.99, lambd3)\n\nlist_exceptions_gaus1 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR1]\nprint(100*\"_\"+ f\"\\nlambda = {lambd1}\\nMoyenne pondérée: {moy_pond1}\\nVariance pondérée: {variance_pond1}\\nVaR gaussienne EWMA: {VaR1}\\nNombre d'exceptions: {len(list_exceptions_gaus1)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus1)/test_size}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Premier graphique\naxes[0].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[0].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[0].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[0].plot(ts_close.index[train_size:], [VaR1 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd1})\", color='red')\naxes[0].scatter(test_close.index[list_exceptions_gaus1], [test_close['log_return'][i] for i in list_exceptions_gaus1], color='red', label='Exceptions')\naxes[0].set_title('Lambda = 0.9')\naxes[0].legend()\n\nlist_exceptions_gaus2 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR2]\nprint(100*\"_\"+ f\"\\nlambda = {lambd2}\\nMoyenne pondérée: {moy_pond2}\\nVariance pondérée: {variance_pond2}\\nVaR gaussienne EWMA: {VaR2}\\nNombre d'exceptions: {len(list_exceptions_gaus2)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus2)/test_size}\")\n\n# Deuxième graphique\naxes[1].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[1].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[1].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[1].plot(ts_close.index[train_size:], [VaR2 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd2})\", color='red')\naxes[1].scatter(test_close.index[list_exceptions_gaus2], [test_close['log_return'][i] for i in list_exceptions_gaus2], color='red', label='Exceptions')\naxes[1].set_title('Lambda = 0.95')\naxes[1].legend()\n\n\nlist_exceptions_gaus3 = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;VaR3]\nprint(100*\"_\"+ f\"\\nlambda = {lambd3}\\nMoyenne pondérée: {moy_pond3}\\nVariance pondérée: {variance_pond3}\\nVaR gaussienne EWMA: {VaR3}\\nNombre d'exceptions: {len(list_exceptions_gaus3)}\\nPourcentage d'exceptions: {len(list_exceptions_gaus3)/test_size}\")\n\n# Troisième graphique\naxes[2].plot(ts_close.index[0:train_size], train_close['log_return'], label=\"Historical train log returns\", color='gray')\naxes[2].axvline(x=ts_close.index[train_size - 251*10 - 1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\naxes[2].plot(ts_close.index[train_size:], test_close['log_return'], label=\"Historical test log returns\", color='blue')\naxes[2].plot(ts_close.index[train_size:], [VaR3 for _ in range(test_size)], label=f\"Gaussian EWMA VaR (lambda= {lambd3})\", color='red')\naxes[2].scatter(test_close.index[list_exceptions_gaus3], [test_close['log_return'][i] for i in list_exceptions_gaus3], color='red', label='Exceptions')\naxes[2].set_title('Lambda = 0.99')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n____________________________________________________________________________________________________\nlambda = 0.9\nMoyenne pondérée: -0.002655293241077962\nVariance pondérée: 0.0003655826690051906\nVaR gaussienne EWMA: -0.047135567638603576\nNombre d'exceptions: 8\nPourcentage d'exceptions: 0.0037122969837587007\n____________________________________________________________________________________________________\nlambda = 0.95\nMoyenne pondérée: -0.0024114946992036604\nVariance pondérée: 0.0003654872172300481\nVaR gaussienne EWMA: -0.04688596193096311\nNombre d'exceptions: 8\nPourcentage d'exceptions: 0.0037122969837587007\n____________________________________________________________________________________________________\nlambda = 0.99\nMoyenne pondérée: -0.0005003090834522245\nVariance pondérée: 0.00022779933004877785\nVaR gaussienne EWMA: -0.03561193003277455\nNombre d'exceptions: 23\nPourcentage d'exceptions: 0.010672853828306265\n\n\n\n\n\n\n\n\n\n\ntest_except_gaus1 = stats.binomtest(len(list_exceptions_gaus1), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus1.pvalue}')\n\nla p-value du test binomial est: 0.0015372581472268324\n\n\n\ntest_except_gaus2 = stats.binomtest(len(list_exceptions_gaus2), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus2.pvalue}')\n\nla p-value du test binomial est: 0.0015372581472268324\n\n\n\ntest_except_gaus3 = stats.binomtest(len(list_exceptions_gaus3), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_gaus3.pvalue}')\n\nla p-value du test binomial est: 0.7444680028961591\n\n\nSeule la VaR estimée avec \\(\\lambda\\) = 0.99 a une p value supérieure au seuil de 5%.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#comparaison-entre-densité-théorique-et-densité-empirique-de-la-skew-student-sur-les-log-rendements",
    "title": "Modélisation de la value at risk",
    "section": "7.1 Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements",
    "text": "7.1 Comparaison entre densité théorique et densité empirique de la skew-student sur les log rendements\n\n\nx_values = np.linspace(min(test_close['log_return']), max(test_close['log_return']), 1000)\n\nmu, sigma, skew, df =est_params\n\ntheoretical_density = SkStudentPdf(x_values, mu, sigma, skew, df)\nplt.figure(figsize = (10,8))\nplt.hist(test_close['log_return'], bins=30, density=True, alpha=0.5, label='Données empiriques')\n\nplt.plot(x_values, theoretical_density, label='Densité Skew Student', color='red')\n\n# Personnalisation du graphique\nplt.xlabel('Rendements')\nplt.ylabel('Densité')\nplt.title('Comparaison entre les données et la densité théorique')\nplt.legend()\n\n# Affichage du graphique\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densité théorique de la skew-student est très proche de la densité empirique des log rendements. Nous pouvons donc conclure que la skew-student est une bonne modélisation des log rendements.\n\n7.1.1 Fonction de repartition de la skew-student et fonction quantile\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\n\ndef integrale_SkewStudent(x):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: SkStudentPdf(x, mu_est, sigma_est, gamma_est, nu_est), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha):\n    value = integrale_SkewStudent(x)-alpha\n    return abs(value)\n\ndef theoretical_quantile(alpha):\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha))\n        return resultat_minimisation.x\n\n\nLe code ci-dessus nous a permis de construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l’inverse de cette fonction de repartition. Cette fonction quantile est le coeur de la modélisation de la VaR.\n\n\n7.1.2 QQ plot :\nLe graphique de QQ plot nous permettra de discuter de la qualité d’ajustement de la loi skew-student à la série des log rendements.\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\n\nquantiles_empiriques = np.quantile(train_close['log_return'], niveaux_quantiles)\nquantiles_theoriques = [theoretical_quantile(alpha) for alpha in niveaux_quantiles]\n\n\n\n# Créer le QQ plot\nplt.figure(figsize=(8, 8))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles théoriques')\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nNous constatons que la loi skew-student est une bonne approximation de la distribution des log rendements. Même si on constate des écarts aux queues de distribution, la loi skew-student semble bien modéliser la distribution des log rendements.\n\n\n7.1.3 comparaison entre loi gaussienne et loi de Skew Student\n\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nprobplot = stats.probplot(train_close[\"log_return\"], \n                        sparams = (np.mean(train_close[\"log_return\"]), np.std(train_close[\"log_return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.xlabel('Quantiles théoriques (distribution loi normale)')\nplt.ylabel('Quantiles empiriques')\nplt.title(\"QQ-plot d'une modélisation par loi normale\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi Skew Student\")\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nComme on peut le voir sur le graphique ci-dessus, la loi skew-student est une meilleure approximation de la distribution des log rendements que la loi normale. En effet, les écarts aux queues de distribution sont moins importants pour la loi skew-student que pour la loi normale.\n\n\n7.1.4 Calcul de la VaR Skew Student\n\ndef var_skewstudent(returns, index, fenetre, seuil):\n    rendements = returns[index-fenetre:index]\n    mean_returns = np.mean(rendements)\n    sd_returns = np.std(rendements)\n    VaR =  theoretical_quantile(1-seuil)\n    return VaR\n\n\n## VaR Skew Student sur base d'apprentissage\n\nvar_skew = var_skewstudent(train_close[\"log_return\"], train_size, train_size, 0.99) #on utilise 10 ans comme fenêtre\nprint(var_skew)\n\n-0.039652480749928415\n\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index[0:train_size], train_close['log_return'], label=\"historical train log returns\", color = 'gray')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.plot(ts_close.index[train_size:], test_close['log_return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], [var_skew for i in range(test_size)], label=\"VaR Skew Student\", color = 'red')\nlist_exceptions_skew = [i for i in range(len(test_close['log_return'])) if test_close['log_return'][i]&lt;var_skew]\nplt.scatter(test_close.index[list_exceptions_skew], test_close['log_return'][list_exceptions_skew], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR Skew student est: {len(list_exceptions_skew)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR Skew Student est: {len(list_exceptions_skew)/test_size}\")\n\nLe nombre d'exceptions pour la VaR Skew student est: 16\nLe pourcentage d'exceptions pour la VaR Skew Student est: 0.007424593967517401\n\n\n\ntest_except_skew = stats.binomtest(len(list_exceptions_skew), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_skew.pvalue}')\n\nla p-value du test binomial est: 0.2775641662941861\n\n\nLa p-value du test binomial est au dessus du seuil de 5%. On ne peut donc pas rejeter l’hypothèse selon laquelle la probabilité d’exception est de 1%. Le modèle VaR Skew Student estimé semble donc satisfaisant.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#commentaires",
    "title": "Modélisation de la value at risk",
    "section": "10.1 Commentaires",
    "text": "10.1 Commentaires\nNous avons estimé jusque là 10 valeurs différentes de la VaR. Ces différentes valeurs estimées correspondent à des approches différentes. Nous avons à chaque fois estimer une VaR 99% à horizon 1 jour. Le taux d’exceptions attendu afin de juger de la bonne qualité de la VaR est de 1%. - La VaR dynamique est particulière car, comme son nom l’indique, elle est dynamique donc pas constante au cours du temps. Elle a plus d’exceptions car elle est testée sur plus de données que les autres VaRs. Toutefois son taux d’exceptions se rapproche de celui des autres VaRs. - Le test binomial fournit une p-value inférieure au seuil de 5% pour les VaRs non paramétrique, Gaussienne EWMA (\\(\\lambda\\) = 0.9 et \\(\\lambda\\) = 0.95), dynamique et TVE par maxima par blocs. Ce qui sous entend que le taux d’exception est statistiquement différent du taux attendu de 1%. On en déduit donc que les modèles utilisés sont peu adéquats pour nos données. - La VaR non paramétrique et les VaR Gaussiennes EWMA pour \\(\\lambda\\) faible ont tendance à sous estimer la VaR tandis que la VaR dynamique et la VaR TVE par maxima par blocs ont tendance à sur estimer la vraie valeur de la VaR - De plus, la validation ex-ante de la VaR TVE POT nous indique une inadéquation entre les quantiles théoriques et les quantiles empiriques des excès. - Nous avons également montré que la VaR Skew Student semblait nettement plus valide que la VaR gaussienne. Toutefois, nous avons également estimé une amélioration de la VaR gaussienne qui est la VaR gaussienne pondérée par la méthode EWMA. - Les VARs qui nous semblent donc les plus pertinentes sont la VaR Bootstrap non paramétrique, la VaR gaussienne EWMA (\\(\\lambda\\)=0.99) et la VaR Skew Student.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#modélisation-du-garch-sur-les-résidus.",
    "title": "Modélisation de la value at risk",
    "section": "11.1 Modélisation du GARCH sur les résidus.",
    "text": "11.1 Modélisation du GARCH sur les résidus.\nNous allons appliquer un modèle GARCH(1,1) sur les résidus.\n\ngarch_final_model = arch_model(residus, vol='Garch', p=1, q=1).fit(disp='off')\nprint(garch_final_model.summary())\n\n                     Constant Mean - GARCH Model Results                      \n==============================================================================\nDep. Variable:                   None   R-squared:                       0.000\nMean Model:             Constant Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                19233.9\nDistribution:                  Normal   AIC:                          -38459.7\nMethod:            Maximum Likelihood   BIC:                          -38432.6\n                                        No. Observations:                 6462\nDate:                Sat, Feb 10 2024   Df Residuals:                     6461\nTime:                        21:16:42   Df Model:                            1\n                                 Mean Model                                 \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nmu         3.3553e-04  7.461e-06     44.969      0.000 [3.209e-04,3.502e-04]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9447e-06  9.286e-12  4.248e+05      0.000 [3.945e-06,3.945e-06]\nalpha[1]       0.1000  1.048e-02      9.539  1.442e-21   [7.945e-02,  0.121]\nbeta[1]        0.8800  8.851e-03     99.420      0.000     [  0.863,  0.897]\n============================================================================\n\nCovariance estimator: robust\n\n\nLes résultats du modèle GARCH(1,1) se trouvent dans le tableau ci-dessus. Nous pouvons constater que les paramètres sont tous significatifs. Nous allons maintenant vérifier la qualité d’ajustement du modèle GARCH(1,1) aux résidus.\n\n11.1.0.1 Analyse des résidus du modèle GARCH\nLa figure ci-dessous représente les différents graphiques : - Le graphique des résidus du modèle GARCH - La volatilité conditionnelle estimée - Le graphique des résidus standardisés\nLe résidu dont l’analyse de la blancheur nous intéresse est le résidu standardisé. Il est le résultat du rapport entre le résidu du modèle GARCH et la volatilité conditionnelle estimée. Nous allons donc vérifier si ce résidu est un bruit blanc. C’est ce résidu que nous allons utiliser pour modéliser la VaR.\n\nstd_residus = garch_final_model.resid / garch_final_model.conditional_volatility\nfig, axes = plt.subplots(3, 1, figsize=(15, 6))\n\n\naxes[0].plot(garch_final_model.resid)\naxes[0].set_title(\"Résidus\")\naxes[1].plot(garch_final_model.conditional_volatility)\naxes[1].set_title(\"Volatilité conditionnelle\")\naxes[2].plot(std_residus)\naxes[2].set_title(\"Résidus standardisés\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.1.1 Vérifions que les résidus de ce modèle GARCH sont des bruits blancs.\nNous allons faire le test de Ljung-Box pour vérifier l’autocorrélation des résidus. Et nous allons également faire ce test sur les carrés des résidus pour vérifier l’absence d’autocorrélation des résidus au carré d’où l’absence d’hetéroscedasticité.\n\n# test de Ljung-Box sur les résidus et les résidus au carré\n\ntest_lyungbow = acorr_ljungbox(std_residus, lags=[10], return_df=False)\nprint(test_lyungbow)\ntest_lyungbow_squared = acorr_ljungbox(std_residus**2, lags=[10], return_df=False)\nprint(test_lyungbow_squared)\n\n      lb_stat  lb_pvalue\n10  15.733691   0.107514\n     lb_stat  lb_pvalue\n10  9.253333   0.508242\n\n\nDans les deux cas, la p-value est supèrieur à 5%. Nous ne pouvons donc pas rejeter l’hypothèse nulle selon laquelle les résidus sont des bruits blancs. Nous pouvons donc conclure que le modèle GARCH(1,1) est satisfaisant. Nous pouvons faire d’autres tests pour vérifier la qualité de notre modèle tel que le test de Jarque-Bera et le QQ plot.\nLes estimations des paramètres du modèle AR(1)-GARCH(1,1) sont significatifs. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements. Les paramètres estimés sont : - alpha = 0.0858 - beta = 0.8978 - omega = 0.0322 - phi = -0.0092",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#estimation-des-paramètres-du-modèle-ar1-garch11.",
    "title": "Modélisation de la value at risk",
    "section": "11.2 Estimation des paramètres du modèle AR(1)-GARCH(1,1).",
    "text": "11.2 Estimation des paramètres du modèle AR(1)-GARCH(1,1).\nNous pouvons directement estimer les paramètres du modèle AR(1)-GARCH(1,1) en utilisant la fonction arch_model de la bibliothèque arch. C’est ce que nous allons faire dans la cellule suivante.\n\n## estimation des paramètres du modèle AR(1) -GARCH(1,1)\n\nar_garch_model = arch_model(train_close['log_return'], vol='Garch', p=1, q=1, mean='AR', lags=1).fit(disp='off')\nprint(ar_garch_model.summary())\n\n                           AR - GARCH Model Results                           \n==============================================================================\nDep. Variable:             log_return   R-squared:                      -0.000\nMean Model:                        AR   Adj. R-squared:                 -0.001\nVol Model:                      GARCH   Log-Likelihood:                19233.9\nDistribution:                  Normal   AIC:                          -38457.7\nMethod:            Maximum Likelihood   BIC:                          -38423.9\n                                        No. Observations:                 6462\nDate:                Sat, Feb 10 2024   Df Residuals:                     6460\nTime:                        21:20:13   Df Model:                            2\n                                    Mean Model                                   \n=================================================================================\n                     coef    std err          t      P&gt;|t|       95.0% Conf. Int.\n---------------------------------------------------------------------------------\nConst          4.7602e-04  2.667e-05     17.851  2.864e-71  [4.238e-04,5.283e-04]\nlog_return[1] -9.2258e-03  1.310e-02     -0.704      0.481 [-3.490e-02,1.644e-02]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.9447e-06  9.121e-12  4.325e+05      0.000 [3.945e-06,3.945e-06]\nalpha[1]       0.1000  1.048e-02      9.544  1.375e-21   [7.946e-02,  0.121]\nbeta[1]        0.8800  8.849e-03     99.450      0.000     [  0.863,  0.897]\n============================================================================\n\nCovariance estimator: robust\n\n\n\n# les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants\n\nprint(f\"Les paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\\n{ar_garch_model.params}\")\nconstance,phi, omega, alpha, beta = ar_garch_model.params\n\nLes paramètres du modèle AR(1)-GARCH(1,1) sont les suivants:\nConst            0.000476\nlog_return[1]   -0.009226\nomega            0.000004\nalpha[1]         0.100000\nbeta[1]          0.880000\nName: params, dtype: float64\n\n\nLes résultats ci-dessus donnent les mêmes résultats que ceux obtenus précédemment. Nous pouvons donc conclure que le modèle AR(1)-GARCH(1,1) est adéquat pour les log-rendements.",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  },
  {
    "objectID": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "href": "posts/ValueAtRisk/ValueAtRisk.html#backtesting-5",
    "title": "Modélisation de la value at risk",
    "section": "16.1 Backtesting",
    "text": "16.1 Backtesting\n\nlist_exceptions_TVE_residus = [i for i in range(train_size,len(ts_close['log_return'])) if ts_close['log_return'][i]&lt;var_dyn_TVE_residus[i]]\nlen(list_exceptions_TVE_residus)\n\n25\n\n\n\nprint(f\"Le nombre d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)}\")\nprint(f\"Le pourcentage d'exceptions pour la VaR TVE est: {len(list_exceptions_TVE_residus)/test_size*100.:0.2f}%\")\n\nLe nombre d'exceptions pour la VaR TVE est: 25\nLe pourcentage d'exceptions pour la VaR TVE est: 1.16%\n\n\n\ntest_except_TVE_residus = stats.binomtest(len(list_exceptions_TVE_residus), test_size, p = 0.01)\nprint(f'la p-value du test binomial est: {test_except_TVE_residus.pvalue:.2f}')\n\nla p-value du test binomial est: 0.45\n\n\nLa pvalue du test est supérieure au seuil de 5%. On ne peut donc pas rejeter l’hypothèse nulle selon laquelle la probabilité d’exception est de 1%. Nous concluons donc que notre VaR dynamique est satisfaisante.\n\nplt.figure(figsize=(12, 8))\nplt.plot(ts_close.index, ts_close['log_return'], label=\"historical log returns\", color = 'blue')\nplt.plot(ts_close.index[train_size:], var_dyn_TVE_residus[train_size:], label='VaR dynamique', color = 'red')\nplt.axvline(x = ts_close.index[train_size-251*10-1], color='green', linestyle='-', label='Limite de données pour le calcul de la VaR')\nplt.scatter(ts_close.index[list_exceptions_TVE_residus], ts_close['log_return'][list_exceptions_TVE_residus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()",
    "crumbs": [
      "About",
      "Ensai 3A",
      "Value at Risk"
    ]
  }
]